

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Forward feature selection &#8212; Machine learning for glycaemia prediction around exercise</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ml_heatmap/2_feature_selection/forward_feature_selection';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Machine learning" href="../3_ml/0_landing.html" />
    <link rel="prev" title="4. Preparation for machine learning" href="../1_preprocessing/4_ml_preparation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../0_landing_page.html">
  
  
  
  
  
    <p class="title logo__title">Machine learning for glycaemia prediction around exercise</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../0_landing_page.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 1 - Python package and Webapp for calculating the metrics of glycemic control</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../diametrics/0_overview.html">Diametrics Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../diametrics/1_upload_demo.html">1. Uploading and transforming data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../diametrics/2_preprocessing_demo.html">2. Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../diametrics/3_metrics_demo.html">3. Metrics of glycemic control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../diametrics/4_visualizations_demo.html">4. Visualizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../diametrics/5_webapp.html">5. Web Application</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 2 - Interpolation for the improved identification of hypoglycaemic episodes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../interpolation/1_create_df.html">1. Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../interpolation/2_EDA.html">2. Exploratory data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../interpolation/3_other_metrics.html">3. Comparing metrics of glycemic control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../interpolation/4_identifying_hypos.html">4. Hypoglycaemic episodes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapter 3 - Machine learning to predict hypoglycemia during exercise</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../1_preprocessing/0_landing.html">Preprocessing</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../1_preprocessing/1_preprocessing_101.html">1. Preprocessing of EXTOD 101 for ML</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1_preprocessing/2_preprocessing_edu.html">2. Preprocessing EXTOD education dataset</a></li>


<li class="toctree-l2"><a class="reference internal" href="../1_preprocessing/3_target_creation.html">3. Target creation</a></li>





<li class="toctree-l2"><a class="reference internal" href="../1_preprocessing/4_ml_preparation.html">4. Preparation for machine learning</a></li>
</ul>
</li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Forward feature selection</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/0_landing.html">Machine learning</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/figs_during_5.html">4. Predicting Euglycemia During Exercise</a></li>

<li class="toctree-l2"><a class="reference internal" href="../3_ml/contour_plot_lr.html">Heatmap (countour plot) of model predictions</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/ml_heatmap/2_feature_selection/forward_feature_selection.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Forward feature selection</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-feature-selection">Logistic regression feature selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xgb-with-all-the-features">XGB with all the features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xgb-with-a-few-features">XGB with a few features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypers">Hypers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">4.1.1. Logistic regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-shit">Other shit</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lr">LR</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="forward-feature-selection">
<h1>Forward feature selection<a class="headerlink" href="#forward-feature-selection" title="Permalink to this heading">#</a></h1>
<p><img alt="image.png" src="ml_heatmap/2_feature_selection/attachment:image.png" /></p>
<p>Reducing the number of features we use can have three benefits:</p>
<ul class="simple">
<li><p>Simplifies model explanation</p></li>
<li><p>Model fit may be improved by the removal of features that add no value</p></li>
<li><p>Model will be faster to fit</p></li>
</ul>
<p>In this notebook we will use a model-based approach whereby we incrementally add features that most increase model performance (we could use simple accuracy, but in this case we will use ROC Area Under Curve as a more thorough analysis of performance).</p>
<p>Two key advantages of this method are:</p>
<ul class="simple">
<li><p>It is relatively simple.</p></li>
<li><p>It is tailored to the model in question.</p></li>
</ul>
<p>Some key disadvantage of this method are:</p>
<ul class="simple">
<li><p>It may be slow if there are many parameters (though the loop to select features could be limited in the number of features to select).</p></li>
<li><p>The selection of features may be dependent on model meta-parameters (such as level of regularisation).</p></li>
<li><p>The selection of features may not transfer between models (e.g. a model that does not allow for feature interactions may not detect features which do not add much value independently).</p></li>
</ul>
<p>We will go through the following steps:</p>
<ul class="simple">
<li><p>Download and save pre-processed data</p></li>
<li><p>Split data into features (X) and label (y)</p></li>
<li><p>Loop through features to select the feature that most increases ROC AUC</p></li>
<li><p>Plot results</p></li>
</ul>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination">https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">hyperopt</span>
<span class="kn">import</span> <span class="nn">optuna</span>

<span class="c1"># Import machine learning methods</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">auc</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">KNNImputer</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">plot_importance</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">loadtxt</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">sort</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFromModel</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#SMOTE = False</span>
<span class="n">K_NEIGHBOURS</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">N_SPLITS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">TUNE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">directory</span> <span class="o">=</span> <span class="s1">&#39;../../Data/tidy_data/&#39;</span>

<span class="c1"># Set up k-fold splits</span>
<span class="n">number_of_splits</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FILENAME</span> <span class="o">=</span> <span class="s1">&#39;ml_during_hypo_5.csv&#39;</span> <span class="c1">#&#39;ml_during_glyc_ts.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">directory</span> <span class="o">+</span> <span class="n">FILENAME</span><span class="p">)</span>


<span class="n">df</span><span class="o">.</span><span class="n">day_of_week</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">day_of_week</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">day</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">day</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;season&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">month</span><span class="o">%</span><span class="k">12</span> // 3 + 1).astype(str)
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;start_glc&#39;</span><span class="p">,</span><span class="s1">&#39;duration&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y_hypo&#39;</span><span class="p">,</span> <span class="s1">&#39;stratify&#39;</span><span class="p">,</span> <span class="s1">&#39;bout_id&#39;</span><span class="p">,</span> <span class="s1">&#39;ID&#39;</span><span class="p">,</span> <span class="s1">&#39;month&#39;</span><span class="p">,</span> <span class="p">])</span> <span class="c1"># &#39;day&#39;, &#39;day_of_week&#39;</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y_hypo&#39;</span><span class="p">]</span>
<span class="n">strat</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;stratify&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="logistic-regression-feature-selection">
<h2>Logistic regression feature selection<a class="headerlink" href="#logistic-regression-feature-selection" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;start_glc&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;duration&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;intensity&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;form_of_exercise_aer&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;form_of_exercise_ana&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;form_of_exercise_mix&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;years_since_diagnosis&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;hba1c&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;cpep&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;time_of_day_morning&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;time_of_day_afternoon&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;time_of_day_evening&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;sex_male&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;sex_female&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;bmi&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;age&#39;</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">selected_features</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span><span class="p">,</span> <span class="n">weights_fig</span> <span class="o">=</span> <span class="n">model_helper</span><span class="o">.</span><span class="n">calculate_coefficient</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">weights_fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fig_directory</span><span class="o">+</span><span class="s1">&#39;weights_&#39;</span><span class="o">+</span><span class="n">period_str</span><span class="o">+</span><span class="s1">&#39;_lr_simple_features.svg&#39;</span><span class="p">)</span>
<span class="n">weights</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">weights_directory</span><span class="o">+</span><span class="s1">&#39;weights_&#39;</span><span class="o">+</span><span class="n">period_str</span><span class="o">+</span><span class="s1">&#39;_lr_simple_features.csv&#39;</span><span class="p">,</span>
               <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fcfa56c989a1ab97b1b11195586f428563eb71931d2dcc2e7d3ae1c9916a538d.png" src="../../_images/fcfa56c989a1ab97b1b11195586f428563eb71931d2dcc2e7d3ae1c9916a538d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#%%capture</span>

<span class="c1"># Create list to store accuracies and chosen features</span>
<span class="n">roc_auc_by_feature_number</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">chosen_features</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Initialise chosen features list and run tracker</span>
<span class="n">available_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">run</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">number_of_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Loop through feature list to select next feature</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">available_features</span><span class="p">)</span><span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

    <span class="c1"># Track and pront progress</span>
    <span class="n">run</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Feature run </span><span class="si">{}</span><span class="s1"> of </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">run</span><span class="p">,</span> <span class="n">number_of_features</span><span class="p">))</span>
    
    <span class="c1"># Convert DataFrames to NumPy arrays</span>
    <span class="n">y_np</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span>
    
    <span class="c1"># Reset best feature and accuracy</span>
    <span class="n">best_result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">best_feature</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="c1"># Loop through available features</span>
    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">available_features</span><span class="p">:</span>

        <span class="c1"># Create copy of already chosen features to avoid original being changed</span>
        <span class="n">features_to_use</span> <span class="o">=</span> <span class="n">chosen_features</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Create a list of features from features already chosen + 1 new feature</span>
        <span class="n">features_to_use</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="c1"># Get data for features, and convert to NumPy array</span>
        <span class="n">X_np</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">features_to_use</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        
        <span class="c1"># Set up lists to hold results for each selected features</span>
        <span class="n">test_auc_results</span> <span class="o">=</span> <span class="p">[]</span>
    
        <span class="c1"># Set up k-fold training/test splits</span>
        <span class="n">number_of_splits</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">number_of_splits</span><span class="p">)</span>
        <span class="c1"># Use strat column to stratify by both ID and y</span>
        <span class="n">skf</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">strat</span><span class="p">)</span>
    
        <span class="c1"># Loop through the k-fold splits</span>
        <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">y_np</span><span class="p">):</span>
            
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_np</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X_np</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
            <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train_std</span><span class="p">,</span> <span class="n">X_test_std</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">standardise_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
    
            <span class="c1"># Set up and fit model</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span>
            <span class="n">best_params</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">hyperopt_tune</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">X_train_std</span><span class="p">,</span>
                                             <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
            <span class="n">tuned_model</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">)</span>
            
            <span class="n">tuned_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
            <span class="c1"># Predict test set labels</span>
            <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            
            <span class="c1"># Calculate accuracy of test sets</span>
            <span class="n">accuracy_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred_test</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
          
            <span class="c1"># Get ROC AUC</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> 
            
            <span class="c1"># Probability of having a hypo</span>
            <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">)</span>
            <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
            <span class="n">test_auc_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">)</span>
            
            <span class="c1">#clear_output()</span>
        
        <span class="c1"># Get average result from all k-fold splits</span>
        <span class="n">feature_auc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_auc_results</span><span class="p">)</span>
    
        <span class="c1"># Update chosen feature and result if this feature is a new best</span>
        <span class="k">if</span> <span class="n">feature_auc</span> <span class="o">&gt;</span> <span class="n">best_result</span><span class="p">:</span>
            <span class="n">best_result</span> <span class="o">=</span> <span class="n">feature_auc</span>
            <span class="n">best_feature</span> <span class="o">=</span> <span class="n">feature</span>
    
    <span class="c1"># k-fold splits are complete    </span>
    <span class="c1"># Add mean accuracy and AUC to record of accuracy by feature number</span>
    <span class="n">roc_auc_by_feature_number</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_result</span><span class="p">)</span>
    <span class="n">chosen_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>
    <span class="n">available_features</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>

<span class="c1"># Put results in DataFrame</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;feature to add&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chosen_features</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;ROC AUC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">roc_auc_by_feature_number</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature to add</th>
      <th>ROC AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>start_glc</td>
      <td>0.876492</td>
    </tr>
    <tr>
      <th>1</th>
      <td>duration</td>
      <td>0.881285</td>
    </tr>
    <tr>
      <th>2</th>
      <td>hba1c</td>
      <td>0.883433</td>
    </tr>
    <tr>
      <th>3</th>
      <td>form_of_exercise_aer</td>
      <td>0.882297</td>
    </tr>
    <tr>
      <th>4</th>
      <td>time_of_day_morning</td>
      <td>0.882669</td>
    </tr>
    <tr>
      <th>5</th>
      <td>sex_male</td>
      <td>0.882452</td>
    </tr>
    <tr>
      <th>6</th>
      <td>time_of_day_evening</td>
      <td>0.882152</td>
    </tr>
    <tr>
      <th>7</th>
      <td>sex_female</td>
      <td>0.881956</td>
    </tr>
    <tr>
      <th>8</th>
      <td>bmi</td>
      <td>0.881987</td>
    </tr>
    <tr>
      <th>9</th>
      <td>form_of_exercise_mix</td>
      <td>0.881874</td>
    </tr>
    <tr>
      <th>10</th>
      <td>years_since_diagnosis</td>
      <td>0.881460</td>
    </tr>
    <tr>
      <th>11</th>
      <td>cpep</td>
      <td>0.881460</td>
    </tr>
    <tr>
      <th>12</th>
      <td>intensity</td>
      <td>0.881853</td>
    </tr>
    <tr>
      <th>13</th>
      <td>time_of_day_afternoon</td>
      <td>0.881894</td>
    </tr>
    <tr>
      <th>14</th>
      <td>form_of_exercise_ana</td>
      <td>0.878796</td>
    </tr>
    <tr>
      <th>15</th>
      <td>age</td>
      <td>0.880469</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span><span class="p">,</span> <span class="n">weights_fig</span> <span class="o">=</span> <span class="n">model_helper</span><span class="o">.</span><span class="n">calculate_coefficient</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">X_sub</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ValueError</span><span class="g g-Whitespace">                                </span>Traceback (most recent call last)
<span class="o">~</span>\<span class="n">AppData</span>\<span class="n">Local</span>\<span class="n">Temp</span><span class="o">/</span><span class="n">ipykernel_12580</span><span class="o">/</span><span class="mf">3409342499.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">weights</span><span class="p">,</span> <span class="n">weights_fig</span> <span class="o">=</span> <span class="n">model_helper</span><span class="o">.</span><span class="n">calculate_coefficient</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">X_sub</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="nn">~\OneDrive - University of Exeter\Desktop\PhD\Projects\Machine learning\ml-cgm\Code\2_machine_learning\model_helper.py</span> in <span class="ni">calculate_coefficient</span><span class="nt">(coeffs, column_names)</span>
<span class="g g-Whitespace">    </span><span class="mi">153</span>         <span class="n">log_coeff_combined</span> <span class="o">=</span> <span class="n">log_coeff_combined</span><span class="o">+</span><span class="n">coeffs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">154</span>     <span class="n">log_coeff_combined</span> <span class="o">=</span> <span class="n">log_coeff_combined</span><span class="o">/</span><span class="n">number_of_splits</span>
<span class="ne">--&gt; </span><span class="mi">155</span>     <span class="n">log_weights</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">log_coeff_combined</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;weights&#39;</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">column_names</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">156</span>     <span class="n">log_weights</span><span class="p">[</span><span class="s1">&#39;abs_weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">log_weights</span><span class="p">[</span><span class="s1">&#39;weights&#39;</span><span class="p">])</span>
<span class="g g-Whitespace">    </span><span class="mi">157</span>     <span class="n">log_weights</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;abs_weight&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nn">~\Anaconda3\envs\ml_cgm_env_test\lib\site-packages\pandas\core\frame.py</span> in <span class="ni">__init__</span><span class="nt">(self, data, index, columns, dtype, copy)</span>
<span class="g g-Whitespace">    </span><span class="mi">670</span>                 <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">671</span>             <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">672</span>                 <span class="n">mgr</span> <span class="o">=</span> <span class="n">ndarray_to_mgr</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">673</span>                     <span class="n">data</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">674</span>                     <span class="n">index</span><span class="p">,</span>

<span class="nn">~\Anaconda3\envs\ml_cgm_env_test\lib\site-packages\pandas\core\internals\construction.py</span> in <span class="ni">ndarray_to_mgr</span><span class="nt">(values, index, columns, dtype, copy, typ)</span>
<span class="g g-Whitespace">    </span><span class="mi">322</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">323</span> 
<span class="ne">--&gt; </span><span class="mi">324</span>     <span class="n">_check_values_indices_shape_match</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">325</span> 
<span class="g g-Whitespace">    </span><span class="mi">326</span>     <span class="k">if</span> <span class="n">typ</span> <span class="o">==</span> <span class="s2">&quot;array&quot;</span><span class="p">:</span>

<span class="nn">~\Anaconda3\envs\ml_cgm_env_test\lib\site-packages\pandas\core\internals\construction.py</span> in <span class="ni">_check_values_indices_shape_match</span><span class="nt">(values, index, columns)</span>
<span class="g g-Whitespace">    </span><span class="mi">391</span>         <span class="n">passed</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span>
<span class="g g-Whitespace">    </span><span class="mi">392</span>         <span class="n">implied</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns</span><span class="p">))</span>
<span class="ne">--&gt; </span><span class="mi">393</span>         <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of passed values is </span><span class="si">{</span><span class="n">passed</span><span class="si">}</span><span class="s2">, indices imply </span><span class="si">{</span><span class="n">implied</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">394</span> 
<span class="g g-Whitespace">    </span><span class="mi">395</span> 

<span class="ne">ValueError</span>: Shape of passed values is (2, 1), indices imply (7, 1)
</pre></div>
</div>
</div>
</div>
</section>
<section id="xgb-with-all-the-features">
<h2>XGB with all the features<a class="headerlink" href="#xgb-with-all-the-features" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#%%capture</span>

<span class="c1"># Create list to store accuracies and chosen features</span>
<span class="n">roc_auc_by_feature_number</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">chosen_features</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Initialise chosen features list and run tracker</span>
<span class="n">available_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">run</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">number_of_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Loop through feature list to select next feature</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">available_features</span><span class="p">)</span><span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

    <span class="c1"># Track and pront progress</span>
    <span class="n">run</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Feature run </span><span class="si">{}</span><span class="s1"> of </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">run</span><span class="p">,</span> <span class="n">number_of_features</span><span class="p">))</span>
    
    <span class="c1"># Convert DataFrames to NumPy arrays</span>
    <span class="n">y_np</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span>
    
    <span class="c1"># Reset best feature and accuracy</span>
    <span class="n">best_result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">best_feature</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="c1"># Loop through available features</span>
    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">available_features</span><span class="p">:</span>

        <span class="c1"># Create copy of already chosen features to avoid original being changed</span>
        <span class="n">features_to_use</span> <span class="o">=</span> <span class="n">chosen_features</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Create a list of features from features already chosen + 1 new feature</span>
        <span class="n">features_to_use</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="c1"># Get data for features, and convert to NumPy array</span>
        <span class="n">X_np</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">features_to_use</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        
        <span class="c1"># Set up lists to hold results for each selected features</span>
        <span class="n">test_auc_results</span> <span class="o">=</span> <span class="p">[]</span>
    
        <span class="c1"># Set up k-fold training/test splits</span>
        <span class="n">number_of_splits</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">number_of_splits</span><span class="p">)</span>
        <span class="c1"># Use strat column to stratify by both ID and y</span>
        <span class="n">skf</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">strat</span><span class="p">)</span>
    
        <span class="c1"># Loop through the k-fold splits</span>
        <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">y_np</span><span class="p">):</span>
            
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_np</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X_np</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
            <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train_std</span><span class="p">,</span> <span class="n">X_test_std</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">standardise_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
    
            <span class="c1"># Set up and fit model</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">eval_metric</span><span class="o">=</span><span class="s1">&#39;logloss&#39;</span><span class="p">)</span>

            <span class="c1"># Fit to training data</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
            <span class="c1"># Predict test set labels</span>
            <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            
            <span class="c1"># Calculate accuracy of test sets</span>
            <span class="n">accuracy_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred_test</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
          
            <span class="c1"># Get ROC AUC</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> 
            
            <span class="c1"># Probability of having a hypo</span>
            <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">)</span>
            <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
            <span class="n">test_auc_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">)</span>
            
            <span class="c1">#clear_output()</span>
        
        <span class="c1"># Get average result from all k-fold splits</span>
        <span class="n">feature_auc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_auc_results</span><span class="p">)</span>
    
        <span class="c1"># Update chosen feature and result if this feature is a new best</span>
        <span class="k">if</span> <span class="n">feature_auc</span> <span class="o">&gt;</span> <span class="n">best_result</span><span class="p">:</span>
            <span class="n">best_result</span> <span class="o">=</span> <span class="n">feature_auc</span>
            <span class="n">best_feature</span> <span class="o">=</span> <span class="n">feature</span>
    
    <span class="c1"># k-fold splits are complete    </span>
    <span class="c1"># Add mean accuracy and AUC to record of accuracy by feature number</span>
    <span class="n">roc_auc_by_feature_number</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_result</span><span class="p">)</span>
    <span class="n">chosen_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>
    <span class="n">available_features</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>

<span class="c1"># Put results in DataFrame</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;feature to add&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chosen_features</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;ROC AUC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">roc_auc_by_feature_number</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Feature run 1 of 237
Feature run 2 of 237
Feature run 3 of 237
Feature run 4 of 237
Feature run 5 of 237
Feature run 6 of 237
Feature run 7 of 237
Feature run 8 of 237
Feature run 9 of 237
Feature run 10 of 237
Feature run 11 of 237
Feature run 12 of 237
Feature run 13 of 237
Feature run 14 of 237
Feature run 15 of 237
Feature run 16 of 237
Feature run 17 of 237
Feature run 18 of 237
Feature run 19 of 237
Feature run 20 of 237
Feature run 21 of 237
Feature run 22 of 237
Feature run 23 of 237
Feature run 24 of 237
Feature run 25 of 237
Feature run 26 of 237
Feature run 27 of 237
Feature run 28 of 237
Feature run 29 of 237
Feature run 30 of 237
Feature run 31 of 237
Feature run 32 of 237
Feature run 33 of 237
Feature run 34 of 237
Feature run 35 of 237
Feature run 36 of 237
Feature run 37 of 237
Feature run 38 of 237
Feature run 39 of 237
Feature run 40 of 237
Feature run 41 of 237
Feature run 42 of 237
Feature run 43 of 237
Feature run 44 of 237
Feature run 45 of 237
Feature run 46 of 237
Feature run 47 of 237
Feature run 48 of 237
Feature run 49 of 237
Feature run 50 of 237
Feature run 51 of 237
Feature run 52 of 237
Feature run 53 of 237
Feature run 54 of 237
Feature run 55 of 237
Feature run 56 of 237
Feature run 57 of 237
Feature run 58 of 237
Feature run 59 of 237
Feature run 60 of 237
Feature run 61 of 237
Feature run 62 of 237
Feature run 63 of 237
Feature run 64 of 237
Feature run 65 of 237
Feature run 66 of 237
Feature run 67 of 237
Feature run 68 of 237
Feature run 69 of 237
Feature run 70 of 237
Feature run 71 of 237
Feature run 72 of 237
Feature run 73 of 237
Feature run 74 of 237
Feature run 75 of 237
Feature run 76 of 237
Feature run 77 of 237
Feature run 78 of 237
Feature run 79 of 237
Feature run 80 of 237
Feature run 81 of 237
Feature run 82 of 237
Feature run 83 of 237
Feature run 84 of 237
Feature run 85 of 237
Feature run 86 of 237
Feature run 87 of 237
Feature run 88 of 237
Feature run 89 of 237
Feature run 90 of 237
Feature run 91 of 237
Feature run 92 of 237
Feature run 93 of 237
Feature run 94 of 237
Feature run 95 of 237
Feature run 96 of 237
Feature run 97 of 237
Feature run 98 of 237
Feature run 99 of 237
Feature run 100 of 237
Feature run 101 of 237
Feature run 102 of 237
Feature run 103 of 237
Feature run 104 of 237
Feature run 105 of 237
Feature run 106 of 237
Feature run 107 of 237
Feature run 108 of 237
Feature run 109 of 237
Feature run 110 of 237
Feature run 111 of 237
Feature run 112 of 237
Feature run 113 of 237
Feature run 114 of 237
Feature run 115 of 237
Feature run 116 of 237
Feature run 117 of 237
Feature run 118 of 237
Feature run 119 of 237
Feature run 120 of 237
Feature run 121 of 237
Feature run 122 of 237
Feature run 123 of 237
Feature run 124 of 237
Feature run 125 of 237
Feature run 126 of 237
Feature run 127 of 237
Feature run 128 of 237
Feature run 129 of 237
Feature run 130 of 237
Feature run 131 of 237
Feature run 132 of 237
Feature run 133 of 237
Feature run 134 of 237
Feature run 135 of 237
Feature run 136 of 237
Feature run 137 of 237
Feature run 138 of 237
Feature run 139 of 237
Feature run 140 of 237
Feature run 141 of 237
Feature run 142 of 237
Feature run 143 of 237
Feature run 144 of 237
Feature run 145 of 237
Feature run 146 of 237
Feature run 147 of 237
Feature run 148 of 237
Feature run 149 of 237
Feature run 150 of 237
Feature run 151 of 237
Feature run 152 of 237
Feature run 153 of 237
Feature run 154 of 237
Feature run 155 of 237
Feature run 156 of 237
Feature run 157 of 237
Feature run 158 of 237
Feature run 159 of 237
Feature run 160 of 237
Feature run 161 of 237
Feature run 162 of 237
Feature run 163 of 237
Feature run 164 of 237
Feature run 165 of 237
Feature run 166 of 237
Feature run 167 of 237
Feature run 168 of 237
Feature run 169 of 237
Feature run 170 of 237
Feature run 171 of 237
Feature run 172 of 237
Feature run 173 of 237
Feature run 174 of 237
Feature run 175 of 237
Feature run 176 of 237
Feature run 177 of 237
Feature run 178 of 237
Feature run 179 of 237
Feature run 180 of 237
Feature run 181 of 237
Feature run 182 of 237
Feature run 183 of 237
Feature run 184 of 237
Feature run 185 of 237
Feature run 186 of 237
Feature run 187 of 237
Feature run 188 of 237
Feature run 189 of 237
Feature run 190 of 237
Feature run 191 of 237
Feature run 192 of 237
Feature run 193 of 237
Feature run 194 of 237
Feature run 195 of 237
Feature run 196 of 237
Feature run 197 of 237
Feature run 198 of 237
Feature run 199 of 237
Feature run 200 of 237
Feature run 201 of 237
Feature run 202 of 237
Feature run 203 of 237
Feature run 204 of 237
Feature run 205 of 237
Feature run 206 of 237
Feature run 207 of 237
Feature run 208 of 237
Feature run 209 of 237
Feature run 210 of 237
Feature run 211 of 237
Feature run 212 of 237
Feature run 213 of 237
Feature run 214 of 237
Feature run 215 of 237
Feature run 216 of 237
Feature run 217 of 237
Feature run 218 of 237
Feature run 219 of 237
Feature run 220 of 237
Feature run 221 of 237
Feature run 222 of 237
Feature run 223 of 237
Feature run 224 of 237
Feature run 225 of 237
Feature run 226 of 237
Feature run 227 of 237
Feature run 228 of 237
Feature run 229 of 237
Feature run 230 of 237
Feature run 231 of 237
Feature run 232 of 237
Feature run 233 of 237
Feature run 234 of 237
Feature run 235 of 237
Feature run 236 of 237
Feature run 237 of 237
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;forward_feature_results.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chart_x</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">number_of_features</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">chart_x</span><span class="p">,</span> <span class="n">roc_auc_by_feature_number</span><span class="p">,</span>
        <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;ROC AUC&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of features&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy (ROC AUC)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d54c088cf1c39690e1784ac35d88968539ebea1ceacadf8bcfd5c20822220116.png" src="../../_images/d54c088cf1c39690e1784ac35d88968539ebea1ceacadf8bcfd5c20822220116.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;ROC AUC&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature to add</th>
      <th>ROC AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>82</th>
      <td>day_of_week_1</td>
      <td>0.870981</td>
    </tr>
    <tr>
      <th>66</th>
      <td>before_glc__agg_linear_trend__attr_"rvalue"__c...</td>
      <td>0.870599</td>
    </tr>
    <tr>
      <th>85</th>
      <td>before_TIR_hyper</td>
      <td>0.870219</td>
    </tr>
    <tr>
      <th>81</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.870139</td>
    </tr>
    <tr>
      <th>69</th>
      <td>before_TIR_hyper_exercise</td>
      <td>0.869824</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>235</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_5</td>
      <td>0.824947</td>
    </tr>
    <tr>
      <th>232</th>
      <td>before_glc__energy_ratio_by_chunks__num_segmen...</td>
      <td>0.821729</td>
    </tr>
    <tr>
      <th>236</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.816382</td>
    </tr>
    <tr>
      <th>1</th>
      <td>duration</td>
      <td>0.815426</td>
    </tr>
    <tr>
      <th>0</th>
      <td>start_glc</td>
      <td>0.782313</td>
    </tr>
  </tbody>
</table>
<p>237 rows × 2 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_features</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span><span class="mi">83</span><span class="p">,</span><span class="s1">&#39;feature to add&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">selected_features_66</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span><span class="mi">67</span><span class="p">,</span><span class="s1">&#39;feature to add&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;features_xgb_during_ts&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>   <span class="c1">#Pickling </span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">selected_cols</span><span class="p">,</span> <span class="n">fp</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;features_xgb_66&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>   <span class="c1">#Pickling </span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">selected_features_66</span><span class="p">,</span> <span class="n">fp</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="xgb-with-a-few-features">
<h2>XGB with a few features<a class="headerlink" href="#xgb-with-a-few-features" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create list to store accuracies and chosen features</span>
<span class="n">roc_auc_by_feature_number</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">chosen_features</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Initialise chosen features list and run tracker</span>
<span class="n">available_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">run</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">number_of_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Loop through feature list to select next feature</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">available_features</span><span class="p">)</span><span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

    <span class="c1"># Track and pront progress</span>
    <span class="n">run</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Feature run </span><span class="si">{}</span><span class="s1"> of </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">run</span><span class="p">,</span> <span class="n">number_of_features</span><span class="p">))</span>
    
    <span class="c1"># Convert DataFrames to NumPy arrays</span>
    <span class="n">y_np</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span>
    
    <span class="c1"># Reset best feature and accuracy</span>
    <span class="n">best_result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">best_feature</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="c1"># Loop through available features</span>
    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">available_features</span><span class="p">:</span>

        <span class="c1"># Create copy of already chosen features to avoid original being changed</span>
        <span class="n">features_to_use</span> <span class="o">=</span> <span class="n">chosen_features</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Create a list of features from features already chosen + 1 new feature</span>
        <span class="n">features_to_use</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="c1"># Get data for features, and convert to NumPy array</span>
        <span class="n">X_np</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">features_to_use</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        
        <span class="c1"># Set up lists to hold results for each selected features</span>
        <span class="n">test_auc_results</span> <span class="o">=</span> <span class="p">[]</span>
    
        <span class="c1"># Set up k-fold training/test splits</span>
        <span class="n">number_of_splits</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">number_of_splits</span><span class="p">)</span>
        <span class="n">skf</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
        <span class="c1"># Loop through the k-fold splits</span>
        <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">y_np</span><span class="p">):</span>
            
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_np</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X_np</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
            <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train_std</span><span class="p">,</span> <span class="n">X_test_std</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">standardise_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
    
            <span class="c1"># Set up and fit model</span>
            <span class="n">pruner</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">pruners</span><span class="o">.</span><span class="n">MedianPruner</span><span class="p">(</span><span class="n">n_warmup_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
            <span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;maximize&quot;</span><span class="p">)</span>
            <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">trial</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">xgb_objective</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># , timeout=600</span>
            <span class="n">clear_output</span><span class="p">()</span>
            <span class="n">best_params</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_params</span>
            <span class="n">tuned_model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">)</span>
            
            <span class="n">tuned_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
            <span class="c1"># Predict test set labels</span>
            <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            
            <span class="c1"># Calculate accuracy of test sets</span>
            <span class="n">accuracy_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred_test</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
          
            <span class="c1"># Get ROC AUC</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># Probability of &#39;survived&#39;</span>
            <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">)</span>
            <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
            <span class="n">test_auc_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">)</span>
            
            <span class="n">clear_output</span><span class="p">()</span>
        
        <span class="c1"># Get average result from all k-fold splits</span>
        <span class="n">feature_auc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_auc_results</span><span class="p">)</span>
    
        <span class="c1"># Update chosen feature and result if this feature is a new best</span>
        <span class="k">if</span> <span class="n">feature_auc</span> <span class="o">&gt;</span> <span class="n">best_result</span><span class="p">:</span>
            <span class="n">best_result</span> <span class="o">=</span> <span class="n">feature_auc</span>
            <span class="n">best_feature</span> <span class="o">=</span> <span class="n">feature</span>
    
    <span class="c1"># k-fold splits are complete    </span>
    <span class="c1"># Add mean accuracy and AUC to record of accuracy by feature number</span>
    <span class="n">roc_auc_by_feature_number</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_result</span><span class="p">)</span>
    <span class="n">chosen_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>
    <span class="n">available_features</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>

<span class="c1"># Put results in DataFrame</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;feature to add&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chosen_features</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;ROC AUC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">roc_auc_by_feature_number</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature to add</th>
      <th>ROC AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>start_glc</td>
      <td>0.873456</td>
    </tr>
    <tr>
      <th>1</th>
      <td>duration</td>
      <td>0.884631</td>
    </tr>
    <tr>
      <th>2</th>
      <td>time_of_day_afternoon</td>
      <td>0.888122</td>
    </tr>
    <tr>
      <th>3</th>
      <td>form_of_exercise_ana</td>
      <td>0.885509</td>
    </tr>
    <tr>
      <th>4</th>
      <td>form_of_exercise_aer</td>
      <td>0.889630</td>
    </tr>
    <tr>
      <th>5</th>
      <td>intensity</td>
      <td>0.889527</td>
    </tr>
    <tr>
      <th>6</th>
      <td>cpep</td>
      <td>0.889713</td>
    </tr>
    <tr>
      <th>7</th>
      <td>form_of_exercise_mix</td>
      <td>0.891407</td>
    </tr>
    <tr>
      <th>8</th>
      <td>bmi</td>
      <td>0.887224</td>
    </tr>
    <tr>
      <th>9</th>
      <td>time_of_day_evening</td>
      <td>0.884476</td>
    </tr>
    <tr>
      <th>10</th>
      <td>time_of_day_morning</td>
      <td>0.887275</td>
    </tr>
    <tr>
      <th>11</th>
      <td>sex_male</td>
      <td>0.891180</td>
    </tr>
    <tr>
      <th>12</th>
      <td>hba1c</td>
      <td>0.882638</td>
    </tr>
    <tr>
      <th>13</th>
      <td>years_since_diagnosis</td>
      <td>0.879240</td>
    </tr>
    <tr>
      <th>14</th>
      <td>sex_female</td>
      <td>0.871659</td>
    </tr>
    <tr>
      <th>15</th>
      <td>age</td>
      <td>0.869862</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_sub</span> <span class="o">=</span> <span class="n">X</span><span class="p">[[</span><span class="s1">&#39;start_glc&#39;</span><span class="p">,</span> <span class="s1">&#39;duration&#39;</span><span class="p">,]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_sub</span> <span class="o">=</span> <span class="n">X</span><span class="p">[[</span><span class="s1">&#39;start_glc&#39;</span><span class="p">,</span> <span class="s1">&#39;duration&#39;</span><span class="p">,</span> <span class="s1">&#39;form_of_exercise_aer&#39;</span><span class="p">,</span>
      <span class="s1">&#39;intensity&#39;</span><span class="p">,</span><span class="s1">&#39;form_of_exercise_ana&#39;</span><span class="p">,</span> <span class="s1">&#39;time_of_day_afternoon&#39;</span><span class="p">,</span>
       <span class="s1">&#39;cpep&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span><span class="p">,</span> <span class="n">observed</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">predicted_proba</span><span class="p">,</span> <span class="n">list_shap_values</span> <span class="o">=</span> <span class="n">model_helper</span><span class="o">.</span><span class="n">run_k_fold_xgb</span><span class="p">(</span><span class="n">X_sub</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">strat</span><span class="p">,</span> <span class="n">number_of_splits</span><span class="o">=</span> <span class="n">N_SPLITS</span><span class="p">,</span> <span class="n">by_id</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">smote</span><span class="o">=</span><span class="n">SMOTE</span><span class="p">,</span> <span class="n">tune_hp</span><span class="o">=</span><span class="n">TUNE</span><span class="p">)</span>
<span class="c1">#clear_output()#wait=True)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_threshold_curves</span><span class="p">,</span> <span class="n">mean_auc</span><span class="p">,</span> <span class="n">sens_spec</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">k_fold_threshold_curves</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">predicted_proba</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/dc294a0cee617369714cb468b4b1dc1c853cef729ff71e419c05bd91e55786cc.png" src="../../_images/dc294a0cee617369714cb468b4b1dc1c853cef729ff71e419c05bd91e55786cc.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_accuracy</span><span class="p">,</span> <span class="n">log_single_fit_results</span> <span class="o">=</span> <span class="n">model_helper</span><span class="o">.</span><span class="n">calculate_k_fold_results</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_threshold_curves</span><span class="p">,</span> <span class="n">mean_auc</span><span class="p">,</span> <span class="n">sens_spec</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">k_fold_threshold_curves</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">predicted_proba</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/dc580522098e1c0d9f84ec9e79c2f5d8a55d29b2ade81ed0143c3ff3c8843cfe.png" src="../../_images/dc580522098e1c0d9f84ec9e79c2f5d8a55d29b2ade81ed0143c3ff3c8843cfe.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_accuracy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>observed_positive_rate</th>
      <td>0.477242</td>
    </tr>
    <tr>
      <th>observed_negative_rate</th>
      <td>0.522758</td>
    </tr>
    <tr>
      <th>predicted_positive_rate</th>
      <td>0.441651</td>
    </tr>
    <tr>
      <th>predicted_negative_rate</th>
      <td>0.558349</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.804071</td>
    </tr>
    <tr>
      <th>precision</th>
      <td>0.817540</td>
    </tr>
    <tr>
      <th>recall</th>
      <td>0.756193</td>
    </tr>
    <tr>
      <th>f1</th>
      <td>0.783647</td>
    </tr>
    <tr>
      <th>sensitivity</th>
      <td>0.756193</td>
    </tr>
    <tr>
      <th>specificity</th>
      <td>0.846982</td>
    </tr>
    <tr>
      <th>positive_likelihood</th>
      <td>6.301975</td>
    </tr>
    <tr>
      <th>negative_likelihood</th>
      <td>0.290149</td>
    </tr>
    <tr>
      <th>false_positive_rate</th>
      <td>0.153018</td>
    </tr>
    <tr>
      <th>false_negative_rate</th>
      <td>0.243807</td>
    </tr>
    <tr>
      <th>true_positive_rate</th>
      <td>0.756193</td>
    </tr>
    <tr>
      <th>true_negative_rate</th>
      <td>0.846982</td>
    </tr>
    <tr>
      <th>positive_predictive_value</th>
      <td>0.756193</td>
    </tr>
    <tr>
      <th>negative_predictive_value</th>
      <td>0.846982</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="hypers">
<h2>Hypers<a class="headerlink" href="#hypers" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">directory</span> <span class="o">+</span> <span class="n">FILENAME</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">day_of_week</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">day_of_week</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;season&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">month</span><span class="o">%</span><span class="k">12</span> // 3 + 1
<span class="c1">#df.month = df.month.astype(str) # let&#39;s have a nosey</span>
<span class="n">df</span><span class="o">.</span><span class="n">season</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">season</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">strat</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;ID&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="n">target_columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ID&#39;</span><span class="p">,</span> <span class="s1">&#39;month&#39;</span><span class="p">,</span> <span class="s1">&#39;day&#39;</span><span class="p">,</span> <span class="s1">&#39;season&#39;</span><span class="p">,</span> <span class="s1">&#39;day_of_week&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">target_columns</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">K_NEIGHBOURS</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#X = X[[&#39;start_glc&#39;, &#39;duration&#39;, &#39;form_of_exercise_aer&#39;,</span>
<span class="c1">#       &#39;years_since_diagnosis&#39;, &#39;hba1c&#39;]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">target_columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define hyperparameter space</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;penalty&#39;</span> <span class="p">:</span> <span class="n">hyperopt</span><span class="o">.</span><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;penalty&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">]),</span> <span class="c1"># , &#39;l1&#39;</span>
          <span class="s1">&#39;C&#39;</span> <span class="p">:</span> <span class="n">hyperopt</span><span class="o">.</span><span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="c1">#20</span>
          <span class="s1">&#39;solver&#39;</span> <span class="p">:</span> <span class="n">hyperopt</span><span class="o">.</span><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;solver&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">]),</span> <span class="c1">#&#39;max_iter&#39;: [1000] &#39;liblinear&#39;,</span>
         <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create list to store accuracies and chosen features</span>
<span class="n">roc_auc_by_feature_number</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">chosen_features</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Initialise chosen features list and run tracker</span>
<span class="n">available_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">run</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">number_of_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Loop through feature list to select next feature</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">available_features</span><span class="p">)</span><span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

    <span class="c1"># Track and pront progress</span>
    <span class="n">run</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Feature run </span><span class="si">{}</span><span class="s1"> of </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">run</span><span class="p">,</span> <span class="n">number_of_features</span><span class="p">))</span>
    
    <span class="c1"># Convert DataFrames to NumPy arrays</span>
    <span class="n">y_np</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span>
    
    <span class="c1"># Reset best feature and accuracy</span>
    <span class="n">best_result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">best_feature</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="c1"># Loop through available features</span>
    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">available_features</span><span class="p">:</span>

        <span class="c1"># Create copy of already chosen features to avoid original being changed</span>
        <span class="n">features_to_use</span> <span class="o">=</span> <span class="n">chosen_features</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Create a list of features from features already chosen + 1 new feature</span>
        <span class="n">features_to_use</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="c1"># Get data for features, and convert to NumPy array</span>
        <span class="n">X_np</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">features_to_use</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        
        <span class="c1"># Set up lists to hold results for each selected features</span>
        <span class="n">test_auc_results</span> <span class="o">=</span> <span class="p">[]</span>
    
        <span class="c1"># Set up k-fold training/test splits</span>
        <span class="n">number_of_splits</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">number_of_splits</span><span class="p">)</span>
        <span class="n">skf</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
        <span class="c1"># Loop through the k-fold splits</span>
        <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">y_np</span><span class="p">):</span>
            
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_np</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X_np</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
            <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train_std</span><span class="p">,</span> <span class="n">X_test_std</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">standardise_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
    
            <span class="c1"># Set up and fit model</span>
            <span class="c1">#model = XGBClassifier()</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span>
            <span class="n">best_params</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">hyperopt_tune</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">X_train_std</span><span class="p">,</span>
                                             <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
            <span class="n">tuned_model</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">)</span>
            
            <span class="n">tuned_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
            <span class="c1"># Predict test set labels</span>
            <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            
            <span class="c1"># Calculate accuracy of test sets</span>
            <span class="n">accuracy_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred_test</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
          
            <span class="c1"># Get ROC AUC</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># Probability of &#39;survived&#39;</span>
            <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">)</span>
            <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
            <span class="n">test_auc_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">)</span>
            
            <span class="c1">#clear_output()</span>
        
        <span class="c1"># Get average result from all k-fold splits</span>
        <span class="n">feature_auc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_auc_results</span><span class="p">)</span>
    
        <span class="c1"># Update chosen feature and result if this feature is a new best</span>
        <span class="k">if</span> <span class="n">feature_auc</span> <span class="o">&gt;</span> <span class="n">best_result</span><span class="p">:</span>
            <span class="n">best_result</span> <span class="o">=</span> <span class="n">feature_auc</span>
            <span class="n">best_feature</span> <span class="o">=</span> <span class="n">feature</span>
    
    <span class="c1"># k-fold splits are complete    </span>
    <span class="c1"># Add mean accuracy and AUC to record of accuracy by feature number</span>
    <span class="n">roc_auc_by_feature_number</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_result</span><span class="p">)</span>
    <span class="n">chosen_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>
    <span class="n">available_features</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>

<span class="c1"># Put results in DataFrame</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;feature to add&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chosen_features</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;ROC AUC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">roc_auc_by_feature_number</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Feature run 1 of 16
100%|███████████████████████████████████████████████| 60/60 [00:08&lt;00:00,  6.95trial/s, best loss: -0.6234066410372381]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.57trial/s, best loss: -0.6186933195328719]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.97trial/s, best loss: -0.6022132200303842]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.86trial/s, best loss: -0.6512181951547623]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.77trial/s, best loss: -0.629134484918067]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.82trial/s, best loss: -0.6197054500226142]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.72trial/s, best loss: -0.6108424225028704]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.53trial/s, best loss: -0.5674344623038653]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.68trial/s, best loss: -0.5929319747416762]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.39trial/s, best loss: -0.586104370339445]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.60trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.35trial/s, best loss: -0.9682601705922602]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.81trial/s, best loss: -0.976344747706688]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 37.04trial/s, best loss: -0.972613404423106]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.31trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 38.06trial/s, best loss: -0.5443511608623548]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.47trial/s, best loss: -0.5580588462118313]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.73trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.78trial/s, best loss: -0.6135314830277516]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.39trial/s, best loss: -0.5393079039534263]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.59trial/s, best loss: -0.5]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.42trial/s, best loss: -0.479768624824595]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.87trial/s, best loss: -0.6057413254241613]
100%|██████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.06trial/s, best loss: -0.35031014797805843]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.67trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.63trial/s, best loss: -0.5291732624755013]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.29trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.76trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.49trial/s, best loss: -0.5047247010866414]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.71trial/s, best loss: -0.5063147983856939]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.03trial/s, best loss: -0.6651091135232926]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.26trial/s, best loss: -0.7084570591100442]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.43trial/s, best loss: -0.6654844222941237]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.61trial/s, best loss: -0.6892406411995964]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.35trial/s, best loss: -0.6988347162787463]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 36.37trial/s, best loss: -0.5234094678124529]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.13trial/s, best loss: -0.5716738278305442]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.51trial/s, best loss: -0.6115992009648725]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.01trial/s, best loss: -0.5244101462385043]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.20trial/s, best loss: -0.5056450701040254]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.46trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.02trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.42trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.16trial/s, best loss: -0.5281107574018021]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.72trial/s, best loss: -0.533080228229482]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.66trial/s, best loss: -0.5443407960199005]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.40trial/s, best loss: -0.5576927425808023]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.99trial/s, best loss: -0.5544589117350311]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.90trial/s, best loss: -0.5799564241728421]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.43trial/s, best loss: -0.5728838325853252]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.46trial/s, best loss: -0.5444199457259159]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.80trial/s, best loss: -0.5577536269700449]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 38.15trial/s, best loss: -0.557442681696413]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.68trial/s, best loss: -0.5518456667710399]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.51trial/s, best loss: -0.5398036043558432]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.05trial/s, best loss: -0.5239993215739485]
100%|██████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 36.35trial/s, best loss: -0.49703319068990703]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 36.79trial/s, best loss: -0.5410265108026302]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.64trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.34trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 36.31trial/s, best loss: -0.5304387155133423]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 38.16trial/s, best loss: -0.5214573983230699]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.78trial/s, best loss: -0.5311588908603834]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.68trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 38.49trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.86trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 38.75trial/s, best loss: -0.5363388477194447]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.88trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.70trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.76trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.15trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 40.54trial/s, best loss: -0.5494459520578924]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.68trial/s, best loss: -0.5]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 37.88trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.84trial/s, best loss: -0.5397161917684306]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.97trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 37.77trial/s, best loss: -0.5494459520578924]
100%|██████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.55trial/s, best loss: -0.37280642243328815]
100%|██████████████████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 36.29trial/s, best loss: -0.5]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.13trial/s, best loss: -0.5397161917684306]
Feature run 2 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.36trial/s, best loss: -0.9729015905321876]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.54trial/s, best loss: -0.9719868750652335]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.92trial/s, best loss: -0.9787673665472173]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.66trial/s, best loss: -0.9755335357014463]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.40trial/s, best loss: -0.9696547855129944]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.37trial/s, best loss: -0.9740841248303933]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.28trial/s, best loss: -0.9709601758109221]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 38.12trial/s, best loss: -0.9778203968502475]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.91trial/s, best loss: -0.9750878474759073]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 30.00trial/s, best loss: -0.968397812800798]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.86trial/s, best loss: -0.970650535202774]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.00trial/s, best loss: -0.9682601705922602]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.32trial/s, best loss: -0.9770781871528141]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.37trial/s, best loss: -0.972613404423106]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.82trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 39.62trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.68trial/s, best loss: -0.9682601705922602]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.79trial/s, best loss: -0.9772025652622668]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.15trial/s, best loss: -0.972613404423106]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.81trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.85trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.68trial/s, best loss: -0.9682601705922602]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.23trial/s, best loss: -0.976344747706688]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.51trial/s, best loss: -0.9726755934778323]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.38trial/s, best loss: -0.9666508657180298]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.41trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.98trial/s, best loss: -0.9682601705922602]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.99trial/s, best loss: -0.976344747706688]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.61trial/s, best loss: -0.974113842210857]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.70trial/s, best loss: -0.9671931716707836]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.93trial/s, best loss: -0.9705911729232625]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.70trial/s, best loss: -0.9683718644539541]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.24trial/s, best loss: -0.9774220389892033]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.25trial/s, best loss: -0.9726755934778323]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.79trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.21trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.11trial/s, best loss: -0.9682601705922602]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.65trial/s, best loss: -0.976344747706688]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.17trial/s, best loss: -0.9732347876004592]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.34trial/s, best loss: -0.9666738423268273]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.38trial/s, best loss: -0.9705949419568822]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.30trial/s, best loss: -0.9682601705922602]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.60trial/s, best loss: -0.9767216510686663]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.24trial/s, best loss: -0.9734178031752659]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.94trial/s, best loss: -0.9667374810098691]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.72trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.70trial/s, best loss: -0.9690613076807107]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.40trial/s, best loss: -0.9766524313166105]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.30trial/s, best loss: -0.972613404423106]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 38.69trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.21trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.14trial/s, best loss: -0.9682601705922602]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.17trial/s, best loss: -0.9761760109707872]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.54trial/s, best loss: -0.972613404423106]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 38.37trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.44trial/s, best loss: -0.9712187170209559]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.47trial/s, best loss: -0.9682601705922602]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.90trial/s, best loss: -0.9760533724153128]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.07trial/s, best loss: -0.972613404423106]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.67trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.76trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.10trial/s, best loss: -0.9682601705922602]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 39.50trial/s, best loss: -0.9765935039255936]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.10trial/s, best loss: -0.972613404423106]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.18trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.27trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.48trial/s, best loss: -0.9682601705922602]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.96trial/s, best loss: -0.976344747706688]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.28trial/s, best loss: -0.972613404423106]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.39trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.05trial/s, best loss: -0.9705289838685361]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.05trial/s, best loss: -0.9682601705922602]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.39trial/s, best loss: -0.976344747706688]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.65trial/s, best loss: -0.972613404423106]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.79trial/s, best loss: -0.966358693131081]
Feature run 3 of 16
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.68trial/s, best loss: -0.974587290818634]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.83trial/s, best loss: -0.9729668962877918]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.76trial/s, best loss: -0.9801789421192405]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.05trial/s, best loss: -0.9783073270013567]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.66trial/s, best loss: -0.9708694290783842]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.36trial/s, best loss: -0.9735903814262024]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.02trial/s, best loss: -0.9710942664300874]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.52trial/s, best loss: -0.9781719317167077]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.72trial/s, best loss: -0.9754725788307876]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.28trial/s, best loss: -0.9673758248385113]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 37.38trial/s, best loss: -0.9735884969093924]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.38trial/s, best loss: -0.9707114195920168]
100%|███████████████████████████████████████████████| 60/60 [00:03&lt;00:00, 17.19trial/s, best loss: -0.9780553816001575]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.43trial/s, best loss: -0.9749634693664543]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.50trial/s, best loss: -0.9676556031033643]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.63trial/s, best loss: -0.9733378561736771]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.89trial/s, best loss: -0.9705889259993737]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.59trial/s, best loss: -0.9778028563476326]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.82trial/s, best loss: -0.9749885479363091]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.80trial/s, best loss: -0.9688580697909055]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.59trial/s, best loss: -0.9737109905020354]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.29trial/s, best loss: -0.9708357977014694]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.17trial/s, best loss: -0.9779272344570853]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.16trial/s, best loss: -0.9752178791357895]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.28trial/s, best loss: -0.968535672453583]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.17trial/s, best loss: -0.9732134780642244]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.91trial/s, best loss: -0.9706917046469286]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.79trial/s, best loss: -0.9787762817613566]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.84trial/s, best loss: -0.9755499889828247]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.09trial/s, best loss: -0.9673993088172193]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.01trial/s, best loss: -0.9733359716568671]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.02trial/s, best loss: -0.9708317387421864]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.14trial/s, best loss: -0.9783041378190631]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 30.00trial/s, best loss: -0.9757178559417365]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.18trial/s, best loss: -0.9679995999025849]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.19trial/s, best loss: -0.974210387456656]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.94trial/s, best loss: -0.9712769195978153]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.14trial/s, best loss: -0.9774977095872618]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.56trial/s, best loss: -0.9761333194169015]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.81trial/s, best loss: -0.9680964350740469]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.67trial/s, best loss: -0.9740841248303933]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.05trial/s, best loss: -0.9708376822182794]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.24trial/s, best loss: -0.9779447749597004]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.60trial/s, best loss: -0.9750120319150171]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.74trial/s, best loss: -0.9679564009787891]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.06trial/s, best loss: -0.9734678878335593]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.16trial/s, best loss: -0.9703636537591762]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.75trial/s, best loss: -0.9778905588607081]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.68trial/s, best loss: -0.974139790557701]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.48trial/s, best loss: -0.9681957346136449]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.30trial/s, best loss: -0.9745891753354439]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.08trial/s, best loss: -0.969928475338923]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.95trial/s, best loss: -0.9773911619060872]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.75trial/s, best loss: -0.9745480058912893]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.14trial/s, best loss: -0.9679607498637349]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.25trial/s, best loss: -0.9734622342831297]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.21trial/s, best loss: -0.9705889259993737]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.75trial/s, best loss: -0.9783041378190631]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.36trial/s, best loss: -0.974714713147549]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 37.18trial/s, best loss: -0.9682715501745351]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.58trial/s, best loss: -0.9737128750188451]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.33trial/s, best loss: -0.9702392756497235]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.46trial/s, best loss: -0.9771192116341367]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.69trial/s, best loss: -0.9746934036113138]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.58trial/s, best loss: -0.9672128866158717]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.91trial/s, best loss: -0.9737128750188451]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.86trial/s, best loss: -0.9695380904336129]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 36.92trial/s, best loss: -0.9774610339908849]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.54trial/s, best loss: -0.9743202692829559]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.63trial/s, best loss: -0.9679995999025849]
Feature run 4 of 16
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.37trial/s, best loss: -0.9748417005879692]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.17trial/s, best loss: -0.9723412367069082]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.00trial/s, best loss: -0.9801128390680628]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.27trial/s, best loss: -0.9785112897053196]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.52trial/s, best loss: -0.968908806781941]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.83trial/s, best loss: -0.974587290818634]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.49trial/s, best loss: -0.9726579804938013]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.80trial/s, best loss: -0.9798778543181527]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.94trial/s, best loss: -0.9777515395052708]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.24trial/s, best loss: -0.9699561632397453]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.14trial/s, best loss: -0.9744629127091813]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.78trial/s, best loss: -0.971674842570365]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 22.35trial/s, best loss: -0.979121873151724]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 22.97trial/s, best loss: -0.9775162648296977]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.76trial/s, best loss: -0.9706751788841341]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.12trial/s, best loss: -0.9745854063018241]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.89trial/s, best loss: -0.9735415289519768]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.11trial/s, best loss: -0.9794984865880387]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.85trial/s, best loss: -0.9778947627828224]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.22trial/s, best loss: -0.9702105730090803]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.07trial/s, best loss: -0.9749604251469922]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.33trial/s, best loss: -0.9726539215345185]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.08trial/s, best loss: -0.9802391016943254]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.27trial/s, best loss: -0.9779943522480835]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.35trial/s, best loss: -0.9700241508077327]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.46trial/s, best loss: -0.9748341625207295]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.07trial/s, best loss: -0.9727024840830809]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.53trial/s, best loss: -0.9796657736956244]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.68trial/s, best loss: -0.9780629196673974]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.70trial/s, best loss: -0.9689814331605374]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.29trial/s, best loss: -0.9754598221016131]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.62trial/s, best loss: -0.9721507555462778]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.12trial/s, best loss: -0.9793919389068642]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.47trial/s, best loss: -0.977883745607626]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.53trial/s, best loss: -0.9708168075705389]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.52trial/s, best loss: -0.9747078998944669]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.03trial/s, best loss: -0.9732758120817822]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.58trial/s, best loss: -0.9789548759698012]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.08trial/s, best loss: -0.9772640295028353]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.28trial/s, best loss: -0.9695595449326792]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 30.00trial/s, best loss: -0.9743441881501582]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.19trial/s, best loss: -0.9718108177063399]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.78trial/s, best loss: -0.9795808254763478]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.66trial/s, best loss: -0.9766928759466073]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.86trial/s, best loss: -0.9693734126569948]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.67trial/s, best loss: -0.9747154379617065]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.86trial/s, best loss: -0.9717431200640154]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 36.10trial/s, best loss: -0.9784830219531712]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.22trial/s, best loss: -0.9764441197277017]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.12trial/s, best loss: -0.9699774727759802]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.13trial/s, best loss: -0.9744610281923716]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.29trial/s, best loss: -0.9717193461596446]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.81trial/s, best loss: -0.9793180078627838]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.39trial/s, best loss: -0.9771879240162822]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.41trial/s, best loss: -0.9693903733082836]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.05trial/s, best loss: -0.9749604251469922]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.24trial/s, best loss: -0.9721996080205034]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.13trial/s, best loss: -0.9788113627665865]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.55trial/s, best loss: -0.9774410291201334]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.36trial/s, best loss: -0.969765972004778]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.53trial/s, best loss: -0.9748360470375396]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.43trial/s, best loss: -0.972069286434958]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.80trial/s, best loss: -0.9786985816836562]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.87trial/s, best loss: -0.9773185355274908]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.25trial/s, best loss: -0.9698903501142306]
Feature run 5 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.45trial/s, best loss: -0.9745891753354439]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.62trial/s, best loss: -0.9714514548469773]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.40trial/s, best loss: -0.979060118985492]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.71trial/s, best loss: -0.9765741513875842]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.58trial/s, best loss: -0.9696181099166173]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.69trial/s, best loss: -0.9748360470375396]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.50trial/s, best loss: -0.9718129921488131]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.12trial/s, best loss: -0.9785626065476812]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.10trial/s, best loss: -0.9773185355274908]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.56trial/s, best loss: -0.9688856127288963]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.48trial/s, best loss: -0.9748360470375396]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.37trial/s, best loss: -0.9712026986280717]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.18trial/s, best loss: -0.9788035347736841]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.21trial/s, best loss: -0.9764460042445116]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.89trial/s, best loss: -0.9698765786452354]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.60trial/s, best loss: -0.974210387456656]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.60trial/s, best loss: -0.9723118092521077]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.60trial/s, best loss: -0.9784473610966147]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.76trial/s, best loss: -0.9763253951686787]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.52trial/s, best loss: -0.9700128437068735]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.98trial/s, best loss: -0.9748360470375396]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.12trial/s, best loss: -0.972069286434958]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.98trial/s, best loss: -0.9792813322664069]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.39trial/s, best loss: -0.9767975391109719]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.36trial/s, best loss: -0.9698687506523326]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.61trial/s, best loss: -0.9749604251469922]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.61trial/s, best loss: -0.9720223184775424]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.71trial/s, best loss: -0.9784382284382283]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.78trial/s, best loss: -0.977313171902724]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.92trial/s, best loss: -0.9681151352793143]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.35trial/s, best loss: -0.9745891753354439]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.48trial/s, best loss: -0.9715758329564299]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.40trial/s, best loss: -0.9788151318002063]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.77trial/s, best loss: -0.9774447981537534]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.25trial/s, best loss: -0.970381919075949]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.92trial/s, best loss: -0.9743385345997286]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.98trial/s, best loss: -0.9730308248964965]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.30trial/s, best loss: -0.9786869846571339]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.20trial/s, best loss: -0.976440350694082]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.11trial/s, best loss: -0.9692455554395852]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.07trial/s, best loss: -0.9744666817428012]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.45trial/s, best loss: -0.9713543297498521]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.46trial/s, best loss: -0.9783940147746117]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.30trial/s, best loss: -0.9765938663326722]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.86trial/s, best loss: -0.968850531723666]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.20trial/s, best loss: -0.9750866877732548]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.69trial/s, best loss: -0.9709526377436823]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.61trial/s, best loss: -0.9776302056152801]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.12trial/s, best loss: -0.9756223254357582]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.68trial/s, best loss: -0.9695112723097796]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.71trial/s, best loss: -0.9745854063018242]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.78trial/s, best loss: -0.9716942675897899]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.96trial/s, best loss: -0.9786985816836562]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.44trial/s, best loss: -0.9761079509213836]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.35trial/s, best loss: -0.968908806781941]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.87trial/s, best loss: -0.974587290818634]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.61trial/s, best loss: -0.971950851801598]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.09trial/s, best loss: -0.9786985816836562]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.66trial/s, best loss: -0.9774410291201334]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.55trial/s, best loss: -0.9698903501142306]
Feature run 6 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.09trial/s, best loss: -0.9740878938640132]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.51trial/s, best loss: -0.9710626645328138]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.78trial/s, best loss: -0.979178553618852]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.18trial/s, best loss: -0.9765741513875842]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.55trial/s, best loss: -0.9692393220378295]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.52trial/s, best loss: -0.9734641187999398]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.16trial/s, best loss: -0.970804485729859]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.42trial/s, best loss: -0.9788113627665865]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.87trial/s, best loss: -0.9764400607684189]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.17trial/s, best loss: -0.9683783877813728]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.46trial/s, best loss: -0.9748360470375396]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.45trial/s, best loss: -0.9715636560785814]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.77trial/s, best loss: -0.9784495355390878]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 37.33trial/s, best loss: -0.9760766389497733]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.67trial/s, best loss: -0.9691440814575142]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.15trial/s, best loss: -0.9748360470375396]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.93trial/s, best loss: -0.9715310394414871]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.18trial/s, best loss: -0.9792753887903141]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.07trial/s, best loss: -0.9764400607684189]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.09trial/s, best loss: -0.9679453838035925]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.11trial/s, best loss: -0.9748360470375396]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.23trial/s, best loss: -0.9709345173897411]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.98trial/s, best loss: -0.9776919597815119]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.91trial/s, best loss: -0.9769362685407461]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.48trial/s, best loss: -0.9684172378202227]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.28trial/s, best loss: -0.9747116689280869]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.13trial/s, best loss: -0.9710610699416671]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.72trial/s, best loss: -0.9788113627665865]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.63trial/s, best loss: -0.9773204200443006]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.70trial/s, best loss: -0.969385009683517]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.58trial/s, best loss: -0.9742122719734659]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.84trial/s, best loss: -0.9721585835391805]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.00trial/s, best loss: -0.9786791566642311]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.63trial/s, best loss: -0.9759374746315045]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.12trial/s, best loss: -0.9688718412599009]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.57trial/s, best loss: -0.9739672847881803]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.96trial/s, best loss: -0.9709482888587366]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.13trial/s, best loss: -0.9780597304851035]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.95trial/s, best loss: -0.9766985294970368]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.72trial/s, best loss: -0.9680481624511474]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.86trial/s, best loss: -0.9749623096638022]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.09trial/s, best loss: -0.9710378758886222]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.96trial/s, best loss: -0.978067268552343]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.92trial/s, best loss: -0.9757426445859281]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.81trial/s, best loss: -0.9692490345475419]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.07trial/s, best loss: -0.9745854063018242]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.63trial/s, best loss: -0.9710745514849993]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.44trial/s, best loss: -0.9785798571246331]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.17trial/s, best loss: -0.9759522608403206]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.53trial/s, best loss: -0.9676321191246563]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.97trial/s, best loss: -0.9740878938640132]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.72trial/s, best loss: -0.9716886140393601]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.73trial/s, best loss: -0.9783235628384881]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.41trial/s, best loss: -0.9773185355274908]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.89trial/s, best loss: -0.9692430910714492]
Feature run 7 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.60trial/s, best loss: -0.9745854063018242]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.08trial/s, best loss: -0.9708179672731913]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.50trial/s, best loss: -0.979178553618852]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.14trial/s, best loss: -0.976162167020376]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.80trial/s, best loss: -0.9692371475953564]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.11trial/s, best loss: -0.9748360470375396]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.28trial/s, best loss: -0.9710567210567209]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.68trial/s, best loss: -0.9780729221027726]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.87trial/s, best loss: -0.976438176251609]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.52trial/s, best loss: -0.9683862157742753]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.73trial/s, best loss: -0.9743347655661088]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.59trial/s, best loss: -0.9708676170429904]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.44trial/s, best loss: -0.9783251574296349]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.72trial/s, best loss: -0.9761681104964687]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.67trial/s, best loss: -0.9692449755882591]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.83trial/s, best loss: -0.9744629127091813]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.67trial/s, best loss: -0.9713079416437627]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.01trial/s, best loss: -0.9789257384406638]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.33trial/s, best loss: -0.9763253951686787]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.65trial/s, best loss: -0.9691205974788064]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.88trial/s, best loss: -0.9749604251469922]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.09trial/s, best loss: -0.9710667234920966]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.37trial/s, best loss: -0.9777016722912244]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.08trial/s, best loss: -0.9769322095814633]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.63trial/s, best loss: -0.9676132739565574]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.34trial/s, best loss: -0.9745891753354439]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.22trial/s, best loss: -0.9711854480511197]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.11trial/s, best loss: -0.9789279128831367]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.99trial/s, best loss: -0.9770813763351075]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 35.11trial/s, best loss: -0.9691343689478018]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.64trial/s, best loss: -0.9748360470375396]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.48trial/s, best loss: -0.9721585835391805]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.18trial/s, best loss: -0.9785547785547785]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.70trial/s, best loss: -0.975942838256271]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.01trial/s, best loss: -0.9689867967853042]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.38trial/s, best loss: -0.9739654002713702]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.73trial/s, best loss: -0.970582692597618]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.63trial/s, best loss: -0.9780597304851035]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.73trial/s, best loss: -0.9764792007329319]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.52trial/s, best loss: -0.9691108849690938]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.29trial/s, best loss: -0.974964194180612]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.66trial/s, best loss: -0.9710259889364365]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.76trial/s, best loss: -0.978067268552343]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.57trial/s, best loss: -0.9764751417736491]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.16trial/s, best loss: -0.9679334968514072]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.29trial/s, best loss: -0.9728441127694858]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.52trial/s, best loss: -0.9711970450776419]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.81trial/s, best loss: -0.9784479409479407]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.99trial/s, best loss: -0.9755772419951523]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.80trial/s, best loss: -0.9688680722262811]
Feature run 8 of 16
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.05trial/s, best loss: -0.974587290818634]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.18trial/s, best loss: -0.9710745514849993]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.89trial/s, best loss: -0.9785528940379684]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.18trial/s, best loss: -0.9763351076783913]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.71trial/s, best loss: -0.9691108849690938]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.05trial/s, best loss: -0.9749604251469922]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.32trial/s, best loss: -0.9713051873499634]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.57trial/s, best loss: -0.9783197938048683]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.25trial/s, best loss: -0.9763507636641965]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.01trial/s, best loss: -0.9692430910714492]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.32trial/s, best loss: -0.9749604251469922]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 22.94trial/s, best loss: -0.9719270778972271]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.03trial/s, best loss: -0.9784241670435699]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.08trial/s, best loss: -0.9764751417736491]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.67trial/s, best loss: -0.9690002783286363]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.49trial/s, best loss: -0.9749604251469922]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.35trial/s, best loss: -0.970701417156641]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.76trial/s, best loss: -0.9780553816001578]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.59trial/s, best loss: -0.976560959769915]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.76trial/s, best loss: -0.968908806781941]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.54trial/s, best loss: -0.9748360470375396]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.06trial/s, best loss: -0.970942635308307]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.45trial/s, best loss: -0.9785644910644911]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.98trial/s, best loss: -0.9765663233946815]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.57trial/s, best loss: -0.9691168284451865]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.11trial/s, best loss: -0.9747097844112769]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.06trial/s, best loss: -0.9707907142608633]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.88trial/s, best loss: -0.9783022533022532]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.63trial/s, best loss: -0.9754315543494647]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.79trial/s, best loss: -0.9691108849690938]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.88trial/s, best loss: -0.9740897783808231]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.54trial/s, best loss: -0.9710745514849993]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.63trial/s, best loss: -0.9778517088218581]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 36.02trial/s, best loss: -0.9759309513040855]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.94trial/s, best loss: -0.9688737257767107]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.18trial/s, best loss: -0.9750866877732548]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.90trial/s, best loss: -0.9706669160027367]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.34trial/s, best loss: -0.978067268552343]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.29trial/s, best loss: -0.9757269886001229]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.25trial/s, best loss: -0.9692371475953564]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.65trial/s, best loss: -0.974587290818634]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.05trial/s, best loss: -0.971198929594452]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 34.33trial/s, best loss: -0.9789179104477611]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.12trial/s, best loss: -0.9762924886059213]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.79trial/s, best loss: -0.968908806781941]
Feature run 9 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.97trial/s, best loss: -0.9747116689280867]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.61trial/s, best loss: -0.9706801076204062]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.31trial/s, best loss: -0.9776822472717994]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.97trial/s, best loss: -0.9760844669426758]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.07trial/s, best loss: -0.9691168284451865]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.73trial/s, best loss: -0.9749604251469922]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.41trial/s, best loss: -0.970804485729859]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.15trial/s, best loss: -0.9785528940379684]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.43trial/s, best loss: -0.9762924886059213]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.83trial/s, best loss: -0.968908806781941]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.13trial/s, best loss: -0.9747135534448967]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.09trial/s, best loss: -0.970179840888796]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.94trial/s, best loss: -0.9784344594046086]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.23trial/s, best loss: -0.9759134108014704]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.66trial/s, best loss: -0.9683978128007977]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.83trial/s, best loss: -0.9744610281923715]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.51trial/s, best loss: -0.9713233077039046]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.65trial/s, best loss: -0.9775675816720593]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.40trial/s, best loss: -0.9760534448967284]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.36trial/s, best loss: -0.968850531723666]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.28trial/s, best loss: -0.9743366500829186]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.43trial/s, best loss: -0.9707863653759177]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.27trial/s, best loss: -0.9770600667988727]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.67trial/s, best loss: -0.9759412436651242]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.92trial/s, best loss: -0.967620812023797]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.67trial/s, best loss: -0.9740916628976329]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.28trial/s, best loss: -0.9709501733755467]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.09trial/s, best loss: -0.9779237553491285]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.17trial/s, best loss: -0.9763253951686787]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.17trial/s, best loss: -0.968850531723666]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 21.34trial/s, best loss: -0.974466681742801]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.01trial/s, best loss: -0.9706161790117015]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.04trial/s, best loss: -0.9778341683192432]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.75trial/s, best loss: -0.9759290667872756]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.41trial/s, best loss: -0.9692430910714492]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.91trial/s, best loss: -0.9744591436755616]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.67trial/s, best loss: -0.9711813890918368]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.80trial/s, best loss: -0.9786772721474215]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.58trial/s, best loss: -0.9762865451298286]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.86trial/s, best loss: -0.969246860105069]
Feature run 10 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.75trial/s, best loss: -0.9748360470375396]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.96trial/s, best loss: -0.970823910749284]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.51trial/s, best loss: -0.9784363439214185]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.65trial/s, best loss: -0.9761681104964687]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.28trial/s, best loss: -0.9691205974788064]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.93trial/s, best loss: -0.9745854063018242]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.37trial/s, best loss: -0.9699410871052662]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.11trial/s, best loss: -0.9783060223358732]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.75trial/s, best loss: -0.9764751417736491]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.51trial/s, best loss: -0.9692412065546392]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.81trial/s, best loss: -0.9730853309211518]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.80trial/s, best loss: -0.971198929594452]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.79trial/s, best loss: -0.9784285159285158]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.63trial/s, best loss: -0.9760280764012107]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.52trial/s, best loss: -0.968908806781941]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.77trial/s, best loss: -0.9747097844112769]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.50trial/s, best loss: -0.9708676170429904]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.54trial/s, best loss: -0.9784266314117058]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.18trial/s, best loss: -0.9761681104964687]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.38trial/s, best loss: -0.969246860105069]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.78trial/s, best loss: -0.9740916628976329]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.10trial/s, best loss: -0.9695892623131428]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.06trial/s, best loss: -0.978030592955966]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.07trial/s, best loss: -0.9758454232334829]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.01trial/s, best loss: -0.9691108849690938]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.30trial/s, best loss: -0.9750866877732548]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.25trial/s, best loss: -0.9706161790117015]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.57trial/s, best loss: -0.9762855303900079]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.88trial/s, best loss: -0.9761602825035659]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.12trial/s, best loss: -0.9692430910714492]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.13trial/s, best loss: -0.9744610281923716]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 33.69trial/s, best loss: -0.9711813890918368]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.90trial/s, best loss: -0.9780529172320216]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.18trial/s, best loss: -0.9760437323870159]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.56trial/s, best loss: -0.9686581660462256]
Feature run 11 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.19trial/s, best loss: -0.9750866877732548]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.87trial/s, best loss: -0.9706161790117015]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.13trial/s, best loss: -0.9780769810620559]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.23trial/s, best loss: -0.9761602825035659]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.26trial/s, best loss: -0.9691168284451865]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.31trial/s, best loss: -0.9747097844112769]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 37.24trial/s, best loss: -0.9705889259993736]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.43trial/s, best loss: -0.9777195027195027]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.08trial/s, best loss: -0.9763507636641965]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.27trial/s, best loss: -0.9691168284451865]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.32trial/s, best loss: -0.9749604251469922]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.49trial/s, best loss: -0.9704918009022485]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.92trial/s, best loss: -0.9780653840355333]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.86trial/s, best loss: -0.9757019100302682]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.53trial/s, best loss: -0.968908806781941]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.27trial/s, best loss: -0.9750866877732548]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.50trial/s, best loss: -0.9707588949193428]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.67trial/s, best loss: -0.977816627816628]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.02trial/s, best loss: -0.9754569228449824]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.90trial/s, best loss: -0.969109000452284]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.67trial/s, best loss: -0.9735922659430122]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.44trial/s, best loss: -0.9689406986048776]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.28trial/s, best loss: -0.9779488339189835]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.04trial/s, best loss: -0.9748622853100464]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.22trial/s, best loss: -0.969109000452284]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.43trial/s, best loss: -0.9745891753354439]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.56trial/s, best loss: -0.9706161790117015]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.86trial/s, best loss: -0.9773814493963748]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.92trial/s, best loss: -0.9752178791357895]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.08trial/s, best loss: -0.9692449755882591]
Feature run 12 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.79trial/s, best loss: -0.9749604251469922]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.70trial/s, best loss: -0.9698699103549849]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.99trial/s, best loss: -0.976344747706688]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.89trial/s, best loss: -0.9762263855547436]
100%|████████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.85trial/s, best loss: -0.966358693131081]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.11trial/s, best loss: -0.9748360470375396]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.81trial/s, best loss: -0.9704918009022485]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.61trial/s, best loss: -0.9778322838024331]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.62trial/s, best loss: -0.9763507636641965]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.26trial/s, best loss: -0.9687825441556784]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.14trial/s, best loss: -0.9748379315543494]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.22trial/s, best loss: -0.9706345168098901]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.17trial/s, best loss: -0.978067268552343]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.36trial/s, best loss: -0.9757968606849203]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.86trial/s, best loss: -0.9687358661239257]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.57trial/s, best loss: -0.9740916628976329]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.30trial/s, best loss: -0.9703674227927959]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.86trial/s, best loss: -0.9779488339189835]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.85trial/s, best loss: -0.9758454232334828]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.84trial/s, best loss: -0.9648033144301801]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.27trial/s, best loss: -0.9745891753354439]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.67trial/s, best loss: -0.9701370043720792]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.13trial/s, best loss: -0.978067268552343]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.28trial/s, best loss: -0.9754722889051246]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.42trial/s, best loss: -0.9691205974788064]
Feature run 13 of 16
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.08trial/s, best loss: -0.974214156490276]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.87trial/s, best loss: -0.9699942884644376]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.84trial/s, best loss: -0.9777351587053079]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.80trial/s, best loss: -0.9761681104964687]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.38trial/s, best loss: -0.9691049414930012]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 32.82trial/s, best loss: -0.9737091059852254]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.45trial/s, best loss: -0.9703674227927959]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.65trial/s, best loss: -0.9780732120284361]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.22trial/s, best loss: -0.9762263855547436]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.35trial/s, best loss: -0.968908806781941]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.34trial/s, best loss: -0.9740897783808231]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.68trial/s, best loss: -0.9706064665019888]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.10trial/s, best loss: -0.9778419963121456]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.20trial/s, best loss: -0.9714040520010669]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.08trial/s, best loss: -0.9691108849690938]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.25trial/s, best loss: -0.9742160410070857]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.83trial/s, best loss: -0.9683037319231348]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 22.03trial/s, best loss: -0.9781897621449861]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.79trial/s, best loss: -0.9763507636641965]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.90trial/s, best loss: -0.9688602442333785]
Feature run 14 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.54trial/s, best loss: -0.9740878938640133]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.76trial/s, best loss: -0.9691295851743611]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.29trial/s, best loss: -0.9781897621449861]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 23.79trial/s, best loss: -0.9762263855547436]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.39trial/s, best loss: -0.9687242690974033]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.63trial/s, best loss: -0.9724709784411276]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.83trial/s, best loss: -0.9682601705922602]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.18trial/s, best loss: -0.9781897621449861]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.18trial/s, best loss: -0.9761681104964687]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.26trial/s, best loss: -0.9688602442333785]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.51trial/s, best loss: -0.9740897783808231]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.72trial/s, best loss: -0.9704470073873059]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.02trial/s, best loss: -0.9780732120284361]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.62trial/s, best loss: -0.9763507636641965]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.83trial/s, best loss: -0.969109000452284]
Feature run 15 of 16
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.76trial/s, best loss: -0.973835368611488]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.19trial/s, best loss: -0.9704470073873059]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.92trial/s, best loss: -0.9773698523698524]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.67trial/s, best loss: -0.9764751417736491]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.77trial/s, best loss: -0.9684172378202227]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.88trial/s, best loss: -0.9739616312377507]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.68trial/s, best loss: -0.9704470073873059]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 24.81trial/s, best loss: -0.9781897621449861]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 30.31trial/s, best loss: -0.9762263855547436]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.69trial/s, best loss: -0.968850531723666]
Feature run 16 of 16
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.83trial/s, best loss: -0.9739616312377507]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.60trial/s, best loss: -0.9705101387004372]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.08trial/s, best loss: -0.9781897621449861]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.93trial/s, best loss: -0.9764751417736491]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.10trial/s, best loss: -0.9686581660462256]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature to add</th>
      <th>ROC AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>start_glc</td>
      <td>0.968914</td>
    </tr>
    <tr>
      <th>1</th>
      <td>duration</td>
      <td>0.971699</td>
    </tr>
    <tr>
      <th>2</th>
      <td>intensity</td>
      <td>0.972748</td>
    </tr>
    <tr>
      <th>3</th>
      <td>sex_female</td>
      <td>0.973007</td>
    </tr>
    <tr>
      <th>4</th>
      <td>years_since_diagnosis</td>
      <td>0.973049</td>
    </tr>
    <tr>
      <th>5</th>
      <td>sex_male</td>
      <td>0.972930</td>
    </tr>
    <tr>
      <th>6</th>
      <td>cpep</td>
      <td>0.973129</td>
    </tr>
    <tr>
      <th>7</th>
      <td>time_of_day_evening</td>
      <td>0.973288</td>
    </tr>
    <tr>
      <th>8</th>
      <td>hba1c</td>
      <td>0.972251</td>
    </tr>
    <tr>
      <th>9</th>
      <td>form_of_exercise_ana</td>
      <td>0.972293</td>
    </tr>
    <tr>
      <th>10</th>
      <td>age</td>
      <td>0.971932</td>
    </tr>
    <tr>
      <th>11</th>
      <td>form_of_exercise_aer</td>
      <td>0.971694</td>
    </tr>
    <tr>
      <th>12</th>
      <td>bmi</td>
      <td>0.971656</td>
    </tr>
    <tr>
      <th>13</th>
      <td>form_of_exercise_mix</td>
      <td>0.971417</td>
    </tr>
    <tr>
      <th>14</th>
      <td>time_of_day_afternoon</td>
      <td>0.971296</td>
    </tr>
    <tr>
      <th>15</th>
      <td>time_of_day_morning</td>
      <td>0.971137</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create list to store accuracies and chosen features</span>
<span class="n">roc_auc_by_feature_number</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">chosen_features</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Initialise chosen features list and run tracker</span>
<span class="n">available_features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">run</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">number_of_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Loop through feature list to select next feature</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">available_features</span><span class="p">)</span><span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

    <span class="c1"># Track and pront progress</span>
    <span class="n">run</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Feature run </span><span class="si">{}</span><span class="s1"> of </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">run</span><span class="p">,</span> <span class="n">number_of_features</span><span class="p">))</span>
    
    <span class="c1"># Convert DataFrames to NumPy arrays</span>
    <span class="n">y_np</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span>
    
    <span class="c1"># Reset best feature and accuracy</span>
    <span class="n">best_result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">best_feature</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="c1"># Loop through available features</span>
    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">available_features</span><span class="p">:</span>

        <span class="c1"># Create copy of already chosen features to avoid original being changed</span>
        <span class="n">features_to_use</span> <span class="o">=</span> <span class="n">chosen_features</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Create a list of features from features already chosen + 1 new feature</span>
        <span class="n">features_to_use</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="c1"># Get data for features, and convert to NumPy array</span>
        <span class="n">X_np</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">features_to_use</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        
        <span class="c1"># Set up lists to hold results for each selected features</span>
        <span class="n">test_auc_results</span> <span class="o">=</span> <span class="p">[]</span>
    
        <span class="c1"># Set up k-fold training/test splits</span>
        <span class="n">number_of_splits</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">number_of_splits</span><span class="p">)</span>
        <span class="n">skf</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
        <span class="c1"># Loop through the k-fold splits</span>
        <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_np</span><span class="p">,</span> <span class="n">y_np</span><span class="p">):</span>
            
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_np</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X_np</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
            <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    
            <span class="c1"># Get X and Y train/test</span>
            <span class="n">X_train_std</span><span class="p">,</span> <span class="n">X_test_std</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">standardise_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
    
            <span class="c1"># Set up and fit model</span>
            <span class="n">pruner</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">pruners</span><span class="o">.</span><span class="n">MedianPruner</span><span class="p">(</span><span class="n">n_warmup_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
            <span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;maximize&quot;</span><span class="p">)</span>
            <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">trial</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">xgb_objective</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># , timeout=600</span>
            <span class="n">clear_output</span><span class="p">()</span>
            <span class="n">best_params</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_params</span>
            <span class="n">tuned_model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">best_params</span><span class="p">)</span>
            
            <span class="n">tuned_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
            <span class="c1"># Predict test set labels</span>
            <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            
            <span class="c1"># Calculate accuracy of test sets</span>
            <span class="n">accuracy_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred_test</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
          
            <span class="c1"># Get ROC AUC</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_std</span><span class="p">)</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># Probability of &#39;survived&#39;</span>
            <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">)</span>
            <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
            <span class="n">test_auc_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">)</span>
            
            <span class="n">clear_output</span><span class="p">()</span>
        
        <span class="c1"># Get average result from all k-fold splits</span>
        <span class="n">feature_auc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_auc_results</span><span class="p">)</span>
    
        <span class="c1"># Update chosen feature and result if this feature is a new best</span>
        <span class="k">if</span> <span class="n">feature_auc</span> <span class="o">&gt;</span> <span class="n">best_result</span><span class="p">:</span>
            <span class="n">best_result</span> <span class="o">=</span> <span class="n">feature_auc</span>
            <span class="n">best_feature</span> <span class="o">=</span> <span class="n">feature</span>
    
    <span class="c1"># k-fold splits are complete    </span>
    <span class="c1"># Add mean accuracy and AUC to record of accuracy by feature number</span>
    <span class="n">roc_auc_by_feature_number</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_result</span><span class="p">)</span>
    <span class="n">chosen_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>
    <span class="n">available_features</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">best_feature</span><span class="p">)</span>

<span class="c1"># Put results in DataFrame</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;feature to add&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chosen_features</span>
<span class="n">results</span><span class="p">[</span><span class="s1">&#39;ROC AUC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">roc_auc_by_feature_number</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature to add</th>
      <th>ROC AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>start_glc</td>
      <td>0.962885</td>
    </tr>
    <tr>
      <th>1</th>
      <td>form_of_exercise_mix</td>
      <td>0.966308</td>
    </tr>
    <tr>
      <th>2</th>
      <td>duration</td>
      <td>0.967343</td>
    </tr>
    <tr>
      <th>3</th>
      <td>sex_male</td>
      <td>0.970408</td>
    </tr>
    <tr>
      <th>4</th>
      <td>cpep</td>
      <td>0.972889</td>
    </tr>
    <tr>
      <th>5</th>
      <td>time_of_day_evening</td>
      <td>0.971970</td>
    </tr>
    <tr>
      <th>6</th>
      <td>form_of_exercise_ana</td>
      <td>0.970227</td>
    </tr>
    <tr>
      <th>7</th>
      <td>age</td>
      <td>0.970183</td>
    </tr>
    <tr>
      <th>8</th>
      <td>hba1c</td>
      <td>0.971607</td>
    </tr>
    <tr>
      <th>9</th>
      <td>intensity</td>
      <td>0.971634</td>
    </tr>
    <tr>
      <th>10</th>
      <td>time_of_day_afternoon</td>
      <td>0.968967</td>
    </tr>
    <tr>
      <th>11</th>
      <td>bmi</td>
      <td>0.969409</td>
    </tr>
    <tr>
      <th>12</th>
      <td>sex_female</td>
      <td>0.967764</td>
    </tr>
    <tr>
      <th>13</th>
      <td>form_of_exercise_aer</td>
      <td>0.967538</td>
    </tr>
    <tr>
      <th>14</th>
      <td>years_since_diagnosis</td>
      <td>0.966790</td>
    </tr>
    <tr>
      <th>15</th>
      <td>time_of_day_morning</td>
      <td>0.963395</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>FILENAME = ‘ml_during_glyc.csv’
df = pd.read_csv(directory + FILENAME)
df.day_of_week = df.day_of_week.astype(str)
df[‘season’]=df.month%12 // 3 + 1
#df.month = df.month.astype(str) # let’s have a nosey
<a class="reference external" href="http://df.ID">df.ID</a> = df.ID.astype(str)</p>
<p>df.drop(columns=[‘ID’, ‘month’], inplace=True)
target_columns = [‘y_hypo’]#, ‘y_hyper’]
strat = df[‘stratify’]</p>
<p>X = df.drop(columns=target_columns+[‘stratify’])
y = df[target_columns[0]]</p>
<p>X = pd.get_dummies(X)
imputer = KNNImputer(n_neighbors=K_NEIGHBOURS)
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)</p>
<section id="logistic-regression">
<h3>4.1.1. Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SMOTE</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">K_NEIGHBOURS</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">N_SPLITS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">TUNE</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[[</span><span class="s1">&#39;start_glc&#39;</span><span class="p">,</span><span class="s1">&#39;duration&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_indices</span><span class="p">,</span> <span class="n">observed</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">predicted_proba</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">,</span> <span class="n">list_shap_values</span> <span class="o">=</span> <span class="n">model_helper</span><span class="o">.</span><span class="n">run_k_fold_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">strat</span><span class="p">,</span> <span class="n">number_of_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">smote</span><span class="o">=</span><span class="n">SMOTE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.44trial/s, best loss: -0.9726084441873916]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 28.47trial/s, best loss: -0.9804810102178523]
100%|███████████████████████████████████████████████| 60/60 [00:01&lt;00:00, 31.88trial/s, best loss: -0.9782182379024483]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.23trial/s, best loss: -0.9750476190476188]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 27.99trial/s, best loss: -0.9741823581823581]
100%|████████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.48trial/s, best loss: -0.973098901098901]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.17trial/s, best loss: -0.9753711201079623]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 26.27trial/s, best loss: -0.9745898425898426]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 25.25trial/s, best loss: -0.9738314028314028]
100%|███████████████████████████████████████████████| 60/60 [00:02&lt;00:00, 29.65trial/s, best loss: -0.9739400424137266]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># glyc only</span>
<span class="n">log_threshold_curves</span><span class="p">,</span> <span class="n">mean_auc</span><span class="p">,</span> <span class="n">sens_spec</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">k_fold_threshold_curves</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">predicted_proba</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/553e0ee06bcf02822a2e17ecb874ad37c807c3cac0d322c3464071ef50fc8b20.png" src="../../_images/553e0ee06bcf02822a2e17ecb874ad37c807c3cac0d322c3464071ef50fc8b20.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_accuracy</span><span class="p">,</span> <span class="n">log_single_fit_results</span> <span class="o">=</span> <span class="n">model_helper</span><span class="o">.</span><span class="n">calculate_k_fold_results</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_accuracy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>observed_positive_rate</th>
      <td>0.153257</td>
    </tr>
    <tr>
      <th>observed_negative_rate</th>
      <td>0.846743</td>
    </tr>
    <tr>
      <th>predicted_positive_rate</th>
      <td>0.153257</td>
    </tr>
    <tr>
      <th>predicted_negative_rate</th>
      <td>0.846743</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.949330</td>
    </tr>
    <tr>
      <th>precision</th>
      <td>0.837448</td>
    </tr>
    <tr>
      <th>recall</th>
      <td>0.837448</td>
    </tr>
    <tr>
      <th>f1</th>
      <td>0.837448</td>
    </tr>
    <tr>
      <th>sensitivity</th>
      <td>0.837448</td>
    </tr>
    <tr>
      <th>specificity</th>
      <td>0.969914</td>
    </tr>
    <tr>
      <th>positive_likelihood</th>
      <td>38.344624</td>
    </tr>
    <tr>
      <th>negative_likelihood</th>
      <td>0.169236</td>
    </tr>
    <tr>
      <th>false_positive_rate</th>
      <td>0.030086</td>
    </tr>
    <tr>
      <th>false_negative_rate</th>
      <td>0.162552</td>
    </tr>
    <tr>
      <th>true_positive_rate</th>
      <td>0.837448</td>
    </tr>
    <tr>
      <th>true_negative_rate</th>
      <td>0.969914</td>
    </tr>
    <tr>
      <th>positive_predictive_value</th>
      <td>0.837448</td>
    </tr>
    <tr>
      <th>negative_predictive_value</th>
      <td>0.969914</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span><span class="p">,</span> <span class="n">observed</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">predicted_proba</span><span class="p">,</span> <span class="n">list_shap_values</span> <span class="o">=</span> <span class="n">model_helper</span><span class="o">.</span><span class="n">run_k_fold_xgb</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">strat</span><span class="p">,</span> <span class="n">number_of_splits</span><span class="o">=</span> <span class="n">N_SPLITS</span><span class="p">,</span> <span class="n">by_id</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">smote</span><span class="o">=</span><span class="n">SMOTE</span><span class="p">,</span> <span class="n">tune_hp</span><span class="o">=</span><span class="n">TUNE</span><span class="p">)</span>
<span class="c1">#clear_output()#wait=True)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_threshold_curves</span><span class="p">,</span> <span class="n">mean_auc</span><span class="p">,</span> <span class="n">sens_spec</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">k_fold_threshold_curves</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">predicted_proba</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/458ec5c6fa4d2c7ff2ca4636e9169874f434b334bd8fa726c3dd4ce3d89a0880.png" src="../../_images/458ec5c6fa4d2c7ff2ca4636e9169874f434b334bd8fa726c3dd4ce3d89a0880.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_accuracy</span><span class="p">,</span> <span class="n">log_single_fit_results</span> <span class="o">=</span> <span class="n">model_helper</span><span class="o">.</span><span class="n">calculate_k_fold_results</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_accuracy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>observed_positive_rate</th>
      <td>0.153257</td>
    </tr>
    <tr>
      <th>observed_negative_rate</th>
      <td>0.846743</td>
    </tr>
    <tr>
      <th>predicted_positive_rate</th>
      <td>0.173665</td>
    </tr>
    <tr>
      <th>predicted_negative_rate</th>
      <td>0.826335</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.935044</td>
    </tr>
    <tr>
      <th>precision</th>
      <td>0.759614</td>
    </tr>
    <tr>
      <th>recall</th>
      <td>0.860483</td>
    </tr>
    <tr>
      <th>f1</th>
      <td>0.801170</td>
    </tr>
    <tr>
      <th>sensitivity</th>
      <td>0.860483</td>
    </tr>
    <tr>
      <th>specificity</th>
      <td>0.949767</td>
    </tr>
    <tr>
      <th>positive_likelihood</th>
      <td>20.218248</td>
    </tr>
    <tr>
      <th>negative_likelihood</th>
      <td>0.146753</td>
    </tr>
    <tr>
      <th>false_positive_rate</th>
      <td>0.050233</td>
    </tr>
    <tr>
      <th>false_negative_rate</th>
      <td>0.139517</td>
    </tr>
    <tr>
      <th>true_positive_rate</th>
      <td>0.860483</td>
    </tr>
    <tr>
      <th>true_negative_rate</th>
      <td>0.949767</td>
    </tr>
    <tr>
      <th>positive_predictive_value</th>
      <td>0.860483</td>
    </tr>
    <tr>
      <th>negative_predictive_value</th>
      <td>0.949767</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="other-shit">
<h2>Other shit<a class="headerlink" href="#other-shit" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;feature to add&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;start_glc&#39;, &#39;duration&#39;, &#39;form_of_exercise_aer&#39;,
       &#39;years_since_diagnosis&#39;, &#39;hba1c&#39;, &#39;intensity&#39;,
       &#39;time_of_day_evening&#39;, &#39;bmi&#39;, &#39;form_of_exercise_ana&#39;,
       &#39;form_of_exercise_mix&#39;, &#39;time_of_day_afternoon&#39;,
       &#39;time_of_day_morning&#39;, &#39;cpep&#39;, &#39;age&#39;, &#39;sex_female&#39;, &#39;sex_male&#39;],
      dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="c1">#.sort_values(&#39;ROC AUC&#39;, ascending=False)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature to add</th>
      <th>ROC AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>start_glc</td>
      <td>0.873456</td>
    </tr>
    <tr>
      <th>1</th>
      <td>duration</td>
      <td>0.884631</td>
    </tr>
    <tr>
      <th>2</th>
      <td>time_of_day_afternoon</td>
      <td>0.888122</td>
    </tr>
    <tr>
      <th>3</th>
      <td>form_of_exercise_ana</td>
      <td>0.885509</td>
    </tr>
    <tr>
      <th>4</th>
      <td>form_of_exercise_aer</td>
      <td>0.889630</td>
    </tr>
    <tr>
      <th>5</th>
      <td>intensity</td>
      <td>0.889527</td>
    </tr>
    <tr>
      <th>6</th>
      <td>cpep</td>
      <td>0.889713</td>
    </tr>
    <tr>
      <th>7</th>
      <td>form_of_exercise_mix</td>
      <td>0.891407</td>
    </tr>
    <tr>
      <th>8</th>
      <td>bmi</td>
      <td>0.887224</td>
    </tr>
    <tr>
      <th>9</th>
      <td>time_of_day_evening</td>
      <td>0.884476</td>
    </tr>
    <tr>
      <th>10</th>
      <td>time_of_day_morning</td>
      <td>0.887275</td>
    </tr>
    <tr>
      <th>11</th>
      <td>sex_male</td>
      <td>0.891180</td>
    </tr>
    <tr>
      <th>12</th>
      <td>hba1c</td>
      <td>0.882638</td>
    </tr>
    <tr>
      <th>13</th>
      <td>years_since_diagnosis</td>
      <td>0.879240</td>
    </tr>
    <tr>
      <th>14</th>
      <td>sex_female</td>
      <td>0.871659</td>
    </tr>
    <tr>
      <th>15</th>
      <td>age</td>
      <td>0.869862</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># k-fold feature importance</span>
<span class="n">thresholds</span> <span class="o">=</span> <span class="p">[]</span>

 <span class="c1"># Set up k-fold training/test splits</span>
<span class="n">number_of_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">number_of_splits</span><span class="p">)</span>
<span class="n">skf</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Loop through the k-fold splits</span>
<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

    <span class="c1"># Get X and Y train/test</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    
    <span class="c1"># fit model on all training data</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c1"># make predictions for test data and evaluate</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">roc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Roc: </span><span class="si">%.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">roc</span><span class="p">))</span>
    <span class="c1"># Fit model using each importance as a threshold</span>
    <span class="n">fold_thresholds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span>
    <span class="n">thresholds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fold_thresholds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[14:20:59] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
Roc: 0.91%
[14:21:00] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
Roc: 0.86%
[14:21:00] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
Roc: 0.84%
[14:21:01] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
Roc: 0.90%
[14:21:01] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
Roc: 0.90%
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_thresholds</span> <span class="o">=</span> <span class="n">thresholds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">thresholds</span><span class="p">)):</span>
    <span class="n">mean_thresholds</span> <span class="o">=</span> <span class="n">mean_thresholds</span> <span class="o">+</span> <span class="n">thresholds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">mean_thresholds</span> <span class="o">=</span> <span class="n">sort</span><span class="p">(</span><span class="n">mean_thresholds</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">thresholds</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thresholds</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.        , 0.01176153, 0.00951999, 0.00155779, 0.        ,
       0.00392267, 0.00108163, 0.01912748, 0.00645288, 0.00144545,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.00285794, 0.003112  ,
       0.00169183, 0.00249984, 0.00100719, 0.00298894, 0.        ,
       0.        , 0.00172891, 0.        , 0.00188886, 0.00321063,
       0.00523246, 0.05063064, 0.00175078, 0.00422426, 0.00305662,
       0.00342761, 0.00267811, 0.        , 0.02438727, 0.        ,
       0.00057665, 0.00744106, 0.00633203, 0.        , 0.        ,
       0.        , 0.00507879, 0.00604111, 0.00014877, 0.00133925,
       0.        , 0.        , 0.00323719, 0.00225269, 0.0010895 ,
       0.00136431, 0.00439227, 0.00550339, 0.        , 0.        ,
       0.        , 0.00055106, 0.00024461, 0.00015076, 0.        ,
       0.00228246, 0.00054126, 0.00033782, 0.00742861, 0.00218853,
       0.0020289 , 0.00069157, 0.00200372, 0.00203358, 0.        ,
       0.02446032, 0.00610313, 0.        , 0.00020634, 0.01059096,
       0.        , 0.        , 0.00362905, 0.00788131, 0.00101247,
       0.00183168, 0.        , 0.00870923, 0.00384349, 0.0041427 ,
       0.00059215, 0.00640956, 0.00932445, 0.00580515, 0.00051112,
       0.00276995, 0.00219074, 0.00062904, 0.00016374, 0.00634856,
       0.        , 0.00016237, 0.00064674, 0.00732062, 0.00805046,
       0.00402954, 0.01189799, 0.00729813, 0.00387581, 0.01499411,
       0.00557747, 0.00221863, 0.0050854 , 0.00272327, 0.00343799,
       0.        , 0.        , 0.01224205, 0.00358538, 0.00804907,
       0.00085206, 0.        , 0.00661812, 0.00313476, 0.02536641,
       0.00176941, 0.00298706, 0.00188392, 0.        , 0.00024215,
       0.00500309, 0.01916301, 0.00478735, 0.01675353, 0.00545705,
       0.00037433, 0.00141796, 0.00066505, 0.00426148, 0.        ,
       0.00622305, 0.0015506 , 0.00312891, 0.00326058, 0.00303908,
       0.00291058, 0.00473246, 0.00268429, 0.00274314, 0.00184242,
       0.        , 0.01982581, 0.        , 0.00056385, 0.00092455,
       0.00682685, 0.00076692, 0.0033004 , 0.        , 0.0029735 ,
       0.        , 0.00936719, 0.0020145 , 0.00338961, 0.        ,
       0.        , 0.00049165, 0.00377691, 0.        , 0.        ,
       0.00105287, 0.        , 0.        , 0.        , 0.00624775,
       0.00380282, 0.00038316, 0.00188391, 0.0044992 , 0.        ,
       0.00019712, 0.00327476, 0.00428442, 0.00075369, 0.00435744,
       0.00168185, 0.00152978, 0.00350907, 0.00558369, 0.0012877 ,
       0.        , 0.        , 0.00172948, 0.00310336, 0.00360966,
       0.00110935, 0.00353123, 0.00621531, 0.00030777, 0.00319475,
       0.00285107, 0.00042495, 0.        , 0.00588864, 0.0025834 ,
       0.00042573, 0.00547349, 0.01225728, 0.00093283, 0.00251829,
       0.00424558, 0.00331412, 0.        , 0.00039287, 0.00379302,
       0.00012404, 0.        , 0.        , 0.02031844, 0.00785684,
       0.01087877, 0.00059541, 0.00029033, 0.01504149, 0.00427135,
       0.0013243 , 0.0009737 , 0.        , 0.        , 0.        ,
       0.        , 0.01423712, 0.        , 0.00368967, 0.        ,
       0.00032842, 0.00198661, 0.        , 0.00040898, 0.        ,
       0.00114307, 0.00960655, 0.00654305, 0.0017055 , 0.00204794,
       0.00277679, 0.00197902, 0.0101966 , 0.        , 0.        ,
       0.00208284, 0.02160638, 0.        , 0.01458514, 0.0017923 ,
       0.00846121, 0.00987756, 0.00112644, 0.        , 0.00314657,
       0.00220868, 0.00295493, 0.00069751, 0.00421588, 0.00098174,
       0.00373842, 0.00246902, 0.00595718, 0.01405488, 0.00223105,
       0.        , 0.00227375, 0.01104987, 0.00499865, 0.00057527,
       0.00227082, 0.00230033, 0.00478871, 0.00070394, 0.00083005,
       0.        , 0.        , 0.00047013, 0.00504844, 0.        ,
       0.        , 0.        , 0.        ], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sort</span><span class="p">(</span><span class="n">mean_thresholds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
       2.4807403e-05, 9.2603113e-05, 1.2201311e-04, 1.4553152e-04,
       1.7821041e-04, 2.0421520e-04, 2.3708488e-04, 2.5769960e-04,
       3.4623471e-04, 3.5140055e-04, 3.5446422e-04, 3.6158794e-04,
       4.2970394e-04, 4.5851548e-04, 4.8149918e-04, 4.9150048e-04,
       4.9728854e-04, 5.4547004e-04, 7.8151998e-04, 8.1732019e-04,
       8.5373706e-04, 8.6475525e-04, 8.6866005e-04, 8.8158331e-04,
       8.9039432e-04, 9.0369058e-04, 9.1344363e-04, 9.4012078e-04,
       9.5615600e-04, 9.7568036e-04, 9.8710356e-04, 9.9498383e-04,
       1.0169239e-03, 1.1024295e-03, 1.1167353e-03, 1.1615809e-03,
       1.1777276e-03, 1.2615257e-03, 1.2897577e-03, 1.3064449e-03,
       1.3323079e-03, 1.3330684e-03, 1.3511908e-03, 1.3642724e-03,
       1.3807561e-03, 1.3868576e-03, 1.4685381e-03, 1.4953148e-03,
       1.5028402e-03, 1.5364352e-03, 1.5631035e-03, 1.5659448e-03,
       1.5781407e-03, 1.5954627e-03, 1.6222114e-03, 1.6240652e-03,
       1.6414355e-03, 1.7201712e-03, 1.7235413e-03, 1.7542432e-03,
       1.7681910e-03, 1.7813053e-03, 1.8005635e-03, 1.8313651e-03,
       1.8365972e-03, 1.8497786e-03, 1.9066151e-03, 1.9236099e-03,
       1.9314380e-03, 1.9347488e-03, 1.9369448e-03, 1.9633416e-03,
       1.9756607e-03, 1.9861136e-03, 1.9898624e-03, 2.0697629e-03,
       2.0814191e-03, 2.1284015e-03, 2.1320728e-03, 2.1429136e-03,
       2.1670745e-03, 2.1706668e-03, 2.2114222e-03, 2.2201750e-03,
       2.2202167e-03, 2.2452956e-03, 2.2459310e-03, 2.2623290e-03,
       2.3165450e-03, 2.3225809e-03, 2.3368497e-03, 2.3412288e-03,
       2.3550848e-03, 2.3551148e-03, 2.3754747e-03, 2.3805215e-03,
       2.4237661e-03, 2.4441653e-03, 2.4620402e-03, 2.4622693e-03,
       2.4684838e-03, 2.4691173e-03, 2.5054249e-03, 2.5205486e-03,
       2.5213566e-03, 2.5238642e-03, 2.5700633e-03, 2.5719195e-03,
       2.5828704e-03, 2.6004009e-03, 2.6138742e-03, 2.6603041e-03,
       2.6612976e-03, 2.7030245e-03, 2.7218941e-03, 2.7698686e-03,
       2.7774889e-03, 2.7806293e-03, 2.8009866e-03, 2.8763439e-03,
       2.8859759e-03, 2.9055406e-03, 2.9262672e-03, 2.9365176e-03,
       2.9623960e-03, 2.9759507e-03, 3.0094665e-03, 3.0129806e-03,
       3.1019538e-03, 3.1050905e-03, 3.1062185e-03, 3.1079627e-03,
       3.1425294e-03, 3.1838499e-03, 3.2117204e-03, 3.2806110e-03,
       3.2931515e-03, 3.2979944e-03, 3.3055102e-03, 3.3055439e-03,
       3.3418280e-03, 3.4001158e-03, 3.4108874e-03, 3.4939225e-03,
       3.5012562e-03, 3.5160326e-03, 3.5274345e-03, 3.5302876e-03,
       3.5535232e-03, 3.6034533e-03, 3.6099579e-03, 3.6651096e-03,
       3.6953702e-03, 3.7167422e-03, 3.7674257e-03, 3.7844740e-03,
       3.8145005e-03, 3.8170691e-03, 3.8889193e-03, 3.8897097e-03,
       3.9351331e-03, 3.9492911e-03, 3.9691720e-03, 4.0480001e-03,
       4.0525165e-03, 4.0689190e-03, 4.0813498e-03, 4.1017765e-03,
       4.1137440e-03, 4.1476903e-03, 4.1959914e-03, 4.2270222e-03,
       4.2569204e-03, 4.2844154e-03, 4.2895176e-03, 4.3021268e-03,
       4.3194206e-03, 4.3433318e-03, 4.3589738e-03, 4.3623955e-03,
       4.3915962e-03, 4.4192541e-03, 4.4195233e-03, 4.4209147e-03,
       4.5007579e-03, 4.5012543e-03, 4.5192456e-03, 4.5197969e-03,
       4.5248419e-03, 4.6563195e-03, 4.6604564e-03, 4.7029806e-03,
       4.7381790e-03, 4.7475072e-03, 4.7586956e-03, 4.7868779e-03,
       4.8198057e-03, 4.8268926e-03, 4.9276124e-03, 4.9346657e-03,
       4.9531134e-03, 4.9712961e-03, 4.9812533e-03, 5.0116461e-03,
       5.0204038e-03, 5.0330302e-03, 5.0330558e-03, 5.0399941e-03,
       5.0419485e-03, 5.1075234e-03, 5.1384782e-03, 5.1880344e-03,
       5.2543324e-03, 5.2588792e-03, 5.2689658e-03, 5.2850610e-03,
       5.5074459e-03, 5.5131242e-03, 5.5205701e-03, 5.5744057e-03,
       5.6086397e-03, 5.6493795e-03, 5.6600817e-03, 5.6936308e-03,
       5.7265712e-03, 5.7440856e-03, 5.7652937e-03, 5.7790265e-03,
       5.8480958e-03, 5.8598397e-03, 5.9667174e-03, 6.0871830e-03,
       6.0972925e-03, 6.1758356e-03, 6.1893342e-03, 6.2193191e-03,
       6.2515447e-03, 6.2591000e-03, 6.2713213e-03, 6.3512800e-03,
       6.4081326e-03, 6.6407733e-03, 6.7058564e-03, 6.7620962e-03,
       6.8224357e-03, 6.8575479e-03, 6.8653873e-03, 7.0889913e-03,
       7.0985914e-03, 7.1418025e-03, 7.4696601e-03, 7.4880859e-03,
       7.5378953e-03, 7.5899512e-03, 7.6684514e-03, 7.8933528e-03,
       8.0094570e-03, 8.0583766e-03, 8.2014501e-03, 8.3269104e-03,
       8.3945338e-03, 8.8079553e-03, 9.2405835e-03, 9.9227112e-03,
       1.0656632e-02, 1.0746715e-02, 1.2475629e-02, 5.3272538e-02],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">=</span><span class="p">[]</span>
<span class="c1"># split data into train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="c1"># fit model on all training data</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">for</span> <span class="n">thresh</span> <span class="ow">in</span> <span class="n">mean_thresholds</span><span class="p">:</span>
    <span class="c1"># select features using threshold</span>
    <span class="n">selection</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">thresh</span><span class="p">,</span> <span class="n">prefit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">select_X_train</span> <span class="o">=</span> <span class="n">selection</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="c1"># train model</span>
    <span class="n">selection_model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
    <span class="n">selection_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">select_X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c1"># eval model</span>
    <span class="n">select_X_test</span> <span class="o">=</span> <span class="n">selection</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">selection_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">select_X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">thresh</span><span class="p">,</span> <span class="n">select_X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">accuracy</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_cols</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">mean_thresholds</span><span class="o">&gt;=</span><span class="mf">0.001148</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_cols</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;sex_female&#39;, &#39;sex_male&#39;], dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;features_xgb_during_ts&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>   <span class="c1">#Pickling </span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">selected_cols</span><span class="p">,</span> <span class="n">fp</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cols&#39;</span><span class="p">])</span>
<span class="n">features</span><span class="p">[</span><span class="s1">&#39;thresh&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_thresholds</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">features</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;thresh&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cols</th>
      <th>thresh</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>287</th>
      <td>sex_male</td>
      <td>0.010655</td>
    </tr>
    <tr>
      <th>286</th>
      <td>sex_female</td>
      <td>0.002495</td>
    </tr>
    <tr>
      <th>285</th>
      <td>form_of_exercise_mix</td>
      <td>0.002149</td>
    </tr>
    <tr>
      <th>284</th>
      <td>form_of_exercise_ana</td>
      <td>0.002131</td>
    </tr>
    <tr>
      <th>283</th>
      <td>form_of_exercise_aer</td>
      <td>0.001985</td>
    </tr>
    <tr>
      <th>282</th>
      <td>time_of_day_morning</td>
      <td>0.001848</td>
    </tr>
    <tr>
      <th>281</th>
      <td>time_of_day_evening</td>
      <td>0.001762</td>
    </tr>
    <tr>
      <th>280</th>
      <td>time_of_day_afternoon</td>
      <td>0.001679</td>
    </tr>
    <tr>
      <th>279</th>
      <td>before_glc__partial_autocorrelation__lag_1</td>
      <td>0.001665</td>
    </tr>
    <tr>
      <th>278</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.001640</td>
    </tr>
    <tr>
      <th>277</th>
      <td>before_glc__approximate_entropy__m_2__r_0.5</td>
      <td>0.001612</td>
    </tr>
    <tr>
      <th>276</th>
      <td>before_glc__approximate_entropy__m_2__r_0.9</td>
      <td>0.001602</td>
    </tr>
    <tr>
      <th>275</th>
      <td>before_glc__autocorrelation__lag_1</td>
      <td>0.001579</td>
    </tr>
    <tr>
      <th>274</th>
      <td>before_glc__permutation_entropy__dimension_7__...</td>
      <td>0.001534</td>
    </tr>
    <tr>
      <th>273</th>
      <td>before_glc__approximate_entropy__m_2__r_0.3</td>
      <td>0.001518</td>
    </tr>
    <tr>
      <th>272</th>
      <td>before_glc__change_quantiles__f_agg_"var"__isa...</td>
      <td>0.001508</td>
    </tr>
    <tr>
      <th>271</th>
      <td>before_glc__change_quantiles__f_agg_"var"__isa...</td>
      <td>0.001498</td>
    </tr>
    <tr>
      <th>270</th>
      <td>before_glc__number_peaks__n_5</td>
      <td>0.001494</td>
    </tr>
    <tr>
      <th>269</th>
      <td>before_glc__cwt_coefficients__coeff_8__w_2__wi...</td>
      <td>0.001428</td>
    </tr>
    <tr>
      <th>268</th>
      <td>before_glc__fft_coefficient__attr_"real"__coeff_2</td>
      <td>0.001420</td>
    </tr>
    <tr>
      <th>267</th>
      <td>before_glc__agg_autocorrelation__f_agg_"median...</td>
      <td>0.001418</td>
    </tr>
    <tr>
      <th>266</th>
      <td>before_glc__change_quantiles__f_agg_"var"__isa...</td>
      <td>0.001373</td>
    </tr>
    <tr>
      <th>265</th>
      <td>before_glc__change_quantiles__f_agg_"var"__isa...</td>
      <td>0.001372</td>
    </tr>
    <tr>
      <th>264</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.001364</td>
    </tr>
    <tr>
      <th>263</th>
      <td>before_glc__cwt_coefficients__coeff_9__w_2__wi...</td>
      <td>0.001352</td>
    </tr>
    <tr>
      <th>262</th>
      <td>before_glc__has_duplicate_max</td>
      <td>0.001341</td>
    </tr>
    <tr>
      <th>261</th>
      <td>before_glc__agg_linear_trend__attr_"stderr"__c...</td>
      <td>0.001328</td>
    </tr>
    <tr>
      <th>260</th>
      <td>before_glc__fft_coefficient__attr_"abs"__coeff_5</td>
      <td>0.001282</td>
    </tr>
    <tr>
      <th>259</th>
      <td>before_glc__change_quantiles__f_agg_"var"__isa...</td>
      <td>0.001270</td>
    </tr>
    <tr>
      <th>258</th>
      <td>before_glc__agg_linear_trend__attr_"intercept"...</td>
      <td>0.001254</td>
    </tr>
    <tr>
      <th>257</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.001252</td>
    </tr>
    <tr>
      <th>256</th>
      <td>before_glc__cwt_coefficients__coeff_0__w_10__w...</td>
      <td>0.001250</td>
    </tr>
    <tr>
      <th>255</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.001244</td>
    </tr>
    <tr>
      <th>254</th>
      <td>before_glc__cwt_coefficients__coeff_1__w_10__w...</td>
      <td>0.001238</td>
    </tr>
    <tr>
      <th>253</th>
      <td>before_glc__number_cwt_peaks__n_1</td>
      <td>0.001235</td>
    </tr>
    <tr>
      <th>252</th>
      <td>before_glc__cwt_coefficients__coeff_2__w_5__wi...</td>
      <td>0.001219</td>
    </tr>
    <tr>
      <th>251</th>
      <td>before_glc__cwt_coefficients__coeff_3__w_5__wi...</td>
      <td>0.001217</td>
    </tr>
    <tr>
      <th>250</th>
      <td>before_glc__cwt_coefficients__coeff_4__w_5__wi...</td>
      <td>0.001193</td>
    </tr>
    <tr>
      <th>249</th>
      <td>before_glc__length</td>
      <td>0.001172</td>
    </tr>
    <tr>
      <th>248</th>
      <td>before_glc__range_count__max_1000000000000.0__...</td>
      <td>0.001170</td>
    </tr>
    <tr>
      <th>247</th>
      <td>before_glc__cwt_coefficients__coeff_1__w_5__wi...</td>
      <td>0.001156</td>
    </tr>
    <tr>
      <th>246</th>
      <td>before_glc__cwt_coefficients__coeff_0__w_5__wi...</td>
      <td>0.001153</td>
    </tr>
    <tr>
      <th>245</th>
      <td>before_glc__agg_linear_trend__attr_"stderr"__c...</td>
      <td>0.001149</td>
    </tr>
    <tr>
      <th>244</th>
      <td>before_glc__cwt_coefficients__coeff_2__w_10__w...</td>
      <td>0.001145</td>
    </tr>
    <tr>
      <th>243</th>
      <td>before_glc__longest_strike_above_mean</td>
      <td>0.001139</td>
    </tr>
    <tr>
      <th>242</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.001132</td>
    </tr>
    <tr>
      <th>241</th>
      <td>before_glc__count_above_mean</td>
      <td>0.001130</td>
    </tr>
    <tr>
      <th>240</th>
      <td>before_glc__fft_coefficient__attr_"abs"__coeff_6</td>
      <td>0.001122</td>
    </tr>
    <tr>
      <th>239</th>
      <td>before_glc__cwt_coefficients__coeff_7__w_5__wi...</td>
      <td>0.001115</td>
    </tr>
    <tr>
      <th>238</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.001104</td>
    </tr>
    <tr>
      <th>237</th>
      <td>before_glc__cwt_coefficients__coeff_3__w_10__w...</td>
      <td>0.001103</td>
    </tr>
    <tr>
      <th>236</th>
      <td>before_glc__fft_coefficient__attr_"abs"__coeff_8</td>
      <td>0.001101</td>
    </tr>
    <tr>
      <th>235</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.001057</td>
    </tr>
    <tr>
      <th>234</th>
      <td>before_glc__cwt_coefficients__coeff_4__w_10__w...</td>
      <td>0.001054</td>
    </tr>
    <tr>
      <th>233</th>
      <td>before_glc__fft_coefficient__attr_"abs"__coeff_7</td>
      <td>0.001052</td>
    </tr>
    <tr>
      <th>232</th>
      <td>before_glc__agg_linear_trend__attr_"stderr"__c...</td>
      <td>0.001051</td>
    </tr>
    <tr>
      <th>231</th>
      <td>before_glc__cwt_coefficients__coeff_8__w_5__wi...</td>
      <td>0.001038</td>
    </tr>
    <tr>
      <th>230</th>
      <td>before_glc__agg_linear_trend__attr_"stderr"__c...</td>
      <td>0.001028</td>
    </tr>
    <tr>
      <th>229</th>
      <td>before_glc__agg_linear_trend__attr_"stderr"__c...</td>
      <td>0.001022</td>
    </tr>
    <tr>
      <th>228</th>
      <td>before_glc__agg_linear_trend__attr_"stderr"__c...</td>
      <td>0.001008</td>
    </tr>
    <tr>
      <th>227</th>
      <td>before_glc__cwt_coefficients__coeff_5__w_10__w...</td>
      <td>0.001008</td>
    </tr>
    <tr>
      <th>226</th>
      <td>before_glc__fft_coefficient__attr_"real"__coeff_3</td>
      <td>0.001007</td>
    </tr>
    <tr>
      <th>225</th>
      <td>before_glc__cwt_coefficients__coeff_6__w_10__w...</td>
      <td>0.001007</td>
    </tr>
    <tr>
      <th>224</th>
      <td>before_glc__mean_second_derivative_central</td>
      <td>0.001004</td>
    </tr>
    <tr>
      <th>223</th>
      <td>before_glc__cwt_coefficients__coeff_12__w_2__w...</td>
      <td>0.001002</td>
    </tr>
    <tr>
      <th>222</th>
      <td>before_glc__cwt_coefficients__coeff_7__w_10__w...</td>
      <td>0.000996</td>
    </tr>
    <tr>
      <th>221</th>
      <td>before_glc__cwt_coefficients__coeff_9__w_5__wi...</td>
      <td>0.000994</td>
    </tr>
    <tr>
      <th>220</th>
      <td>before_glc__cwt_coefficients__coeff_8__w_10__w...</td>
      <td>0.000991</td>
    </tr>
    <tr>
      <th>219</th>
      <td>before_glc__fft_coefficient__attr_"real"__coeff_4</td>
      <td>0.000987</td>
    </tr>
    <tr>
      <th>218</th>
      <td>before_glc__cwt_coefficients__coeff_9__w_10__w...</td>
      <td>0.000986</td>
    </tr>
    <tr>
      <th>217</th>
      <td>before_glc__cwt_coefficients__coeff_10__w_10__...</td>
      <td>0.000965</td>
    </tr>
    <tr>
      <th>216</th>
      <td>before_glc__cwt_coefficients__coeff_10__w_5__w...</td>
      <td>0.000964</td>
    </tr>
    <tr>
      <th>215</th>
      <td>before_glc__cwt_coefficients__coeff_11__w_10__...</td>
      <td>0.000957</td>
    </tr>
    <tr>
      <th>214</th>
      <td>before_glc__cwt_coefficients__coeff_12__w_10__...</td>
      <td>0.000952</td>
    </tr>
    <tr>
      <th>213</th>
      <td>before_glc__cwt_coefficients__coeff_14__w_10__...</td>
      <td>0.000950</td>
    </tr>
    <tr>
      <th>212</th>
      <td>before_glc__cwt_coefficients__coeff_13__w_10__...</td>
      <td>0.000948</td>
    </tr>
    <tr>
      <th>211</th>
      <td>before_glc__cwt_coefficients__coeff_11__w_5__w...</td>
      <td>0.000941</td>
    </tr>
    <tr>
      <th>210</th>
      <td>before_glc__cwt_coefficients__coeff_12__w_5__w...</td>
      <td>0.000932</td>
    </tr>
    <tr>
      <th>209</th>
      <td>before_glc__cwt_coefficients__coeff_13__w_2__w...</td>
      <td>0.000931</td>
    </tr>
    <tr>
      <th>208</th>
      <td>before_glc__cwt_coefficients__coeff_13__w_5__w...</td>
      <td>0.000905</td>
    </tr>
    <tr>
      <th>207</th>
      <td>before_glc__cwt_coefficients__coeff_14__w_5__w...</td>
      <td>0.000904</td>
    </tr>
    <tr>
      <th>206</th>
      <td>before_glc__cwt_coefficients__coeff_14__w_2__w...</td>
      <td>0.000904</td>
    </tr>
    <tr>
      <th>205</th>
      <td>before_glc__lempel_ziv_complexity__bins_100</td>
      <td>0.000900</td>
    </tr>
    <tr>
      <th>204</th>
      <td>before_glc__lempel_ziv_complexity__bins_3</td>
      <td>0.000900</td>
    </tr>
    <tr>
      <th>203</th>
      <td>before_glc__agg_linear_trend__attr_"intercept"...</td>
      <td>0.000884</td>
    </tr>
    <tr>
      <th>202</th>
      <td>before_glc__cwt_coefficients__coeff_5__w_5__wi...</td>
      <td>0.000884</td>
    </tr>
    <tr>
      <th>201</th>
      <td>before_glc__cwt_coefficients__coeff_6__w_5__wi...</td>
      <td>0.000884</td>
    </tr>
    <tr>
      <th>200</th>
      <td>before_glc__lempel_ziv_complexity__bins_5</td>
      <td>0.000878</td>
    </tr>
    <tr>
      <th>199</th>
      <td>before_glc__energy_ratio_by_chunks__num_segmen...</td>
      <td>0.000872</td>
    </tr>
    <tr>
      <th>198</th>
      <td>before_glc__index_mass_quantile__q_0.1</td>
      <td>0.000872</td>
    </tr>
    <tr>
      <th>197</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000869</td>
    </tr>
    <tr>
      <th>196</th>
      <td>before_glc__variation_coefficient</td>
      <td>0.000864</td>
    </tr>
    <tr>
      <th>195</th>
      <td>before_glc__agg_linear_trend__attr_"intercept"...</td>
      <td>0.000860</td>
    </tr>
    <tr>
      <th>194</th>
      <td>before_glc__cwt_coefficients__coeff_1__w_2__wi...</td>
      <td>0.000858</td>
    </tr>
    <tr>
      <th>193</th>
      <td>before_glc__energy_ratio_by_chunks__num_segmen...</td>
      <td>0.000857</td>
    </tr>
    <tr>
      <th>192</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000851</td>
    </tr>
    <tr>
      <th>191</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000845</td>
    </tr>
    <tr>
      <th>190</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000839</td>
    </tr>
    <tr>
      <th>189</th>
      <td>before_glc__agg_linear_trend__attr_"intercept"...</td>
      <td>0.000830</td>
    </tr>
    <tr>
      <th>188</th>
      <td>before_glc__cwt_coefficients__coeff_4__w_2__wi...</td>
      <td>0.000823</td>
    </tr>
    <tr>
      <th>187</th>
      <td>before_glc__cwt_coefficients__coeff_2__w_2__wi...</td>
      <td>0.000820</td>
    </tr>
    <tr>
      <th>186</th>
      <td>before_glc__fft_coefficient__attr_"real"__coeff_5</td>
      <td>0.000816</td>
    </tr>
    <tr>
      <th>185</th>
      <td>before_glc__energy_ratio_by_chunks__num_segmen...</td>
      <td>0.000814</td>
    </tr>
    <tr>
      <th>184</th>
      <td>before_glc__cwt_coefficients__coeff_3__w_2__wi...</td>
      <td>0.000811</td>
    </tr>
    <tr>
      <th>183</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000810</td>
    </tr>
    <tr>
      <th>182</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_1</td>
      <td>0.000794</td>
    </tr>
    <tr>
      <th>181</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000790</td>
    </tr>
    <tr>
      <th>180</th>
      <td>before_glc__energy_ratio_by_chunks__num_segmen...</td>
      <td>0.000787</td>
    </tr>
    <tr>
      <th>179</th>
      <td>before_glc__index_mass_quantile__q_0.2</td>
      <td>0.000778</td>
    </tr>
    <tr>
      <th>178</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000778</td>
    </tr>
    <tr>
      <th>177</th>
      <td>before_glc__energy_ratio_by_chunks__num_segmen...</td>
      <td>0.000763</td>
    </tr>
    <tr>
      <th>176</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000763</td>
    </tr>
    <tr>
      <th>175</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000757</td>
    </tr>
    <tr>
      <th>174</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000753</td>
    </tr>
    <tr>
      <th>173</th>
      <td>before_glc__cwt_coefficients__coeff_0__w_20__w...</td>
      <td>0.000743</td>
    </tr>
    <tr>
      <th>172</th>
      <td>before_glc__cwt_coefficients__coeff_1__w_20__w...</td>
      <td>0.000739</td>
    </tr>
    <tr>
      <th>171</th>
      <td>before_glc__cwt_coefficients__coeff_2__w_20__w...</td>
      <td>0.000733</td>
    </tr>
    <tr>
      <th>170</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000722</td>
    </tr>
    <tr>
      <th>169</th>
      <td>before_glc__cwt_coefficients__coeff_3__w_20__w...</td>
      <td>0.000721</td>
    </tr>
    <tr>
      <th>168</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000711</td>
    </tr>
    <tr>
      <th>167</th>
      <td>before_glc__index_mass_quantile__q_0.3</td>
      <td>0.000706</td>
    </tr>
    <tr>
      <th>166</th>
      <td>before_glc__cwt_coefficients__coeff_4__w_20__w...</td>
      <td>0.000705</td>
    </tr>
    <tr>
      <th>165</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000703</td>
    </tr>
    <tr>
      <th>164</th>
      <td>before_glc__cwt_coefficients__coeff_5__w_20__w...</td>
      <td>0.000700</td>
    </tr>
    <tr>
      <th>163</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000699</td>
    </tr>
    <tr>
      <th>162</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000682</td>
    </tr>
    <tr>
      <th>161</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_2</td>
      <td>0.000680</td>
    </tr>
    <tr>
      <th>160</th>
      <td>before_glc__cwt_coefficients__coeff_6__w_20__w...</td>
      <td>0.000668</td>
    </tr>
    <tr>
      <th>159</th>
      <td>before_glc__cwt_coefficients__coeff_7__w_20__w...</td>
      <td>0.000661</td>
    </tr>
    <tr>
      <th>158</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000661</td>
    </tr>
    <tr>
      <th>157</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000660</td>
    </tr>
    <tr>
      <th>156</th>
      <td>before_glc__cwt_coefficients__coeff_8__w_20__w...</td>
      <td>0.000659</td>
    </tr>
    <tr>
      <th>155</th>
      <td>before_glc__fft_coefficient__attr_"real"__coeff_6</td>
      <td>0.000656</td>
    </tr>
    <tr>
      <th>154</th>
      <td>before_glc__index_mass_quantile__q_0.4</td>
      <td>0.000642</td>
    </tr>
    <tr>
      <th>153</th>
      <td>before_glc__cwt_coefficients__coeff_10__w_20__...</td>
      <td>0.000637</td>
    </tr>
    <tr>
      <th>152</th>
      <td>before_glc__cwt_coefficients__coeff_9__w_20__w...</td>
      <td>0.000629</td>
    </tr>
    <tr>
      <th>151</th>
      <td>before_glc__cwt_coefficients__coeff_11__w_20__...</td>
      <td>0.000622</td>
    </tr>
    <tr>
      <th>150</th>
      <td>before_glc__linear_trend__attr_"slope"</td>
      <td>0.000621</td>
    </tr>
    <tr>
      <th>149</th>
      <td>before_glc__last_location_of_maximum</td>
      <td>0.000621</td>
    </tr>
    <tr>
      <th>148</th>
      <td>before_glc__agg_linear_trend__attr_"rvalue"__c...</td>
      <td>0.000620</td>
    </tr>
    <tr>
      <th>147</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000603</td>
    </tr>
    <tr>
      <th>146</th>
      <td>before_glc__first_location_of_maximum</td>
      <td>0.000602</td>
    </tr>
    <tr>
      <th>145</th>
      <td>before_glc__cwt_coefficients__coeff_12__w_20__...</td>
      <td>0.000595</td>
    </tr>
    <tr>
      <th>144</th>
      <td>before_glc__linear_trend__attr_"rvalue"</td>
      <td>0.000592</td>
    </tr>
    <tr>
      <th>143</th>
      <td>before_glc__time_reversal_asymmetry_statistic_...</td>
      <td>0.000587</td>
    </tr>
    <tr>
      <th>142</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000585</td>
    </tr>
    <tr>
      <th>141</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000581</td>
    </tr>
    <tr>
      <th>140</th>
      <td>before_glc__cwt_coefficients__coeff_13__w_20__...</td>
      <td>0.000577</td>
    </tr>
    <tr>
      <th>139</th>
      <td>before_glc__agg_linear_trend__attr_"rvalue"__c...</td>
      <td>0.000575</td>
    </tr>
    <tr>
      <th>138</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000560</td>
    </tr>
    <tr>
      <th>137</th>
      <td>before_glc__cwt_coefficients__coeff_14__w_20__...</td>
      <td>0.000556</td>
    </tr>
    <tr>
      <th>136</th>
      <td>before_glc__index_mass_quantile__q_0.6</td>
      <td>0.000555</td>
    </tr>
    <tr>
      <th>135</th>
      <td>before_glc__agg_linear_trend__attr_"rvalue"__c...</td>
      <td>0.000554</td>
    </tr>
    <tr>
      <th>134</th>
      <td>before_glc__first_location_of_minimum</td>
      <td>0.000544</td>
    </tr>
    <tr>
      <th>133</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000541</td>
    </tr>
    <tr>
      <th>132</th>
      <td>before_glc__agg_linear_trend__attr_"rvalue"__c...</td>
      <td>0.000532</td>
    </tr>
    <tr>
      <th>131</th>
      <td>before_glc__agg_linear_trend__attr_"slope"__ch...</td>
      <td>0.000532</td>
    </tr>
    <tr>
      <th>130</th>
      <td>before_glc__agg_linear_trend__attr_"slope"__ch...</td>
      <td>0.000523</td>
    </tr>
    <tr>
      <th>129</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000520</td>
    </tr>
    <tr>
      <th>128</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000517</td>
    </tr>
    <tr>
      <th>127</th>
      <td>before_glc__agg_linear_trend__attr_"rvalue"__c...</td>
      <td>0.000514</td>
    </tr>
    <tr>
      <th>126</th>
      <td>before_glc__agg_linear_trend__attr_"rvalue"__c...</td>
      <td>0.000514</td>
    </tr>
    <tr>
      <th>125</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000505</td>
    </tr>
    <tr>
      <th>124</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000504</td>
    </tr>
    <tr>
      <th>123</th>
      <td>before_glc__agg_linear_trend__attr_"slope"__ch...</td>
      <td>0.000504</td>
    </tr>
    <tr>
      <th>122</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000501</td>
    </tr>
    <tr>
      <th>121</th>
      <td>before_glc__agg_linear_trend__attr_"slope"__ch...</td>
      <td>0.000494</td>
    </tr>
    <tr>
      <th>120</th>
      <td>before_glc__agg_linear_trend__attr_"slope"__ch...</td>
      <td>0.000494</td>
    </tr>
    <tr>
      <th>119</th>
      <td>before_glc__agg_linear_trend__attr_"slope"__ch...</td>
      <td>0.000492</td>
    </tr>
    <tr>
      <th>118</th>
      <td>before_glc__time_reversal_asymmetry_statistic_...</td>
      <td>0.000492</td>
    </tr>
    <tr>
      <th>117</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000489</td>
    </tr>
    <tr>
      <th>116</th>
      <td>before_glc__energy_ratio_by_chunks__num_segmen...</td>
      <td>0.000485</td>
    </tr>
    <tr>
      <th>115</th>
      <td>before_glc__index_mass_quantile__q_0.7</td>
      <td>0.000476</td>
    </tr>
    <tr>
      <th>114</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000475</td>
    </tr>
    <tr>
      <th>113</th>
      <td>before_glc__fft_coefficient__attr_"real"__coeff_7</td>
      <td>0.000471</td>
    </tr>
    <tr>
      <th>112</th>
      <td>before_glc__last_location_of_minimum</td>
      <td>0.000471</td>
    </tr>
    <tr>
      <th>111</th>
      <td>before_glc__fft_coefficient__attr_"real"__coeff_8</td>
      <td>0.000468</td>
    </tr>
    <tr>
      <th>110</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_4</td>
      <td>0.000467</td>
    </tr>
    <tr>
      <th>109</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000465</td>
    </tr>
    <tr>
      <th>108</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000463</td>
    </tr>
    <tr>
      <th>107</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000452</td>
    </tr>
    <tr>
      <th>106</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000449</td>
    </tr>
    <tr>
      <th>105</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000449</td>
    </tr>
    <tr>
      <th>104</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000444</td>
    </tr>
    <tr>
      <th>103</th>
      <td>before_glc__time_reversal_asymmetry_statistic_...</td>
      <td>0.000444</td>
    </tr>
    <tr>
      <th>102</th>
      <td>before_glc__benford_correlation</td>
      <td>0.000442</td>
    </tr>
    <tr>
      <th>101</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000434</td>
    </tr>
    <tr>
      <th>100</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000433</td>
    </tr>
    <tr>
      <th>99</th>
      <td>before_glc__index_mass_quantile__q_0.8</td>
      <td>0.000429</td>
    </tr>
    <tr>
      <th>98</th>
      <td>before_glc__fft_coefficient__attr_"real"__coeff_9</td>
      <td>0.000426</td>
    </tr>
    <tr>
      <th>97</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000426</td>
    </tr>
    <tr>
      <th>96</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000416</td>
    </tr>
    <tr>
      <th>95</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000414</td>
    </tr>
    <tr>
      <th>94</th>
      <td>before_glc__sum_of_reoccurring_data_points</td>
      <td>0.000398</td>
    </tr>
    <tr>
      <th>93</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_3</td>
      <td>0.000397</td>
    </tr>
    <tr>
      <th>92</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000395</td>
    </tr>
    <tr>
      <th>91</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000393</td>
    </tr>
    <tr>
      <th>90</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000387</td>
    </tr>
    <tr>
      <th>89</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000387</td>
    </tr>
    <tr>
      <th>88</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000386</td>
    </tr>
    <tr>
      <th>87</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000385</td>
    </tr>
    <tr>
      <th>86</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000381</td>
    </tr>
    <tr>
      <th>85</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000370</td>
    </tr>
    <tr>
      <th>84</th>
      <td>before_glc__sum_of_reoccurring_values</td>
      <td>0.000367</td>
    </tr>
    <tr>
      <th>83</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000366</td>
    </tr>
    <tr>
      <th>82</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000360</td>
    </tr>
    <tr>
      <th>81</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000356</td>
    </tr>
    <tr>
      <th>80</th>
      <td>before_glc__change_quantiles__f_agg_"mean"__is...</td>
      <td>0.000354</td>
    </tr>
    <tr>
      <th>79</th>
      <td>before_glc__mean_change</td>
      <td>0.000351</td>
    </tr>
    <tr>
      <th>78</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000345</td>
    </tr>
    <tr>
      <th>77</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000344</td>
    </tr>
    <tr>
      <th>76</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_6</td>
      <td>0.000328</td>
    </tr>
    <tr>
      <th>75</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000325</td>
    </tr>
    <tr>
      <th>74</th>
      <td>before_glc__fft_coefficient__attr_"angle"__coe...</td>
      <td>0.000324</td>
    </tr>
    <tr>
      <th>73</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_9</td>
      <td>0.000319</td>
    </tr>
    <tr>
      <th>72</th>
      <td>before_glc__index_mass_quantile__q_0.9</td>
      <td>0.000316</td>
    </tr>
    <tr>
      <th>71</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000313</td>
    </tr>
    <tr>
      <th>70</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_5</td>
      <td>0.000313</td>
    </tr>
    <tr>
      <th>69</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000307</td>
    </tr>
    <tr>
      <th>68</th>
      <td>before_glc__quantile__q_0.9</td>
      <td>0.000301</td>
    </tr>
    <tr>
      <th>67</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000299</td>
    </tr>
    <tr>
      <th>66</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000294</td>
    </tr>
    <tr>
      <th>65</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_7</td>
      <td>0.000277</td>
    </tr>
    <tr>
      <th>64</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000276</td>
    </tr>
    <tr>
      <th>63</th>
      <td>before_glc__quantile__q_0.6</td>
      <td>0.000273</td>
    </tr>
    <tr>
      <th>62</th>
      <td>before_glc__quantile__q_0.7</td>
      <td>0.000270</td>
    </tr>
    <tr>
      <th>61</th>
      <td>before_glc__quantile__q_0.8</td>
      <td>0.000267</td>
    </tr>
    <tr>
      <th>60</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coeff_8</td>
      <td>0.000266</td>
    </tr>
    <tr>
      <th>59</th>
      <td>before_glc__mean_n_absolute_max__number_of_max...</td>
      <td>0.000261</td>
    </tr>
    <tr>
      <th>58</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000258</td>
    </tr>
    <tr>
      <th>57</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000252</td>
    </tr>
    <tr>
      <th>56</th>
      <td>before_glc__fft_coefficient__attr_"real"__coef...</td>
      <td>0.000236</td>
    </tr>
    <tr>
      <th>55</th>
      <td>before_glc__median</td>
      <td>0.000232</td>
    </tr>
    <tr>
      <th>54</th>
      <td>before_glc__quantile__q_0.4</td>
      <td>0.000223</td>
    </tr>
    <tr>
      <th>53</th>
      <td>before_glc__energy_ratio_by_chunks__num_segmen...</td>
      <td>0.000220</td>
    </tr>
    <tr>
      <th>52</th>
      <td>before_glc__c3__lag_3</td>
      <td>0.000203</td>
    </tr>
    <tr>
      <th>51</th>
      <td>before_glc__maximum</td>
      <td>0.000199</td>
    </tr>
    <tr>
      <th>50</th>
      <td>before_glc__absolute_maximum</td>
      <td>0.000197</td>
    </tr>
    <tr>
      <th>49</th>
      <td>before_glc__fft_coefficient__attr_"imag"__coef...</td>
      <td>0.000195</td>
    </tr>
    <tr>
      <th>48</th>
      <td>before_glc__c3__lag_2</td>
      <td>0.000191</td>
    </tr>
    <tr>
      <th>47</th>
      <td>before_glc__quantile__q_0.3</td>
      <td>0.000188</td>
    </tr>
    <tr>
      <th>46</th>
      <td>before_glc__c3__lag_1</td>
      <td>0.000183</td>
    </tr>
    <tr>
      <th>45</th>
      <td>before_glc__sum_values</td>
      <td>0.000181</td>
    </tr>
    <tr>
      <th>44</th>
      <td>before_glc__fft_coefficient__attr_"abs"__coeff_0</td>
      <td>0.000178</td>
    </tr>
    <tr>
      <th>43</th>
      <td>before_glc__fft_coefficient__attr_"real"__coeff_0</td>
      <td>0.000176</td>
    </tr>
    <tr>
      <th>42</th>
      <td>before_glc__abs_energy</td>
      <td>0.000174</td>
    </tr>
    <tr>
      <th>41</th>
      <td>before_glc__quantile__q_0.2</td>
      <td>0.000173</td>
    </tr>
    <tr>
      <th>40</th>
      <td>before_glc__root_mean_square</td>
      <td>0.000171</td>
    </tr>
    <tr>
      <th>39</th>
      <td>before_glc__mean</td>
      <td>0.000163</td>
    </tr>
    <tr>
      <th>38</th>
      <td>before_glc__quantile__q_0.1</td>
      <td>0.000156</td>
    </tr>
    <tr>
      <th>37</th>
      <td>before_glc__minimum</td>
      <td>0.000109</td>
    </tr>
    <tr>
      <th>36</th>
      <td>cpep</td>
      <td>0.000099</td>
    </tr>
    <tr>
      <th>35</th>
      <td>hba1c</td>
      <td>0.000098</td>
    </tr>
    <tr>
      <th>34</th>
      <td>bmi</td>
      <td>0.000096</td>
    </tr>
    <tr>
      <th>33</th>
      <td>years_since_diagnosis</td>
      <td>0.000092</td>
    </tr>
    <tr>
      <th>32</th>
      <td>age</td>
      <td>0.000086</td>
    </tr>
    <tr>
      <th>31</th>
      <td>start_glc</td>
      <td>0.000072</td>
    </tr>
    <tr>
      <th>30</th>
      <td>duration</td>
      <td>0.000071</td>
    </tr>
    <tr>
      <th>29</th>
      <td>day_of_week</td>
      <td>0.000070</td>
    </tr>
    <tr>
      <th>28</th>
      <td>day</td>
      <td>0.000069</td>
    </tr>
    <tr>
      <th>27</th>
      <td>month</td>
      <td>0.000052</td>
    </tr>
    <tr>
      <th>26</th>
      <td>intensity</td>
      <td>0.000047</td>
    </tr>
    <tr>
      <th>25</th>
      <td>before_percent_missing</td>
      <td>0.000041</td>
    </tr>
    <tr>
      <th>24</th>
      <td>before_ea1c</td>
      <td>0.000036</td>
    </tr>
    <tr>
      <th>23</th>
      <td>before_mage_mean</td>
      <td>0.000029</td>
    </tr>
    <tr>
      <th>22</th>
      <td>before_average_glucose</td>
      <td>0.000024</td>
    </tr>
    <tr>
      <th>21</th>
      <td>before_maximum_glucose</td>
      <td>0.000019</td>
    </tr>
    <tr>
      <th>20</th>
      <td>before_minimum_glucose</td>
      <td>0.000005</td>
    </tr>
    <tr>
      <th>7</th>
      <td>before_TIR_hypo_exercise</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>before_TIR_lv1_hyper</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>before_TIR_normal_exercise</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>before_TIR_lv2_hyper</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>10</th>
      <td>before_number_hypos</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>before_TIR_hyper</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>before_TIR_norm</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>before_TIR_hypo</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>before_TIR_hyper_exercise</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>before_TIR_lv1_hypo</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>11</th>
      <td>before_avg_length_of_hypo</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>12</th>
      <td>before_total_time_in_hypos</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>13</th>
      <td>before_number_lv1_hypos</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>14</th>
      <td>before_number_lv2_hypos</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>15</th>
      <td>before_number_hypos_below_5</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>16</th>
      <td>before_avg_length_hypo_below_5</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>17</th>
      <td>before_total_time_in_hypos_below_5</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>18</th>
      <td>before_sd</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>19</th>
      <td>before_cv</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>0</th>
      <td>before_TIR_lv2_hypo</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">features</span><span class="p">[</span><span class="n">features</span><span class="o">.</span><span class="n">thresh</span><span class="o">&gt;=</span><span class="mf">0.001148</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;thresh&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;features_xgb_during_ts.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thresholds</span> <span class="o">=</span> <span class="n">sort</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># split data into train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># fit model on all training data</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># make predictions for test data and evaluate</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">roc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">roc</span> <span class="o">*</span> <span class="mf">100.0</span><span class="p">))</span>
<span class="c1"># Fit model using each importance as a threshold</span>
<span class="n">thresholds</span> <span class="o">=</span> <span class="n">sort</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[16:34:30] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
Accuracy: 89.91%
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thresholds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.52423434e-05,
       1.29362859e-04, 1.59985546e-04, 1.92742853e-04, 2.11246603e-04,
       2.16641289e-04, 2.45755626e-04, 2.80866749e-04, 3.10661941e-04,
       3.13753029e-04, 3.22905020e-04, 3.63932224e-04, 3.64315463e-04,
       3.79420962e-04, 4.15464601e-04, 4.16237570e-04, 4.21166624e-04,
       4.37819253e-04, 4.49889019e-04, 4.82455536e-04, 4.85675439e-04,
       4.93507541e-04, 4.99735179e-04, 5.06505778e-04, 5.59247448e-04,
       5.80141728e-04, 5.81527886e-04, 5.83336747e-04, 5.98195591e-04,
       6.36937795e-04, 6.38626225e-04, 6.60882739e-04, 6.92158646e-04,
       7.03661237e-04, 7.06305902e-04, 7.32373795e-04, 7.63347547e-04,
       8.07442237e-04, 8.16059473e-04, 8.37071741e-04, 8.47318734e-04,
       8.73333483e-04, 8.78104474e-04, 8.85103480e-04, 1.01657794e-03,
       1.04717654e-03, 1.07868982e-03, 1.09086616e-03, 1.14768196e-03,
       1.22005586e-03, 1.34729978e-03, 1.36161037e-03, 1.43593003e-03,
       1.45374262e-03, 1.48800819e-03, 1.53208023e-03, 1.54934218e-03,
       1.58719346e-03, 1.63619150e-03, 1.64775830e-03, 1.67493511e-03,
       1.67545467e-03, 1.72428589e-03, 1.73336919e-03, 1.73580437e-03,
       1.77408196e-03, 1.79221854e-03, 1.80338463e-03, 1.80759770e-03,
       1.84874109e-03, 1.87000097e-03, 1.88076915e-03, 1.89762318e-03,
       1.93823501e-03, 1.99075695e-03, 1.99593417e-03, 2.03091651e-03,
       2.05900962e-03, 2.08020792e-03, 2.11589108e-03, 2.11723358e-03,
       2.21706321e-03, 2.23902310e-03, 2.31557759e-03, 2.35444633e-03,
       2.38089520e-03, 2.39400310e-03, 2.40636780e-03, 2.44291243e-03,
       2.49806931e-03, 2.51353462e-03, 2.55812518e-03, 2.57476396e-03,
       2.62272637e-03, 2.65048072e-03, 2.65325094e-03, 2.71690497e-03,
       2.72179791e-03, 2.80090235e-03, 2.83905515e-03, 2.89599807e-03,
       2.91101891e-03, 2.92348652e-03, 2.98566604e-03, 2.99884845e-03,
       3.04021151e-03, 3.06141260e-03, 3.06812092e-03, 3.07276309e-03,
       3.11367423e-03, 3.15203681e-03, 3.25227831e-03, 3.29411286e-03,
       3.33045749e-03, 3.34083452e-03, 3.35194636e-03, 3.35702975e-03,
       3.41201480e-03, 3.41218687e-03, 3.47275636e-03, 3.47682042e-03,
       3.48401442e-03, 3.54254339e-03, 3.55522102e-03, 3.57555202e-03,
       3.58640659e-03, 3.73226893e-03, 3.74243013e-03, 3.76813137e-03,
       3.84013564e-03, 3.84346140e-03, 3.92714422e-03, 4.03854391e-03,
       4.05559083e-03, 4.15927591e-03, 4.31963429e-03, 4.39461647e-03,
       4.47852490e-03, 4.57224622e-03, 4.77150688e-03, 4.80488688e-03,
       4.83512646e-03, 4.88450611e-03, 4.94077383e-03, 5.15243923e-03,
       5.21211931e-03, 5.35981590e-03, 5.40239271e-03, 5.73603483e-03,
       5.87078230e-03, 5.97470300e-03, 5.98206976e-03, 6.00559264e-03,
       6.11538859e-03, 6.14342513e-03, 6.18690113e-03, 6.22051489e-03,
       6.36405312e-03, 6.36500912e-03, 6.52577775e-03, 6.57385867e-03,
       6.63130963e-03, 6.68972824e-03, 7.28421845e-03, 7.43870996e-03,
       7.49616837e-03, 7.66196428e-03, 7.91799370e-03, 8.02744366e-03,
       8.07141606e-03, 8.34282860e-03, 8.52511264e-03, 8.69581196e-03,
       8.76704790e-03, 8.79825465e-03, 8.84870440e-03, 8.92592221e-03,
       9.03511513e-03, 9.07676667e-03, 9.16826446e-03, 9.18324105e-03,
       9.53302439e-03, 9.70894936e-03, 9.96214803e-03, 9.97643732e-03,
       1.00158537e-02, 1.03144823e-02, 1.06301922e-02, 1.10506285e-02,
       1.11288531e-02, 1.13483258e-02, 1.13958772e-02, 1.19092297e-02,
       1.33235725e-02, 1.35994423e-02, 1.40974158e-02, 1.41359903e-02,
       1.42733343e-02, 1.53467869e-02, 1.55403158e-02, 1.66359004e-02,
       1.66996364e-02, 1.67355761e-02, 1.76407676e-02, 1.96627229e-02,
       2.03335267e-02, 2.28757486e-02, 2.39648093e-02, 5.89442924e-02],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plot_importance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">max_num_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 360x360 with 0 Axes&gt;
</pre></div>
</div>
<img alt="../../_images/d3299f36238bafe1c0d8d4e2ea84f08c60f85f20914390acae82a9723843c5a0.png" src="../../_images/d3299f36238bafe1c0d8d4e2ea84f08c60f85f20914390acae82a9723843c5a0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">thresh</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
    <span class="c1"># select features using threshold</span>
    <span class="n">selection</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">thresh</span><span class="p">,</span> <span class="n">prefit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">select_X_train</span> <span class="o">=</span> <span class="n">selection</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="c1"># train model</span>
    <span class="n">selection_model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
    <span class="n">selection_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">select_X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c1"># eval model</span>
    <span class="n">select_X_test</span> <span class="o">=</span> <span class="n">selection</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">selection_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">select_X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">roc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">thresh</span><span class="p">,</span> <span class="n">select_X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">roc</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[16:24:40] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
Accuracy: 89.91%
[16:24:40] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:41] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:41] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:44] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:44] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:45] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:46] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:46] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:47] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:47] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:48] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:49] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:50] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:50] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:51] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:52] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:53] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:53] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:54] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:55] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:56] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:57] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:57] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:58] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:59] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:24:59] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:00] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:00] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:01] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:02] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:02] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:03] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:04] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:04] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:05] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:06] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:07] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:08] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:08] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:09] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:10] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:10] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:11] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:12] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:12] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:13] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:14] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:14] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:15] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:16] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:16] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:17] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:18] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:18] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:19] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:20] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:20] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:21] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:21] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:22] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:23] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:23] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:24] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:25] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:25] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:26] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:26] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:27] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:28] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:29] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:29] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:30] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:31] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:31] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:32] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:32] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:33] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:33] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:34] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:34] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:35] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:35] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:36] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:36] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:37] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:38] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:38] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:39] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:39] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:40] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:40] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:41] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:44] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:44] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:45] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:45] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:46] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:46] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:47] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:47] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:48] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:48] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:49] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:49] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:50] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:51] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:51] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:52] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:52] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:53] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:53] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:54] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:54] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:55] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:55] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:56] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:56] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:57] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:57] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:58] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:58] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:59] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:25:59] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:00] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:00] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:01] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:01] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:02] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:02] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:02] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:03] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:03] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:04] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:04] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:04] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:05] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:05] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:06] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:06] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:07] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:07] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:08] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:08] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:08] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:09] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:09] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:09] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:10] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:10] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:10] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:11] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:11] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:11] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:12] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:12] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:13] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:13] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:14] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:14] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:14] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:15] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:15] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:15] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:16] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:16] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:16] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:17] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:17] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:17] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:18] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:18] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:18] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:18] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:19] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:19] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:19] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:20] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:20] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:20] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:21] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:21] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:21] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:21] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:22] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:22] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:22] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:23] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:23] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:23] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:24] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:24] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:24] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:24] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:25] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:25] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:25] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:26] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:26] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:26] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:26] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:27] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:27] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:27] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:27] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:28] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:28] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:28] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:29] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:29] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:29] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:29] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:30] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:30] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:30] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:30] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:31] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:31] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:31] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:32] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:32] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:32] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:32] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:33] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:33] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:33] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:33] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:34] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:34] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:34] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:34] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:35] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:35] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:35] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:36] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:36] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:36] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:36] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:36] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:37] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:37] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:37] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:37] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:38] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:38] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:38] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:38] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:38] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:39] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:39] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:39] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:39] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:40] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:40] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:40] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:40] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:40] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:41] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:41] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:41] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:41] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:41] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:42] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:43] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:44] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:44] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:44] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:44] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
[16:26:44] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;thresh&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;roc&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span><span class="c1">#.sort_values(&#39;accuracy&#39;,ascending=False)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>thresh</th>
      <th>n</th>
      <th>roc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>288</td>
      <td>0.899112</td>
    </tr>
    <tr>
      <th>79</th>
      <td>0.000095</td>
      <td>209</td>
      <td>0.899112</td>
    </tr>
    <tr>
      <th>80</th>
      <td>0.000129</td>
      <td>208</td>
      <td>0.898031</td>
    </tr>
    <tr>
      <th>81</th>
      <td>0.000160</td>
      <td>207</td>
      <td>0.900463</td>
    </tr>
    <tr>
      <th>82</th>
      <td>0.000193</td>
      <td>206</td>
      <td>0.900463</td>
    </tr>
    <tr>
      <th>83</th>
      <td>0.000211</td>
      <td>205</td>
      <td>0.900347</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.000217</td>
      <td>204</td>
      <td>0.900347</td>
    </tr>
    <tr>
      <th>85</th>
      <td>0.000246</td>
      <td>203</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>86</th>
      <td>0.000281</td>
      <td>202</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>87</th>
      <td>0.000311</td>
      <td>201</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>88</th>
      <td>0.000314</td>
      <td>200</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>89</th>
      <td>0.000323</td>
      <td>199</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>90</th>
      <td>0.000364</td>
      <td>198</td>
      <td>0.901390</td>
    </tr>
    <tr>
      <th>91</th>
      <td>0.000364</td>
      <td>197</td>
      <td>0.899035</td>
    </tr>
    <tr>
      <th>92</th>
      <td>0.000379</td>
      <td>196</td>
      <td>0.899151</td>
    </tr>
    <tr>
      <th>93</th>
      <td>0.000415</td>
      <td>195</td>
      <td>0.899151</td>
    </tr>
    <tr>
      <th>94</th>
      <td>0.000416</td>
      <td>194</td>
      <td>0.898147</td>
    </tr>
    <tr>
      <th>95</th>
      <td>0.000421</td>
      <td>193</td>
      <td>0.898147</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.000438</td>
      <td>192</td>
      <td>0.897992</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.000450</td>
      <td>191</td>
      <td>0.897722</td>
    </tr>
    <tr>
      <th>98</th>
      <td>0.000482</td>
      <td>190</td>
      <td>0.895444</td>
    </tr>
    <tr>
      <th>99</th>
      <td>0.000486</td>
      <td>189</td>
      <td>0.895869</td>
    </tr>
    <tr>
      <th>100</th>
      <td>0.000494</td>
      <td>188</td>
      <td>0.895869</td>
    </tr>
    <tr>
      <th>101</th>
      <td>0.000500</td>
      <td>187</td>
      <td>0.895792</td>
    </tr>
    <tr>
      <th>102</th>
      <td>0.000507</td>
      <td>186</td>
      <td>0.896680</td>
    </tr>
    <tr>
      <th>103</th>
      <td>0.000559</td>
      <td>185</td>
      <td>0.896680</td>
    </tr>
    <tr>
      <th>104</th>
      <td>0.000580</td>
      <td>184</td>
      <td>0.896332</td>
    </tr>
    <tr>
      <th>105</th>
      <td>0.000582</td>
      <td>183</td>
      <td>0.895830</td>
    </tr>
    <tr>
      <th>106</th>
      <td>0.000583</td>
      <td>182</td>
      <td>0.899575</td>
    </tr>
    <tr>
      <th>107</th>
      <td>0.000598</td>
      <td>181</td>
      <td>0.898571</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0.000637</td>
      <td>180</td>
      <td>0.898108</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.000639</td>
      <td>179</td>
      <td>0.897375</td>
    </tr>
    <tr>
      <th>110</th>
      <td>0.000661</td>
      <td>178</td>
      <td>0.894710</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0.000692</td>
      <td>177</td>
      <td>0.895676</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0.000704</td>
      <td>176</td>
      <td>0.894324</td>
    </tr>
    <tr>
      <th>113</th>
      <td>0.000706</td>
      <td>175</td>
      <td>0.898803</td>
    </tr>
    <tr>
      <th>114</th>
      <td>0.000732</td>
      <td>174</td>
      <td>0.897143</td>
    </tr>
    <tr>
      <th>115</th>
      <td>0.000763</td>
      <td>173</td>
      <td>0.896564</td>
    </tr>
    <tr>
      <th>116</th>
      <td>0.000807</td>
      <td>172</td>
      <td>0.896564</td>
    </tr>
    <tr>
      <th>117</th>
      <td>0.000816</td>
      <td>171</td>
      <td>0.898533</td>
    </tr>
    <tr>
      <th>118</th>
      <td>0.000837</td>
      <td>170</td>
      <td>0.893514</td>
    </tr>
    <tr>
      <th>119</th>
      <td>0.000847</td>
      <td>169</td>
      <td>0.896216</td>
    </tr>
    <tr>
      <th>120</th>
      <td>0.000873</td>
      <td>168</td>
      <td>0.896950</td>
    </tr>
    <tr>
      <th>121</th>
      <td>0.000878</td>
      <td>167</td>
      <td>0.897027</td>
    </tr>
    <tr>
      <th>122</th>
      <td>0.000885</td>
      <td>166</td>
      <td>0.899382</td>
    </tr>
    <tr>
      <th>123</th>
      <td>0.001017</td>
      <td>165</td>
      <td>0.899382</td>
    </tr>
    <tr>
      <th>124</th>
      <td>0.001047</td>
      <td>164</td>
      <td>0.895946</td>
    </tr>
    <tr>
      <th>125</th>
      <td>0.001079</td>
      <td>163</td>
      <td>0.902317</td>
    </tr>
    <tr>
      <th>126</th>
      <td>0.001091</td>
      <td>162</td>
      <td>0.902317</td>
    </tr>
    <tr>
      <th>127</th>
      <td>0.001148</td>
      <td>161</td>
      <td>0.903784</td>
    </tr>
    <tr>
      <th>128</th>
      <td>0.001220</td>
      <td>160</td>
      <td>0.897375</td>
    </tr>
    <tr>
      <th>129</th>
      <td>0.001347</td>
      <td>159</td>
      <td>0.898185</td>
    </tr>
    <tr>
      <th>130</th>
      <td>0.001362</td>
      <td>158</td>
      <td>0.895444</td>
    </tr>
    <tr>
      <th>131</th>
      <td>0.001436</td>
      <td>157</td>
      <td>0.901197</td>
    </tr>
    <tr>
      <th>132</th>
      <td>0.001454</td>
      <td>156</td>
      <td>0.898185</td>
    </tr>
    <tr>
      <th>133</th>
      <td>0.001488</td>
      <td>155</td>
      <td>0.899073</td>
    </tr>
    <tr>
      <th>134</th>
      <td>0.001532</td>
      <td>154</td>
      <td>0.899305</td>
    </tr>
    <tr>
      <th>135</th>
      <td>0.001549</td>
      <td>153</td>
      <td>0.897181</td>
    </tr>
    <tr>
      <th>136</th>
      <td>0.001587</td>
      <td>152</td>
      <td>0.897297</td>
    </tr>
    <tr>
      <th>137</th>
      <td>0.001636</td>
      <td>151</td>
      <td>0.897375</td>
    </tr>
    <tr>
      <th>138</th>
      <td>0.001648</td>
      <td>150</td>
      <td>0.904402</td>
    </tr>
    <tr>
      <th>139</th>
      <td>0.001675</td>
      <td>149</td>
      <td>0.898726</td>
    </tr>
    <tr>
      <th>140</th>
      <td>0.001675</td>
      <td>148</td>
      <td>0.900309</td>
    </tr>
    <tr>
      <th>141</th>
      <td>0.001724</td>
      <td>147</td>
      <td>0.897490</td>
    </tr>
    <tr>
      <th>142</th>
      <td>0.001733</td>
      <td>146</td>
      <td>0.897915</td>
    </tr>
    <tr>
      <th>143</th>
      <td>0.001736</td>
      <td>145</td>
      <td>0.900927</td>
    </tr>
    <tr>
      <th>144</th>
      <td>0.001774</td>
      <td>144</td>
      <td>0.900965</td>
    </tr>
    <tr>
      <th>145</th>
      <td>0.001792</td>
      <td>143</td>
      <td>0.900965</td>
    </tr>
    <tr>
      <th>146</th>
      <td>0.001803</td>
      <td>142</td>
      <td>0.902510</td>
    </tr>
    <tr>
      <th>147</th>
      <td>0.001808</td>
      <td>141</td>
      <td>0.898726</td>
    </tr>
    <tr>
      <th>148</th>
      <td>0.001849</td>
      <td>140</td>
      <td>0.899189</td>
    </tr>
    <tr>
      <th>149</th>
      <td>0.001870</td>
      <td>139</td>
      <td>0.899073</td>
    </tr>
    <tr>
      <th>150</th>
      <td>0.001881</td>
      <td>138</td>
      <td>0.898417</td>
    </tr>
    <tr>
      <th>151</th>
      <td>0.001898</td>
      <td>137</td>
      <td>0.901351</td>
    </tr>
    <tr>
      <th>152</th>
      <td>0.001938</td>
      <td>136</td>
      <td>0.897761</td>
    </tr>
    <tr>
      <th>153</th>
      <td>0.001991</td>
      <td>135</td>
      <td>0.898417</td>
    </tr>
    <tr>
      <th>154</th>
      <td>0.001996</td>
      <td>134</td>
      <td>0.900849</td>
    </tr>
    <tr>
      <th>155</th>
      <td>0.002031</td>
      <td>133</td>
      <td>0.901120</td>
    </tr>
    <tr>
      <th>156</th>
      <td>0.002059</td>
      <td>132</td>
      <td>0.900077</td>
    </tr>
    <tr>
      <th>157</th>
      <td>0.002080</td>
      <td>131</td>
      <td>0.896178</td>
    </tr>
    <tr>
      <th>158</th>
      <td>0.002116</td>
      <td>130</td>
      <td>0.894826</td>
    </tr>
    <tr>
      <th>159</th>
      <td>0.002117</td>
      <td>129</td>
      <td>0.905444</td>
    </tr>
    <tr>
      <th>160</th>
      <td>0.002217</td>
      <td>128</td>
      <td>0.898340</td>
    </tr>
    <tr>
      <th>161</th>
      <td>0.002239</td>
      <td>127</td>
      <td>0.899768</td>
    </tr>
    <tr>
      <th>162</th>
      <td>0.002316</td>
      <td>126</td>
      <td>0.902896</td>
    </tr>
    <tr>
      <th>163</th>
      <td>0.002354</td>
      <td>125</td>
      <td>0.901120</td>
    </tr>
    <tr>
      <th>164</th>
      <td>0.002381</td>
      <td>124</td>
      <td>0.902510</td>
    </tr>
    <tr>
      <th>165</th>
      <td>0.002394</td>
      <td>123</td>
      <td>0.902510</td>
    </tr>
    <tr>
      <th>166</th>
      <td>0.002406</td>
      <td>122</td>
      <td>0.899305</td>
    </tr>
    <tr>
      <th>167</th>
      <td>0.002443</td>
      <td>121</td>
      <td>0.903745</td>
    </tr>
    <tr>
      <th>168</th>
      <td>0.002498</td>
      <td>120</td>
      <td>0.905328</td>
    </tr>
    <tr>
      <th>169</th>
      <td>0.002514</td>
      <td>119</td>
      <td>0.904710</td>
    </tr>
    <tr>
      <th>170</th>
      <td>0.002558</td>
      <td>118</td>
      <td>0.895830</td>
    </tr>
    <tr>
      <th>171</th>
      <td>0.002575</td>
      <td>117</td>
      <td>0.903475</td>
    </tr>
    <tr>
      <th>172</th>
      <td>0.002623</td>
      <td>116</td>
      <td>0.905483</td>
    </tr>
    <tr>
      <th>173</th>
      <td>0.002650</td>
      <td>115</td>
      <td>0.905135</td>
    </tr>
    <tr>
      <th>174</th>
      <td>0.002653</td>
      <td>114</td>
      <td>0.904942</td>
    </tr>
    <tr>
      <th>175</th>
      <td>0.002717</td>
      <td>113</td>
      <td>0.901274</td>
    </tr>
    <tr>
      <th>176</th>
      <td>0.002722</td>
      <td>112</td>
      <td>0.901313</td>
    </tr>
    <tr>
      <th>177</th>
      <td>0.002801</td>
      <td>111</td>
      <td>0.901853</td>
    </tr>
    <tr>
      <th>178</th>
      <td>0.002839</td>
      <td>110</td>
      <td>0.899035</td>
    </tr>
    <tr>
      <th>179</th>
      <td>0.002896</td>
      <td>109</td>
      <td>0.903320</td>
    </tr>
    <tr>
      <th>180</th>
      <td>0.002911</td>
      <td>108</td>
      <td>0.900656</td>
    </tr>
    <tr>
      <th>181</th>
      <td>0.002923</td>
      <td>107</td>
      <td>0.902625</td>
    </tr>
    <tr>
      <th>182</th>
      <td>0.002986</td>
      <td>106</td>
      <td>0.899807</td>
    </tr>
    <tr>
      <th>183</th>
      <td>0.002999</td>
      <td>105</td>
      <td>0.897992</td>
    </tr>
    <tr>
      <th>184</th>
      <td>0.003040</td>
      <td>104</td>
      <td>0.904286</td>
    </tr>
    <tr>
      <th>185</th>
      <td>0.003061</td>
      <td>103</td>
      <td>0.903514</td>
    </tr>
    <tr>
      <th>186</th>
      <td>0.003068</td>
      <td>102</td>
      <td>0.896834</td>
    </tr>
    <tr>
      <th>187</th>
      <td>0.003073</td>
      <td>101</td>
      <td>0.899768</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.003114</td>
      <td>100</td>
      <td>0.896178</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.003152</td>
      <td>99</td>
      <td>0.897876</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.003252</td>
      <td>98</td>
      <td>0.894247</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.003294</td>
      <td>97</td>
      <td>0.895637</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.003330</td>
      <td>96</td>
      <td>0.896139</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.003341</td>
      <td>95</td>
      <td>0.897606</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.003352</td>
      <td>94</td>
      <td>0.898919</td>
    </tr>
    <tr>
      <th>195</th>
      <td>0.003357</td>
      <td>93</td>
      <td>0.903514</td>
    </tr>
    <tr>
      <th>196</th>
      <td>0.003412</td>
      <td>92</td>
      <td>0.900309</td>
    </tr>
    <tr>
      <th>197</th>
      <td>0.003412</td>
      <td>91</td>
      <td>0.897606</td>
    </tr>
    <tr>
      <th>198</th>
      <td>0.003473</td>
      <td>90</td>
      <td>0.894865</td>
    </tr>
    <tr>
      <th>199</th>
      <td>0.003477</td>
      <td>89</td>
      <td>0.900734</td>
    </tr>
    <tr>
      <th>200</th>
      <td>0.003484</td>
      <td>88</td>
      <td>0.897876</td>
    </tr>
    <tr>
      <th>201</th>
      <td>0.003543</td>
      <td>87</td>
      <td>0.899228</td>
    </tr>
    <tr>
      <th>202</th>
      <td>0.003555</td>
      <td>86</td>
      <td>0.900811</td>
    </tr>
    <tr>
      <th>203</th>
      <td>0.003576</td>
      <td>85</td>
      <td>0.901042</td>
    </tr>
    <tr>
      <th>204</th>
      <td>0.003586</td>
      <td>84</td>
      <td>0.901429</td>
    </tr>
    <tr>
      <th>205</th>
      <td>0.003732</td>
      <td>83</td>
      <td>0.893089</td>
    </tr>
    <tr>
      <th>206</th>
      <td>0.003742</td>
      <td>82</td>
      <td>0.892664</td>
    </tr>
    <tr>
      <th>207</th>
      <td>0.003768</td>
      <td>81</td>
      <td>0.892394</td>
    </tr>
    <tr>
      <th>208</th>
      <td>0.003840</td>
      <td>80</td>
      <td>0.893552</td>
    </tr>
    <tr>
      <th>209</th>
      <td>0.003843</td>
      <td>79</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>210</th>
      <td>0.003927</td>
      <td>78</td>
      <td>0.891737</td>
    </tr>
    <tr>
      <th>211</th>
      <td>0.004039</td>
      <td>77</td>
      <td>0.897992</td>
    </tr>
    <tr>
      <th>212</th>
      <td>0.004056</td>
      <td>76</td>
      <td>0.897143</td>
    </tr>
    <tr>
      <th>213</th>
      <td>0.004159</td>
      <td>75</td>
      <td>0.898803</td>
    </tr>
    <tr>
      <th>214</th>
      <td>0.004320</td>
      <td>74</td>
      <td>0.896853</td>
    </tr>
    <tr>
      <th>215</th>
      <td>0.004395</td>
      <td>73</td>
      <td>0.899884</td>
    </tr>
    <tr>
      <th>216</th>
      <td>0.004479</td>
      <td>72</td>
      <td>0.902780</td>
    </tr>
    <tr>
      <th>217</th>
      <td>0.004572</td>
      <td>71</td>
      <td>0.894595</td>
    </tr>
    <tr>
      <th>218</th>
      <td>0.004772</td>
      <td>70</td>
      <td>0.900792</td>
    </tr>
    <tr>
      <th>219</th>
      <td>0.004805</td>
      <td>69</td>
      <td>0.897722</td>
    </tr>
    <tr>
      <th>220</th>
      <td>0.004835</td>
      <td>68</td>
      <td>0.894286</td>
    </tr>
    <tr>
      <th>221</th>
      <td>0.004885</td>
      <td>67</td>
      <td>0.901544</td>
    </tr>
    <tr>
      <th>222</th>
      <td>0.004941</td>
      <td>66</td>
      <td>0.903398</td>
    </tr>
    <tr>
      <th>223</th>
      <td>0.005152</td>
      <td>65</td>
      <td>0.900116</td>
    </tr>
    <tr>
      <th>224</th>
      <td>0.005212</td>
      <td>64</td>
      <td>0.899344</td>
    </tr>
    <tr>
      <th>225</th>
      <td>0.005360</td>
      <td>63</td>
      <td>0.897336</td>
    </tr>
    <tr>
      <th>226</th>
      <td>0.005402</td>
      <td>62</td>
      <td>0.902973</td>
    </tr>
    <tr>
      <th>227</th>
      <td>0.005736</td>
      <td>61</td>
      <td>0.893977</td>
    </tr>
    <tr>
      <th>228</th>
      <td>0.005871</td>
      <td>60</td>
      <td>0.895483</td>
    </tr>
    <tr>
      <th>229</th>
      <td>0.005975</td>
      <td>59</td>
      <td>0.895328</td>
    </tr>
    <tr>
      <th>230</th>
      <td>0.005982</td>
      <td>58</td>
      <td>0.887413</td>
    </tr>
    <tr>
      <th>231</th>
      <td>0.006006</td>
      <td>57</td>
      <td>0.884710</td>
    </tr>
    <tr>
      <th>232</th>
      <td>0.006115</td>
      <td>56</td>
      <td>0.891081</td>
    </tr>
    <tr>
      <th>233</th>
      <td>0.006143</td>
      <td>55</td>
      <td>0.890347</td>
    </tr>
    <tr>
      <th>234</th>
      <td>0.006187</td>
      <td>54</td>
      <td>0.890849</td>
    </tr>
    <tr>
      <th>235</th>
      <td>0.006221</td>
      <td>53</td>
      <td>0.891274</td>
    </tr>
    <tr>
      <th>236</th>
      <td>0.006364</td>
      <td>52</td>
      <td>0.886950</td>
    </tr>
    <tr>
      <th>237</th>
      <td>0.006365</td>
      <td>51</td>
      <td>0.889498</td>
    </tr>
    <tr>
      <th>238</th>
      <td>0.006526</td>
      <td>50</td>
      <td>0.892664</td>
    </tr>
    <tr>
      <th>239</th>
      <td>0.006574</td>
      <td>49</td>
      <td>0.884015</td>
    </tr>
    <tr>
      <th>240</th>
      <td>0.006631</td>
      <td>48</td>
      <td>0.884170</td>
    </tr>
    <tr>
      <th>241</th>
      <td>0.006690</td>
      <td>47</td>
      <td>0.884865</td>
    </tr>
    <tr>
      <th>242</th>
      <td>0.007284</td>
      <td>46</td>
      <td>0.885753</td>
    </tr>
    <tr>
      <th>243</th>
      <td>0.007439</td>
      <td>45</td>
      <td>0.889112</td>
    </tr>
    <tr>
      <th>244</th>
      <td>0.007496</td>
      <td>44</td>
      <td>0.885560</td>
    </tr>
    <tr>
      <th>245</th>
      <td>0.007662</td>
      <td>43</td>
      <td>0.886448</td>
    </tr>
    <tr>
      <th>246</th>
      <td>0.007918</td>
      <td>42</td>
      <td>0.876795</td>
    </tr>
    <tr>
      <th>247</th>
      <td>0.008027</td>
      <td>41</td>
      <td>0.883822</td>
    </tr>
    <tr>
      <th>248</th>
      <td>0.008071</td>
      <td>40</td>
      <td>0.882857</td>
    </tr>
    <tr>
      <th>249</th>
      <td>0.008343</td>
      <td>39</td>
      <td>0.880077</td>
    </tr>
    <tr>
      <th>250</th>
      <td>0.008525</td>
      <td>38</td>
      <td>0.889691</td>
    </tr>
    <tr>
      <th>251</th>
      <td>0.008696</td>
      <td>37</td>
      <td>0.865058</td>
    </tr>
    <tr>
      <th>252</th>
      <td>0.008767</td>
      <td>36</td>
      <td>0.862587</td>
    </tr>
    <tr>
      <th>253</th>
      <td>0.008798</td>
      <td>35</td>
      <td>0.861892</td>
    </tr>
    <tr>
      <th>254</th>
      <td>0.008849</td>
      <td>34</td>
      <td>0.866178</td>
    </tr>
    <tr>
      <th>255</th>
      <td>0.008926</td>
      <td>33</td>
      <td>0.856873</td>
    </tr>
    <tr>
      <th>256</th>
      <td>0.009035</td>
      <td>32</td>
      <td>0.854595</td>
    </tr>
    <tr>
      <th>257</th>
      <td>0.009077</td>
      <td>31</td>
      <td>0.857915</td>
    </tr>
    <tr>
      <th>258</th>
      <td>0.009168</td>
      <td>30</td>
      <td>0.860695</td>
    </tr>
    <tr>
      <th>259</th>
      <td>0.009183</td>
      <td>29</td>
      <td>0.854015</td>
    </tr>
    <tr>
      <th>260</th>
      <td>0.009533</td>
      <td>28</td>
      <td>0.852201</td>
    </tr>
    <tr>
      <th>261</th>
      <td>0.009709</td>
      <td>27</td>
      <td>0.849575</td>
    </tr>
    <tr>
      <th>262</th>
      <td>0.009962</td>
      <td>26</td>
      <td>0.846680</td>
    </tr>
    <tr>
      <th>263</th>
      <td>0.009976</td>
      <td>25</td>
      <td>0.861429</td>
    </tr>
    <tr>
      <th>264</th>
      <td>0.010016</td>
      <td>24</td>
      <td>0.853398</td>
    </tr>
    <tr>
      <th>265</th>
      <td>0.010314</td>
      <td>23</td>
      <td>0.870386</td>
    </tr>
    <tr>
      <th>266</th>
      <td>0.010630</td>
      <td>22</td>
      <td>0.861699</td>
    </tr>
    <tr>
      <th>267</th>
      <td>0.011051</td>
      <td>21</td>
      <td>0.870849</td>
    </tr>
    <tr>
      <th>268</th>
      <td>0.011129</td>
      <td>20</td>
      <td>0.867201</td>
    </tr>
    <tr>
      <th>269</th>
      <td>0.011348</td>
      <td>19</td>
      <td>0.864903</td>
    </tr>
    <tr>
      <th>270</th>
      <td>0.011396</td>
      <td>18</td>
      <td>0.859112</td>
    </tr>
    <tr>
      <th>271</th>
      <td>0.011909</td>
      <td>17</td>
      <td>0.862819</td>
    </tr>
    <tr>
      <th>272</th>
      <td>0.013324</td>
      <td>16</td>
      <td>0.859112</td>
    </tr>
    <tr>
      <th>273</th>
      <td>0.013599</td>
      <td>15</td>
      <td>0.865405</td>
    </tr>
    <tr>
      <th>274</th>
      <td>0.014097</td>
      <td>14</td>
      <td>0.863089</td>
    </tr>
    <tr>
      <th>275</th>
      <td>0.014136</td>
      <td>13</td>
      <td>0.866525</td>
    </tr>
    <tr>
      <th>276</th>
      <td>0.014273</td>
      <td>12</td>
      <td>0.865753</td>
    </tr>
    <tr>
      <th>277</th>
      <td>0.015347</td>
      <td>11</td>
      <td>0.870039</td>
    </tr>
    <tr>
      <th>278</th>
      <td>0.015540</td>
      <td>10</td>
      <td>0.872066</td>
    </tr>
    <tr>
      <th>279</th>
      <td>0.016636</td>
      <td>9</td>
      <td>0.872104</td>
    </tr>
    <tr>
      <th>280</th>
      <td>0.016700</td>
      <td>8</td>
      <td>0.859344</td>
    </tr>
    <tr>
      <th>281</th>
      <td>0.016736</td>
      <td>7</td>
      <td>0.867066</td>
    </tr>
    <tr>
      <th>282</th>
      <td>0.017641</td>
      <td>6</td>
      <td>0.870193</td>
    </tr>
    <tr>
      <th>283</th>
      <td>0.019663</td>
      <td>5</td>
      <td>0.870077</td>
    </tr>
    <tr>
      <th>284</th>
      <td>0.020334</td>
      <td>4</td>
      <td>0.852220</td>
    </tr>
    <tr>
      <th>285</th>
      <td>0.022876</td>
      <td>3</td>
      <td>0.828494</td>
    </tr>
    <tr>
      <th>286</th>
      <td>0.023965</td>
      <td>2</td>
      <td>0.843571</td>
    </tr>
    <tr>
      <th>287</th>
      <td>0.058944</td>
      <td>1</td>
      <td>0.843958</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;thresh&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;roc&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>thresh</th>
      <th>n</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>172</th>
      <td>0.002623</td>
      <td>116</td>
      <td>0.905483</td>
    </tr>
    <tr>
      <th>159</th>
      <td>0.002117</td>
      <td>129</td>
      <td>0.905444</td>
    </tr>
    <tr>
      <th>168</th>
      <td>0.002498</td>
      <td>120</td>
      <td>0.905328</td>
    </tr>
    <tr>
      <th>173</th>
      <td>0.002650</td>
      <td>115</td>
      <td>0.905135</td>
    </tr>
    <tr>
      <th>174</th>
      <td>0.002653</td>
      <td>114</td>
      <td>0.904942</td>
    </tr>
    <tr>
      <th>169</th>
      <td>0.002514</td>
      <td>119</td>
      <td>0.904710</td>
    </tr>
    <tr>
      <th>138</th>
      <td>0.001648</td>
      <td>150</td>
      <td>0.904402</td>
    </tr>
    <tr>
      <th>184</th>
      <td>0.003040</td>
      <td>104</td>
      <td>0.904286</td>
    </tr>
    <tr>
      <th>127</th>
      <td>0.001148</td>
      <td>161</td>
      <td>0.903784</td>
    </tr>
    <tr>
      <th>167</th>
      <td>0.002443</td>
      <td>121</td>
      <td>0.903745</td>
    </tr>
    <tr>
      <th>195</th>
      <td>0.003357</td>
      <td>93</td>
      <td>0.903514</td>
    </tr>
    <tr>
      <th>185</th>
      <td>0.003061</td>
      <td>103</td>
      <td>0.903514</td>
    </tr>
    <tr>
      <th>171</th>
      <td>0.002575</td>
      <td>117</td>
      <td>0.903475</td>
    </tr>
    <tr>
      <th>222</th>
      <td>0.004941</td>
      <td>66</td>
      <td>0.903398</td>
    </tr>
    <tr>
      <th>179</th>
      <td>0.002896</td>
      <td>109</td>
      <td>0.903320</td>
    </tr>
    <tr>
      <th>226</th>
      <td>0.005402</td>
      <td>62</td>
      <td>0.902973</td>
    </tr>
    <tr>
      <th>162</th>
      <td>0.002316</td>
      <td>126</td>
      <td>0.902896</td>
    </tr>
    <tr>
      <th>216</th>
      <td>0.004479</td>
      <td>72</td>
      <td>0.902780</td>
    </tr>
    <tr>
      <th>181</th>
      <td>0.002923</td>
      <td>107</td>
      <td>0.902625</td>
    </tr>
    <tr>
      <th>164</th>
      <td>0.002381</td>
      <td>124</td>
      <td>0.902510</td>
    </tr>
    <tr>
      <th>146</th>
      <td>0.001803</td>
      <td>142</td>
      <td>0.902510</td>
    </tr>
    <tr>
      <th>165</th>
      <td>0.002394</td>
      <td>123</td>
      <td>0.902510</td>
    </tr>
    <tr>
      <th>126</th>
      <td>0.001091</td>
      <td>162</td>
      <td>0.902317</td>
    </tr>
    <tr>
      <th>125</th>
      <td>0.001079</td>
      <td>163</td>
      <td>0.902317</td>
    </tr>
    <tr>
      <th>177</th>
      <td>0.002801</td>
      <td>111</td>
      <td>0.901853</td>
    </tr>
    <tr>
      <th>221</th>
      <td>0.004885</td>
      <td>67</td>
      <td>0.901544</td>
    </tr>
    <tr>
      <th>204</th>
      <td>0.003586</td>
      <td>84</td>
      <td>0.901429</td>
    </tr>
    <tr>
      <th>90</th>
      <td>0.000364</td>
      <td>198</td>
      <td>0.901390</td>
    </tr>
    <tr>
      <th>151</th>
      <td>0.001898</td>
      <td>137</td>
      <td>0.901351</td>
    </tr>
    <tr>
      <th>176</th>
      <td>0.002722</td>
      <td>112</td>
      <td>0.901313</td>
    </tr>
    <tr>
      <th>175</th>
      <td>0.002717</td>
      <td>113</td>
      <td>0.901274</td>
    </tr>
    <tr>
      <th>131</th>
      <td>0.001436</td>
      <td>157</td>
      <td>0.901197</td>
    </tr>
    <tr>
      <th>155</th>
      <td>0.002031</td>
      <td>133</td>
      <td>0.901120</td>
    </tr>
    <tr>
      <th>163</th>
      <td>0.002354</td>
      <td>125</td>
      <td>0.901120</td>
    </tr>
    <tr>
      <th>203</th>
      <td>0.003576</td>
      <td>85</td>
      <td>0.901042</td>
    </tr>
    <tr>
      <th>145</th>
      <td>0.001792</td>
      <td>143</td>
      <td>0.900965</td>
    </tr>
    <tr>
      <th>144</th>
      <td>0.001774</td>
      <td>144</td>
      <td>0.900965</td>
    </tr>
    <tr>
      <th>143</th>
      <td>0.001736</td>
      <td>145</td>
      <td>0.900927</td>
    </tr>
    <tr>
      <th>154</th>
      <td>0.001996</td>
      <td>134</td>
      <td>0.900849</td>
    </tr>
    <tr>
      <th>202</th>
      <td>0.003555</td>
      <td>86</td>
      <td>0.900811</td>
    </tr>
    <tr>
      <th>218</th>
      <td>0.004772</td>
      <td>70</td>
      <td>0.900792</td>
    </tr>
    <tr>
      <th>199</th>
      <td>0.003477</td>
      <td>89</td>
      <td>0.900734</td>
    </tr>
    <tr>
      <th>180</th>
      <td>0.002911</td>
      <td>108</td>
      <td>0.900656</td>
    </tr>
    <tr>
      <th>82</th>
      <td>0.000193</td>
      <td>206</td>
      <td>0.900463</td>
    </tr>
    <tr>
      <th>81</th>
      <td>0.000160</td>
      <td>207</td>
      <td>0.900463</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.000217</td>
      <td>204</td>
      <td>0.900347</td>
    </tr>
    <tr>
      <th>83</th>
      <td>0.000211</td>
      <td>205</td>
      <td>0.900347</td>
    </tr>
    <tr>
      <th>196</th>
      <td>0.003412</td>
      <td>92</td>
      <td>0.900309</td>
    </tr>
    <tr>
      <th>140</th>
      <td>0.001675</td>
      <td>148</td>
      <td>0.900309</td>
    </tr>
    <tr>
      <th>209</th>
      <td>0.003843</td>
      <td>79</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>89</th>
      <td>0.000323</td>
      <td>199</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>86</th>
      <td>0.000281</td>
      <td>202</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>87</th>
      <td>0.000311</td>
      <td>201</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>85</th>
      <td>0.000246</td>
      <td>203</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>88</th>
      <td>0.000314</td>
      <td>200</td>
      <td>0.900154</td>
    </tr>
    <tr>
      <th>223</th>
      <td>0.005152</td>
      <td>65</td>
      <td>0.900116</td>
    </tr>
    <tr>
      <th>156</th>
      <td>0.002059</td>
      <td>132</td>
      <td>0.900077</td>
    </tr>
    <tr>
      <th>215</th>
      <td>0.004395</td>
      <td>73</td>
      <td>0.899884</td>
    </tr>
    <tr>
      <th>182</th>
      <td>0.002986</td>
      <td>106</td>
      <td>0.899807</td>
    </tr>
    <tr>
      <th>187</th>
      <td>0.003073</td>
      <td>101</td>
      <td>0.899768</td>
    </tr>
    <tr>
      <th>161</th>
      <td>0.002239</td>
      <td>127</td>
      <td>0.899768</td>
    </tr>
    <tr>
      <th>106</th>
      <td>0.000583</td>
      <td>182</td>
      <td>0.899575</td>
    </tr>
    <tr>
      <th>123</th>
      <td>0.001017</td>
      <td>165</td>
      <td>0.899382</td>
    </tr>
    <tr>
      <th>122</th>
      <td>0.000885</td>
      <td>166</td>
      <td>0.899382</td>
    </tr>
    <tr>
      <th>224</th>
      <td>0.005212</td>
      <td>64</td>
      <td>0.899344</td>
    </tr>
    <tr>
      <th>134</th>
      <td>0.001532</td>
      <td>154</td>
      <td>0.899305</td>
    </tr>
    <tr>
      <th>166</th>
      <td>0.002406</td>
      <td>122</td>
      <td>0.899305</td>
    </tr>
    <tr>
      <th>201</th>
      <td>0.003543</td>
      <td>87</td>
      <td>0.899228</td>
    </tr>
    <tr>
      <th>148</th>
      <td>0.001849</td>
      <td>140</td>
      <td>0.899189</td>
    </tr>
    <tr>
      <th>93</th>
      <td>0.000415</td>
      <td>195</td>
      <td>0.899151</td>
    </tr>
    <tr>
      <th>92</th>
      <td>0.000379</td>
      <td>196</td>
      <td>0.899151</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>288</td>
      <td>0.899112</td>
    </tr>
    <tr>
      <th>79</th>
      <td>0.000095</td>
      <td>209</td>
      <td>0.899112</td>
    </tr>
    <tr>
      <th>133</th>
      <td>0.001488</td>
      <td>155</td>
      <td>0.899073</td>
    </tr>
    <tr>
      <th>149</th>
      <td>0.001870</td>
      <td>139</td>
      <td>0.899073</td>
    </tr>
    <tr>
      <th>91</th>
      <td>0.000364</td>
      <td>197</td>
      <td>0.899035</td>
    </tr>
    <tr>
      <th>178</th>
      <td>0.002839</td>
      <td>110</td>
      <td>0.899035</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.003352</td>
      <td>94</td>
      <td>0.898919</td>
    </tr>
    <tr>
      <th>213</th>
      <td>0.004159</td>
      <td>75</td>
      <td>0.898803</td>
    </tr>
    <tr>
      <th>113</th>
      <td>0.000706</td>
      <td>175</td>
      <td>0.898803</td>
    </tr>
    <tr>
      <th>147</th>
      <td>0.001808</td>
      <td>141</td>
      <td>0.898726</td>
    </tr>
    <tr>
      <th>139</th>
      <td>0.001675</td>
      <td>149</td>
      <td>0.898726</td>
    </tr>
    <tr>
      <th>107</th>
      <td>0.000598</td>
      <td>181</td>
      <td>0.898571</td>
    </tr>
    <tr>
      <th>117</th>
      <td>0.000816</td>
      <td>171</td>
      <td>0.898533</td>
    </tr>
    <tr>
      <th>153</th>
      <td>0.001991</td>
      <td>135</td>
      <td>0.898417</td>
    </tr>
    <tr>
      <th>150</th>
      <td>0.001881</td>
      <td>138</td>
      <td>0.898417</td>
    </tr>
    <tr>
      <th>160</th>
      <td>0.002217</td>
      <td>128</td>
      <td>0.898340</td>
    </tr>
    <tr>
      <th>129</th>
      <td>0.001347</td>
      <td>159</td>
      <td>0.898185</td>
    </tr>
    <tr>
      <th>132</th>
      <td>0.001454</td>
      <td>156</td>
      <td>0.898185</td>
    </tr>
    <tr>
      <th>95</th>
      <td>0.000421</td>
      <td>193</td>
      <td>0.898147</td>
    </tr>
    <tr>
      <th>94</th>
      <td>0.000416</td>
      <td>194</td>
      <td>0.898147</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0.000637</td>
      <td>180</td>
      <td>0.898108</td>
    </tr>
    <tr>
      <th>80</th>
      <td>0.000129</td>
      <td>208</td>
      <td>0.898031</td>
    </tr>
    <tr>
      <th>211</th>
      <td>0.004039</td>
      <td>77</td>
      <td>0.897992</td>
    </tr>
    <tr>
      <th>183</th>
      <td>0.002999</td>
      <td>105</td>
      <td>0.897992</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.000438</td>
      <td>192</td>
      <td>0.897992</td>
    </tr>
    <tr>
      <th>142</th>
      <td>0.001733</td>
      <td>146</td>
      <td>0.897915</td>
    </tr>
    <tr>
      <th>200</th>
      <td>0.003484</td>
      <td>88</td>
      <td>0.897876</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.003152</td>
      <td>99</td>
      <td>0.897876</td>
    </tr>
    <tr>
      <th>152</th>
      <td>0.001938</td>
      <td>136</td>
      <td>0.897761</td>
    </tr>
    <tr>
      <th>219</th>
      <td>0.004805</td>
      <td>69</td>
      <td>0.897722</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.000450</td>
      <td>191</td>
      <td>0.897722</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.003341</td>
      <td>95</td>
      <td>0.897606</td>
    </tr>
    <tr>
      <th>197</th>
      <td>0.003412</td>
      <td>91</td>
      <td>0.897606</td>
    </tr>
    <tr>
      <th>141</th>
      <td>0.001724</td>
      <td>147</td>
      <td>0.897490</td>
    </tr>
    <tr>
      <th>137</th>
      <td>0.001636</td>
      <td>151</td>
      <td>0.897375</td>
    </tr>
    <tr>
      <th>128</th>
      <td>0.001220</td>
      <td>160</td>
      <td>0.897375</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.000639</td>
      <td>179</td>
      <td>0.897375</td>
    </tr>
    <tr>
      <th>225</th>
      <td>0.005360</td>
      <td>63</td>
      <td>0.897336</td>
    </tr>
    <tr>
      <th>136</th>
      <td>0.001587</td>
      <td>152</td>
      <td>0.897297</td>
    </tr>
    <tr>
      <th>135</th>
      <td>0.001549</td>
      <td>153</td>
      <td>0.897181</td>
    </tr>
    <tr>
      <th>212</th>
      <td>0.004056</td>
      <td>76</td>
      <td>0.897143</td>
    </tr>
    <tr>
      <th>114</th>
      <td>0.000732</td>
      <td>174</td>
      <td>0.897143</td>
    </tr>
    <tr>
      <th>121</th>
      <td>0.000878</td>
      <td>167</td>
      <td>0.897027</td>
    </tr>
    <tr>
      <th>120</th>
      <td>0.000873</td>
      <td>168</td>
      <td>0.896950</td>
    </tr>
    <tr>
      <th>214</th>
      <td>0.004320</td>
      <td>74</td>
      <td>0.896853</td>
    </tr>
    <tr>
      <th>186</th>
      <td>0.003068</td>
      <td>102</td>
      <td>0.896834</td>
    </tr>
    <tr>
      <th>103</th>
      <td>0.000559</td>
      <td>185</td>
      <td>0.896680</td>
    </tr>
    <tr>
      <th>102</th>
      <td>0.000507</td>
      <td>186</td>
      <td>0.896680</td>
    </tr>
    <tr>
      <th>115</th>
      <td>0.000763</td>
      <td>173</td>
      <td>0.896564</td>
    </tr>
    <tr>
      <th>116</th>
      <td>0.000807</td>
      <td>172</td>
      <td>0.896564</td>
    </tr>
    <tr>
      <th>104</th>
      <td>0.000580</td>
      <td>184</td>
      <td>0.896332</td>
    </tr>
    <tr>
      <th>119</th>
      <td>0.000847</td>
      <td>169</td>
      <td>0.896216</td>
    </tr>
    <tr>
      <th>157</th>
      <td>0.002080</td>
      <td>131</td>
      <td>0.896178</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.003114</td>
      <td>100</td>
      <td>0.896178</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.003330</td>
      <td>96</td>
      <td>0.896139</td>
    </tr>
    <tr>
      <th>124</th>
      <td>0.001047</td>
      <td>164</td>
      <td>0.895946</td>
    </tr>
    <tr>
      <th>99</th>
      <td>0.000486</td>
      <td>189</td>
      <td>0.895869</td>
    </tr>
    <tr>
      <th>100</th>
      <td>0.000494</td>
      <td>188</td>
      <td>0.895869</td>
    </tr>
    <tr>
      <th>105</th>
      <td>0.000582</td>
      <td>183</td>
      <td>0.895830</td>
    </tr>
    <tr>
      <th>170</th>
      <td>0.002558</td>
      <td>118</td>
      <td>0.895830</td>
    </tr>
    <tr>
      <th>101</th>
      <td>0.000500</td>
      <td>187</td>
      <td>0.895792</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0.000692</td>
      <td>177</td>
      <td>0.895676</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.003294</td>
      <td>97</td>
      <td>0.895637</td>
    </tr>
    <tr>
      <th>228</th>
      <td>0.005871</td>
      <td>60</td>
      <td>0.895483</td>
    </tr>
    <tr>
      <th>130</th>
      <td>0.001362</td>
      <td>158</td>
      <td>0.895444</td>
    </tr>
    <tr>
      <th>98</th>
      <td>0.000482</td>
      <td>190</td>
      <td>0.895444</td>
    </tr>
    <tr>
      <th>229</th>
      <td>0.005975</td>
      <td>59</td>
      <td>0.895328</td>
    </tr>
    <tr>
      <th>198</th>
      <td>0.003473</td>
      <td>90</td>
      <td>0.894865</td>
    </tr>
    <tr>
      <th>158</th>
      <td>0.002116</td>
      <td>130</td>
      <td>0.894826</td>
    </tr>
    <tr>
      <th>110</th>
      <td>0.000661</td>
      <td>178</td>
      <td>0.894710</td>
    </tr>
    <tr>
      <th>217</th>
      <td>0.004572</td>
      <td>71</td>
      <td>0.894595</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0.000704</td>
      <td>176</td>
      <td>0.894324</td>
    </tr>
    <tr>
      <th>220</th>
      <td>0.004835</td>
      <td>68</td>
      <td>0.894286</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.003252</td>
      <td>98</td>
      <td>0.894247</td>
    </tr>
    <tr>
      <th>227</th>
      <td>0.005736</td>
      <td>61</td>
      <td>0.893977</td>
    </tr>
    <tr>
      <th>208</th>
      <td>0.003840</td>
      <td>80</td>
      <td>0.893552</td>
    </tr>
    <tr>
      <th>118</th>
      <td>0.000837</td>
      <td>170</td>
      <td>0.893514</td>
    </tr>
    <tr>
      <th>205</th>
      <td>0.003732</td>
      <td>83</td>
      <td>0.893089</td>
    </tr>
    <tr>
      <th>206</th>
      <td>0.003742</td>
      <td>82</td>
      <td>0.892664</td>
    </tr>
    <tr>
      <th>238</th>
      <td>0.006526</td>
      <td>50</td>
      <td>0.892664</td>
    </tr>
    <tr>
      <th>207</th>
      <td>0.003768</td>
      <td>81</td>
      <td>0.892394</td>
    </tr>
    <tr>
      <th>210</th>
      <td>0.003927</td>
      <td>78</td>
      <td>0.891737</td>
    </tr>
    <tr>
      <th>235</th>
      <td>0.006221</td>
      <td>53</td>
      <td>0.891274</td>
    </tr>
    <tr>
      <th>232</th>
      <td>0.006115</td>
      <td>56</td>
      <td>0.891081</td>
    </tr>
    <tr>
      <th>234</th>
      <td>0.006187</td>
      <td>54</td>
      <td>0.890849</td>
    </tr>
    <tr>
      <th>233</th>
      <td>0.006143</td>
      <td>55</td>
      <td>0.890347</td>
    </tr>
    <tr>
      <th>250</th>
      <td>0.008525</td>
      <td>38</td>
      <td>0.889691</td>
    </tr>
    <tr>
      <th>237</th>
      <td>0.006365</td>
      <td>51</td>
      <td>0.889498</td>
    </tr>
    <tr>
      <th>243</th>
      <td>0.007439</td>
      <td>45</td>
      <td>0.889112</td>
    </tr>
    <tr>
      <th>230</th>
      <td>0.005982</td>
      <td>58</td>
      <td>0.887413</td>
    </tr>
    <tr>
      <th>236</th>
      <td>0.006364</td>
      <td>52</td>
      <td>0.886950</td>
    </tr>
    <tr>
      <th>245</th>
      <td>0.007662</td>
      <td>43</td>
      <td>0.886448</td>
    </tr>
    <tr>
      <th>242</th>
      <td>0.007284</td>
      <td>46</td>
      <td>0.885753</td>
    </tr>
    <tr>
      <th>244</th>
      <td>0.007496</td>
      <td>44</td>
      <td>0.885560</td>
    </tr>
    <tr>
      <th>241</th>
      <td>0.006690</td>
      <td>47</td>
      <td>0.884865</td>
    </tr>
    <tr>
      <th>231</th>
      <td>0.006006</td>
      <td>57</td>
      <td>0.884710</td>
    </tr>
    <tr>
      <th>240</th>
      <td>0.006631</td>
      <td>48</td>
      <td>0.884170</td>
    </tr>
    <tr>
      <th>239</th>
      <td>0.006574</td>
      <td>49</td>
      <td>0.884015</td>
    </tr>
    <tr>
      <th>247</th>
      <td>0.008027</td>
      <td>41</td>
      <td>0.883822</td>
    </tr>
    <tr>
      <th>248</th>
      <td>0.008071</td>
      <td>40</td>
      <td>0.882857</td>
    </tr>
    <tr>
      <th>249</th>
      <td>0.008343</td>
      <td>39</td>
      <td>0.880077</td>
    </tr>
    <tr>
      <th>246</th>
      <td>0.007918</td>
      <td>42</td>
      <td>0.876795</td>
    </tr>
    <tr>
      <th>279</th>
      <td>0.016636</td>
      <td>9</td>
      <td>0.872104</td>
    </tr>
    <tr>
      <th>278</th>
      <td>0.015540</td>
      <td>10</td>
      <td>0.872066</td>
    </tr>
    <tr>
      <th>267</th>
      <td>0.011051</td>
      <td>21</td>
      <td>0.870849</td>
    </tr>
    <tr>
      <th>265</th>
      <td>0.010314</td>
      <td>23</td>
      <td>0.870386</td>
    </tr>
    <tr>
      <th>282</th>
      <td>0.017641</td>
      <td>6</td>
      <td>0.870193</td>
    </tr>
    <tr>
      <th>283</th>
      <td>0.019663</td>
      <td>5</td>
      <td>0.870077</td>
    </tr>
    <tr>
      <th>277</th>
      <td>0.015347</td>
      <td>11</td>
      <td>0.870039</td>
    </tr>
    <tr>
      <th>268</th>
      <td>0.011129</td>
      <td>20</td>
      <td>0.867201</td>
    </tr>
    <tr>
      <th>281</th>
      <td>0.016736</td>
      <td>7</td>
      <td>0.867066</td>
    </tr>
    <tr>
      <th>275</th>
      <td>0.014136</td>
      <td>13</td>
      <td>0.866525</td>
    </tr>
    <tr>
      <th>254</th>
      <td>0.008849</td>
      <td>34</td>
      <td>0.866178</td>
    </tr>
    <tr>
      <th>276</th>
      <td>0.014273</td>
      <td>12</td>
      <td>0.865753</td>
    </tr>
    <tr>
      <th>273</th>
      <td>0.013599</td>
      <td>15</td>
      <td>0.865405</td>
    </tr>
    <tr>
      <th>251</th>
      <td>0.008696</td>
      <td>37</td>
      <td>0.865058</td>
    </tr>
    <tr>
      <th>269</th>
      <td>0.011348</td>
      <td>19</td>
      <td>0.864903</td>
    </tr>
    <tr>
      <th>274</th>
      <td>0.014097</td>
      <td>14</td>
      <td>0.863089</td>
    </tr>
    <tr>
      <th>271</th>
      <td>0.011909</td>
      <td>17</td>
      <td>0.862819</td>
    </tr>
    <tr>
      <th>252</th>
      <td>0.008767</td>
      <td>36</td>
      <td>0.862587</td>
    </tr>
    <tr>
      <th>253</th>
      <td>0.008798</td>
      <td>35</td>
      <td>0.861892</td>
    </tr>
    <tr>
      <th>266</th>
      <td>0.010630</td>
      <td>22</td>
      <td>0.861699</td>
    </tr>
    <tr>
      <th>263</th>
      <td>0.009976</td>
      <td>25</td>
      <td>0.861429</td>
    </tr>
    <tr>
      <th>258</th>
      <td>0.009168</td>
      <td>30</td>
      <td>0.860695</td>
    </tr>
    <tr>
      <th>280</th>
      <td>0.016700</td>
      <td>8</td>
      <td>0.859344</td>
    </tr>
    <tr>
      <th>272</th>
      <td>0.013324</td>
      <td>16</td>
      <td>0.859112</td>
    </tr>
    <tr>
      <th>270</th>
      <td>0.011396</td>
      <td>18</td>
      <td>0.859112</td>
    </tr>
    <tr>
      <th>257</th>
      <td>0.009077</td>
      <td>31</td>
      <td>0.857915</td>
    </tr>
    <tr>
      <th>255</th>
      <td>0.008926</td>
      <td>33</td>
      <td>0.856873</td>
    </tr>
    <tr>
      <th>256</th>
      <td>0.009035</td>
      <td>32</td>
      <td>0.854595</td>
    </tr>
    <tr>
      <th>259</th>
      <td>0.009183</td>
      <td>29</td>
      <td>0.854015</td>
    </tr>
    <tr>
      <th>264</th>
      <td>0.010016</td>
      <td>24</td>
      <td>0.853398</td>
    </tr>
    <tr>
      <th>284</th>
      <td>0.020334</td>
      <td>4</td>
      <td>0.852220</td>
    </tr>
    <tr>
      <th>260</th>
      <td>0.009533</td>
      <td>28</td>
      <td>0.852201</td>
    </tr>
    <tr>
      <th>261</th>
      <td>0.009709</td>
      <td>27</td>
      <td>0.849575</td>
    </tr>
    <tr>
      <th>262</th>
      <td>0.009962</td>
      <td>26</td>
      <td>0.846680</td>
    </tr>
    <tr>
      <th>287</th>
      <td>0.058944</td>
      <td>1</td>
      <td>0.843958</td>
    </tr>
    <tr>
      <th>286</th>
      <td>0.023965</td>
      <td>2</td>
      <td>0.843571</td>
    </tr>
    <tr>
      <th>285</th>
      <td>0.022876</td>
      <td>3</td>
      <td>0.828494</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="lr">
<h2>LR<a class="headerlink" href="#lr" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># split data into train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">ml_helper</span><span class="o">.</span><span class="n">standardise_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
<span class="c1"># fit model on all training data</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># make predictions for test data and evaluate</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">roc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">roc</span> <span class="o">*</span> <span class="mf">100.0</span><span class="p">))</span>
<span class="c1"># Fit model using each importance as a threshold</span>
<span class="n">thresholds</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sort</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">thresh</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
    <span class="c1"># select features using threshold</span>
    <span class="n">selection</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">thresh</span><span class="p">,</span> <span class="n">prefit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">select_X_train</span> <span class="o">=</span> <span class="n">selection</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="c1"># train model</span>
    <span class="n">selection_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
    <span class="n">selection_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">select_X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c1"># eval model</span>
    <span class="n">select_X_test</span> <span class="o">=</span> <span class="n">selection</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">selection_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">select_X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">roc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">thresh</span><span class="p">,</span> <span class="n">select_X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">roc</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 86.57%
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;thresh&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;roc&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>thresh</th>
      <th>n</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>266</th>
      <td>0.551398</td>
      <td>48</td>
      <td>0.875598</td>
    </tr>
    <tr>
      <th>260</th>
      <td>0.437167</td>
      <td>63</td>
      <td>0.874247</td>
    </tr>
    <tr>
      <th>267</th>
      <td>0.574718</td>
      <td>44</td>
      <td>0.873900</td>
    </tr>
    <tr>
      <th>261</th>
      <td>0.439035</td>
      <td>61</td>
      <td>0.873784</td>
    </tr>
    <tr>
      <th>265</th>
      <td>0.521254</td>
      <td>50</td>
      <td>0.873205</td>
    </tr>
    <tr>
      <th>264</th>
      <td>0.495338</td>
      <td>53</td>
      <td>0.872625</td>
    </tr>
    <tr>
      <th>244</th>
      <td>0.317056</td>
      <td>105</td>
      <td>0.872587</td>
    </tr>
    <tr>
      <th>245</th>
      <td>0.323485</td>
      <td>104</td>
      <td>0.872239</td>
    </tr>
    <tr>
      <th>262</th>
      <td>0.465105</td>
      <td>58</td>
      <td>0.872046</td>
    </tr>
    <tr>
      <th>246</th>
      <td>0.325059</td>
      <td>103</td>
      <td>0.872008</td>
    </tr>
    <tr>
      <th>259</th>
      <td>0.436414</td>
      <td>64</td>
      <td>0.872008</td>
    </tr>
    <tr>
      <th>263</th>
      <td>0.489959</td>
      <td>54</td>
      <td>0.871969</td>
    </tr>
    <tr>
      <th>243</th>
      <td>0.310427</td>
      <td>109</td>
      <td>0.871544</td>
    </tr>
    <tr>
      <th>242</th>
      <td>0.300660</td>
      <td>110</td>
      <td>0.871081</td>
    </tr>
    <tr>
      <th>247</th>
      <td>0.327236</td>
      <td>100</td>
      <td>0.869846</td>
    </tr>
    <tr>
      <th>240</th>
      <td>0.288558</td>
      <td>113</td>
      <td>0.869537</td>
    </tr>
    <tr>
      <th>241</th>
      <td>0.299538</td>
      <td>111</td>
      <td>0.869112</td>
    </tr>
    <tr>
      <th>248</th>
      <td>0.341719</td>
      <td>96</td>
      <td>0.868803</td>
    </tr>
    <tr>
      <th>235</th>
      <td>0.278914</td>
      <td>120</td>
      <td>0.868610</td>
    </tr>
    <tr>
      <th>270</th>
      <td>0.624955</td>
      <td>35</td>
      <td>0.868571</td>
    </tr>
    <tr>
      <th>236</th>
      <td>0.283903</td>
      <td>118</td>
      <td>0.868301</td>
    </tr>
    <tr>
      <th>217</th>
      <td>0.190931</td>
      <td>155</td>
      <td>0.868108</td>
    </tr>
    <tr>
      <th>215</th>
      <td>0.187845</td>
      <td>160</td>
      <td>0.868069</td>
    </tr>
    <tr>
      <th>216</th>
      <td>0.189397</td>
      <td>159</td>
      <td>0.867915</td>
    </tr>
    <tr>
      <th>238</th>
      <td>0.286108</td>
      <td>116</td>
      <td>0.867838</td>
    </tr>
    <tr>
      <th>222</th>
      <td>0.204530</td>
      <td>145</td>
      <td>0.867761</td>
    </tr>
    <tr>
      <th>232</th>
      <td>0.262038</td>
      <td>125</td>
      <td>0.867761</td>
    </tr>
    <tr>
      <th>271</th>
      <td>0.652863</td>
      <td>31</td>
      <td>0.867645</td>
    </tr>
    <tr>
      <th>258</th>
      <td>0.435097</td>
      <td>67</td>
      <td>0.867606</td>
    </tr>
    <tr>
      <th>272</th>
      <td>0.703458</td>
      <td>28</td>
      <td>0.867529</td>
    </tr>
    <tr>
      <th>219</th>
      <td>0.195614</td>
      <td>151</td>
      <td>0.867490</td>
    </tr>
    <tr>
      <th>226</th>
      <td>0.222015</td>
      <td>136</td>
      <td>0.867413</td>
    </tr>
    <tr>
      <th>234</th>
      <td>0.268539</td>
      <td>123</td>
      <td>0.867375</td>
    </tr>
    <tr>
      <th>230</th>
      <td>0.241731</td>
      <td>129</td>
      <td>0.867336</td>
    </tr>
    <tr>
      <th>231</th>
      <td>0.259702</td>
      <td>127</td>
      <td>0.867297</td>
    </tr>
    <tr>
      <th>218</th>
      <td>0.191129</td>
      <td>154</td>
      <td>0.867220</td>
    </tr>
    <tr>
      <th>221</th>
      <td>0.196893</td>
      <td>149</td>
      <td>0.867181</td>
    </tr>
    <tr>
      <th>239</th>
      <td>0.287361</td>
      <td>114</td>
      <td>0.867143</td>
    </tr>
    <tr>
      <th>233</th>
      <td>0.265229</td>
      <td>124</td>
      <td>0.867066</td>
    </tr>
    <tr>
      <th>275</th>
      <td>0.764450</td>
      <td>23</td>
      <td>0.867066</td>
    </tr>
    <tr>
      <th>274</th>
      <td>0.740240</td>
      <td>24</td>
      <td>0.866988</td>
    </tr>
    <tr>
      <th>229</th>
      <td>0.228375</td>
      <td>133</td>
      <td>0.866641</td>
    </tr>
    <tr>
      <th>195</th>
      <td>0.101560</td>
      <td>207</td>
      <td>0.866602</td>
    </tr>
    <tr>
      <th>184</th>
      <td>0.061284</td>
      <td>238</td>
      <td>0.866602</td>
    </tr>
    <tr>
      <th>186</th>
      <td>0.066975</td>
      <td>230</td>
      <td>0.866602</td>
    </tr>
    <tr>
      <th>220</th>
      <td>0.195733</td>
      <td>150</td>
      <td>0.866602</td>
    </tr>
    <tr>
      <th>182</th>
      <td>0.054179</td>
      <td>244</td>
      <td>0.866602</td>
    </tr>
    <tr>
      <th>183</th>
      <td>0.054896</td>
      <td>243</td>
      <td>0.866486</td>
    </tr>
    <tr>
      <th>168</th>
      <td>0.026311</td>
      <td>267</td>
      <td>0.866486</td>
    </tr>
    <tr>
      <th>224</th>
      <td>0.218030</td>
      <td>141</td>
      <td>0.866448</td>
    </tr>
    <tr>
      <th>223</th>
      <td>0.214775</td>
      <td>143</td>
      <td>0.866371</td>
    </tr>
    <tr>
      <th>228</th>
      <td>0.228213</td>
      <td>134</td>
      <td>0.866332</td>
    </tr>
    <tr>
      <th>227</th>
      <td>0.226550</td>
      <td>135</td>
      <td>0.866293</td>
    </tr>
    <tr>
      <th>178</th>
      <td>0.044589</td>
      <td>252</td>
      <td>0.866255</td>
    </tr>
    <tr>
      <th>174</th>
      <td>0.028927</td>
      <td>261</td>
      <td>0.866255</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.085617</td>
      <td>214</td>
      <td>0.866216</td>
    </tr>
    <tr>
      <th>187</th>
      <td>0.068435</td>
      <td>228</td>
      <td>0.866178</td>
    </tr>
    <tr>
      <th>179</th>
      <td>0.047281</td>
      <td>250</td>
      <td>0.866178</td>
    </tr>
    <tr>
      <th>196</th>
      <td>0.106298</td>
      <td>204</td>
      <td>0.866178</td>
    </tr>
    <tr>
      <th>180</th>
      <td>0.051647</td>
      <td>249</td>
      <td>0.866178</td>
    </tr>
    <tr>
      <th>181</th>
      <td>0.053617</td>
      <td>245</td>
      <td>0.866139</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.087274</td>
      <td>213</td>
      <td>0.866100</td>
    </tr>
    <tr>
      <th>172</th>
      <td>0.027942</td>
      <td>263</td>
      <td>0.866062</td>
    </tr>
    <tr>
      <th>157</th>
      <td>0.003837</td>
      <td>287</td>
      <td>0.866023</td>
    </tr>
    <tr>
      <th>166</th>
      <td>0.020341</td>
      <td>272</td>
      <td>0.865985</td>
    </tr>
    <tr>
      <th>173</th>
      <td>0.028455</td>
      <td>262</td>
      <td>0.865907</td>
    </tr>
    <tr>
      <th>211</th>
      <td>0.164019</td>
      <td>171</td>
      <td>0.865907</td>
    </tr>
    <tr>
      <th>175</th>
      <td>0.029439</td>
      <td>260</td>
      <td>0.865907</td>
    </tr>
    <tr>
      <th>164</th>
      <td>0.011828</td>
      <td>279</td>
      <td>0.865869</td>
    </tr>
    <tr>
      <th>169</th>
      <td>0.027045</td>
      <td>266</td>
      <td>0.865869</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.088407</td>
      <td>212</td>
      <td>0.865869</td>
    </tr>
    <tr>
      <th>167</th>
      <td>0.020372</td>
      <td>271</td>
      <td>0.865869</td>
    </tr>
    <tr>
      <th>163</th>
      <td>0.011244</td>
      <td>280</td>
      <td>0.865869</td>
    </tr>
    <tr>
      <th>165</th>
      <td>0.012654</td>
      <td>277</td>
      <td>0.865830</td>
    </tr>
    <tr>
      <th>162</th>
      <td>0.007304</td>
      <td>281</td>
      <td>0.865830</td>
    </tr>
    <tr>
      <th>213</th>
      <td>0.168641</td>
      <td>167</td>
      <td>0.865830</td>
    </tr>
    <tr>
      <th>269</th>
      <td>0.614623</td>
      <td>38</td>
      <td>0.865830</td>
    </tr>
    <tr>
      <th>177</th>
      <td>0.040024</td>
      <td>254</td>
      <td>0.865830</td>
    </tr>
    <tr>
      <th>197</th>
      <td>0.111705</td>
      <td>201</td>
      <td>0.865830</td>
    </tr>
    <tr>
      <th>161</th>
      <td>0.005916</td>
      <td>282</td>
      <td>0.865792</td>
    </tr>
    <tr>
      <th>176</th>
      <td>0.031460</td>
      <td>259</td>
      <td>0.865753</td>
    </tr>
    <tr>
      <th>212</th>
      <td>0.165895</td>
      <td>169</td>
      <td>0.865753</td>
    </tr>
    <tr>
      <th>159</th>
      <td>0.005916</td>
      <td>284</td>
      <td>0.865714</td>
    </tr>
    <tr>
      <th>214</th>
      <td>0.182343</td>
      <td>162</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>210</th>
      <td>0.158082</td>
      <td>173</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>138</th>
      <td>-0.053093</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>132</th>
      <td>-0.061987</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>137</th>
      <td>-0.057044</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>136</th>
      <td>-0.057061</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>139</th>
      <td>-0.052234</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>135</th>
      <td>-0.057695</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>134</th>
      <td>-0.059232</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>133</th>
      <td>-0.061880</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>128</th>
      <td>-0.063299</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>131</th>
      <td>-0.062592</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>130</th>
      <td>-0.062729</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>129</th>
      <td>-0.062964</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>127</th>
      <td>-0.068387</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>126</th>
      <td>-0.071808</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>141</th>
      <td>-0.046938</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>125</th>
      <td>-0.073377</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>140</th>
      <td>-0.051733</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>0</th>
      <td>-1.384380</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>142</th>
      <td>-0.041556</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.188373</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>123</th>
      <td>-0.075629</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>160</th>
      <td>0.005916</td>
      <td>283</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>156</th>
      <td>0.002146</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>155</th>
      <td>-0.005129</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>154</th>
      <td>-0.011935</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>153</th>
      <td>-0.013520</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>152</th>
      <td>-0.014362</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>151</th>
      <td>-0.014388</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>150</th>
      <td>-0.019579</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>149</th>
      <td>-0.025440</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>148</th>
      <td>-0.026292</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>147</th>
      <td>-0.026311</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>146</th>
      <td>-0.031627</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>145</th>
      <td>-0.033167</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>144</th>
      <td>-0.033214</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>124</th>
      <td>-0.074107</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>143</th>
      <td>-0.038114</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>122</th>
      <td>-0.076440</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>62</th>
      <td>-0.311414</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>41</th>
      <td>-0.405446</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>42</th>
      <td>-0.405446</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>43</th>
      <td>-0.393944</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>44</th>
      <td>-0.390329</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>45</th>
      <td>-0.381061</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>46</th>
      <td>-0.379917</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>47</th>
      <td>-0.379053</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>48</th>
      <td>-0.378282</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>49</th>
      <td>-0.375212</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>50</th>
      <td>-0.371483</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>51</th>
      <td>-0.359434</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>52</th>
      <td>-0.359187</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>53</th>
      <td>-0.355142</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>54</th>
      <td>-0.344277</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>55</th>
      <td>-0.342112</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>121</th>
      <td>-0.076839</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.838820</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.849804</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.861405</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.063868</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>56</th>
      <td>-0.331339</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>57</th>
      <td>-0.330134</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>58</th>
      <td>-0.327981</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>59</th>
      <td>-0.326676</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>16</th>
      <td>-0.628204</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>61</th>
      <td>-0.313997</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.151950</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>40</th>
      <td>-0.408097</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>39</th>
      <td>-0.422531</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>38</th>
      <td>-0.425008</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>22</th>
      <td>-0.587532</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>15</th>
      <td>-0.630057</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>14</th>
      <td>-0.631017</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>13</th>
      <td>-0.668035</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>12</th>
      <td>-0.700221</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>11</th>
      <td>-0.712015</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>10</th>
      <td>-0.725220</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.781169</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-0.814247</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>18</th>
      <td>-0.615999</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>19</th>
      <td>-0.603171</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>20</th>
      <td>-0.591739</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>21</th>
      <td>-0.589466</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>23</th>
      <td>-0.566122</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>37</th>
      <td>-0.426491</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>25</th>
      <td>-0.556143</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>26</th>
      <td>-0.532040</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>27</th>
      <td>-0.519144</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>28</th>
      <td>-0.499811</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>29</th>
      <td>-0.481261</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>30</th>
      <td>-0.477039</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>31</th>
      <td>-0.476676</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>32</th>
      <td>-0.456299</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>33</th>
      <td>-0.454222</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>34</th>
      <td>-0.438419</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>35</th>
      <td>-0.436371</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>36</th>
      <td>-0.435695</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.153508</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>60</th>
      <td>-0.325266</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>63</th>
      <td>-0.310477</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>97</th>
      <td>-0.152393</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>103</th>
      <td>-0.126104</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>102</th>
      <td>-0.131965</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>101</th>
      <td>-0.136864</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>100</th>
      <td>-0.140771</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>99</th>
      <td>-0.143656</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>98</th>
      <td>-0.146166</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>96</th>
      <td>-0.153246</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>88</th>
      <td>-0.181806</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>95</th>
      <td>-0.157502</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>94</th>
      <td>-0.158576</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>93</th>
      <td>-0.165020</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>92</th>
      <td>-0.166672</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>91</th>
      <td>-0.170458</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>90</th>
      <td>-0.180566</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>104</th>
      <td>-0.123972</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>105</th>
      <td>-0.122366</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>106</th>
      <td>-0.122024</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>107</th>
      <td>-0.119172</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>108</th>
      <td>-0.116878</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>109</th>
      <td>-0.112494</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>110</th>
      <td>-0.109996</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>111</th>
      <td>-0.108687</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>112</th>
      <td>-0.105871</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>113</th>
      <td>-0.102355</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>114</th>
      <td>-0.097681</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>115</th>
      <td>-0.096697</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>116</th>
      <td>-0.093615</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>117</th>
      <td>-0.085071</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>118</th>
      <td>-0.083224</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>119</th>
      <td>-0.078550</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>120</th>
      <td>-0.078395</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>89</th>
      <td>-0.181280</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>17</th>
      <td>-0.621828</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>73</th>
      <td>-0.231984</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>64</th>
      <td>-0.298100</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>69</th>
      <td>-0.260627</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>71</th>
      <td>-0.241077</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>68</th>
      <td>-0.269518</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>67</th>
      <td>-0.277004</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>72</th>
      <td>-0.232555</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>87</th>
      <td>-0.187812</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>74</th>
      <td>-0.221612</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>75</th>
      <td>-0.220904</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>66</th>
      <td>-0.280302</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>65</th>
      <td>-0.287132</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>76</th>
      <td>-0.219743</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>70</th>
      <td>-0.242722</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>77</th>
      <td>-0.216751</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>78</th>
      <td>-0.214341</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>79</th>
      <td>-0.204354</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>80</th>
      <td>-0.202758</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>81</th>
      <td>-0.197179</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>82</th>
      <td>-0.195381</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>83</th>
      <td>-0.193251</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>84</th>
      <td>-0.190178</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>85</th>
      <td>-0.190178</td>
      <td>288</td>
      <td>0.865676</td>
    </tr>
    <tr>
      <th>268</th>
      <td>0.604952</td>
      <td>39</td>
      <td>0.865637</td>
    </tr>
    <tr>
      <th>185</th>
      <td>0.061555</td>
      <td>237</td>
      <td>0.865598</td>
    </tr>
    <tr>
      <th>158</th>
      <td>0.004550</td>
      <td>286</td>
      <td>0.865598</td>
    </tr>
    <tr>
      <th>199</th>
      <td>0.115297</td>
      <td>198</td>
      <td>0.865598</td>
    </tr>
    <tr>
      <th>225</th>
      <td>0.219075</td>
      <td>140</td>
      <td>0.865598</td>
    </tr>
    <tr>
      <th>208</th>
      <td>0.145704</td>
      <td>179</td>
      <td>0.865560</td>
    </tr>
    <tr>
      <th>198</th>
      <td>0.111783</td>
      <td>200</td>
      <td>0.865560</td>
    </tr>
    <tr>
      <th>209</th>
      <td>0.145833</td>
      <td>178</td>
      <td>0.865483</td>
    </tr>
    <tr>
      <th>207</th>
      <td>0.140812</td>
      <td>181</td>
      <td>0.865251</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.089112</td>
      <td>211</td>
      <td>0.865212</td>
    </tr>
    <tr>
      <th>201</th>
      <td>0.119723</td>
      <td>194</td>
      <td>0.865174</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.077394</td>
      <td>221</td>
      <td>0.865174</td>
    </tr>
    <tr>
      <th>202</th>
      <td>0.120010</td>
      <td>193</td>
      <td>0.865019</td>
    </tr>
    <tr>
      <th>206</th>
      <td>0.136798</td>
      <td>184</td>
      <td>0.864942</td>
    </tr>
    <tr>
      <th>205</th>
      <td>0.131485</td>
      <td>186</td>
      <td>0.864903</td>
    </tr>
    <tr>
      <th>256</th>
      <td>0.399936</td>
      <td>75</td>
      <td>0.864865</td>
    </tr>
    <tr>
      <th>255</th>
      <td>0.395617</td>
      <td>76</td>
      <td>0.864595</td>
    </tr>
    <tr>
      <th>257</th>
      <td>0.405094</td>
      <td>74</td>
      <td>0.864517</td>
    </tr>
    <tr>
      <th>200</th>
      <td>0.118341</td>
      <td>196</td>
      <td>0.864286</td>
    </tr>
    <tr>
      <th>204</th>
      <td>0.122552</td>
      <td>189</td>
      <td>0.864247</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.078721</td>
      <td>218</td>
      <td>0.863822</td>
    </tr>
    <tr>
      <th>203</th>
      <td>0.122040</td>
      <td>191</td>
      <td>0.863707</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.085042</td>
      <td>216</td>
      <td>0.863629</td>
    </tr>
    <tr>
      <th>254</th>
      <td>0.391604</td>
      <td>78</td>
      <td>0.862780</td>
    </tr>
    <tr>
      <th>252</th>
      <td>0.374290</td>
      <td>86</td>
      <td>0.860541</td>
    </tr>
    <tr>
      <th>251</th>
      <td>0.364577</td>
      <td>88</td>
      <td>0.859691</td>
    </tr>
    <tr>
      <th>253</th>
      <td>0.390256</td>
      <td>80</td>
      <td>0.859653</td>
    </tr>
    <tr>
      <th>249</th>
      <td>0.361093</td>
      <td>90</td>
      <td>0.859537</td>
    </tr>
    <tr>
      <th>250</th>
      <td>0.363924</td>
      <td>89</td>
      <td>0.859498</td>
    </tr>
    <tr>
      <th>273</th>
      <td>0.706925</td>
      <td>27</td>
      <td>0.859344</td>
    </tr>
    <tr>
      <th>276</th>
      <td>0.864692</td>
      <td>17</td>
      <td>0.829768</td>
    </tr>
    <tr>
      <th>277</th>
      <td>0.874200</td>
      <td>16</td>
      <td>0.817799</td>
    </tr>
    <tr>
      <th>281</th>
      <td>0.932741</td>
      <td>12</td>
      <td>0.812973</td>
    </tr>
    <tr>
      <th>280</th>
      <td>0.927508</td>
      <td>13</td>
      <td>0.809228</td>
    </tr>
    <tr>
      <th>279</th>
      <td>0.905442</td>
      <td>14</td>
      <td>0.807529</td>
    </tr>
    <tr>
      <th>278</th>
      <td>0.887972</td>
      <td>15</td>
      <td>0.806216</td>
    </tr>
    <tr>
      <th>282</th>
      <td>0.968478</td>
      <td>11</td>
      <td>0.714015</td>
    </tr>
    <tr>
      <th>283</th>
      <td>1.074339</td>
      <td>9</td>
      <td>0.707992</td>
    </tr>
    <tr>
      <th>285</th>
      <td>1.101534</td>
      <td>7</td>
      <td>0.705367</td>
    </tr>
    <tr>
      <th>284</th>
      <td>1.101210</td>
      <td>8</td>
      <td>0.703050</td>
    </tr>
    <tr>
      <th>286</th>
      <td>1.215731</td>
      <td>3</td>
      <td>0.699691</td>
    </tr>
    <tr>
      <th>287</th>
      <td>1.225033</td>
      <td>2</td>
      <td>0.694131</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_cols</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">thresholds</span><span class="o">&gt;=</span><span class="mf">0.8498</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_cols</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;before_TIR_lv2_hypo&#39;, &#39;before_TIR_lv1_hypo&#39;, &#39;before_TIR_hypo&#39;,
       &#39;before_TIR_norm&#39;, &#39;before_TIR_hyper&#39;, &#39;before_TIR_lv1_hyper&#39;,
       &#39;before_TIR_lv2_hyper&#39;, &#39;before_glc__approximate_entropy__m_2__r_0.9&#39;,
       &#39;before_glc__approximate_entropy__m_2__r_0.5&#39;,
       &#39;before_glc__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.8__ql_0.6&#39;,
       &#39;before_glc__partial_autocorrelation__lag_1&#39;, &#39;time_of_day_afternoon&#39;,
       &#39;time_of_day_evening&#39;, &#39;time_of_day_morning&#39;, &#39;form_of_exercise_aer&#39;,
       &#39;form_of_exercise_ana&#39;, &#39;form_of_exercise_mix&#39;, &#39;sex_female&#39;,
       &#39;sex_male&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
<span class="n">xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[16:25:41] WARNING: ..\src\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,
              gamma=0, gpu_id=-1, importance_type=None,
              interaction_constraints=&#39;&#39;, learning_rate=0.300000012,
              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,
              monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=8,
              num_parallel_tree=1, predictor=&#39;auto&#39;, random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
              tree_method=&#39;exact&#39;, validate_parameters=1, verbosity=None)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plot_importance</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span><span class="n">max_num_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 360x360 with 0 Axes&gt;
</pre></div>
</div>
<img alt="../../_images/92057d0315c2abd733a354342c5c16430f565585bf8809fa78c43699ad0faa46.png" src="../../_images/92057d0315c2abd733a354342c5c16430f565585bf8809fa78c43699ad0faa46.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_importance</span><span class="p">(</span><span class="n">xgb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;F score&#39;, ylabel=&#39;Features&#39;&gt;
</pre></div>
</div>
<img alt="../../_images/6a96b85cb24eaa3ddc0bf1e6002a836c60725aaaa2a5a2a1eafe29821c10243c.png" src="../../_images/6a96b85cb24eaa3ddc0bf1e6002a836c60725aaaa2a5a2a1eafe29821c10243c.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature to add</th>
      <th>ROC AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>start_glc</td>
      <td>0.863634</td>
    </tr>
    <tr>
      <th>1</th>
      <td>before_TIR_lv2_hypo</td>
      <td>0.868886</td>
    </tr>
    <tr>
      <th>2</th>
      <td>before_number_lv2_hypos</td>
      <td>0.868886</td>
    </tr>
    <tr>
      <th>3</th>
      <td>form_of_exercise_ana</td>
      <td>0.867886</td>
    </tr>
    <tr>
      <th>4</th>
      <td>time_of_day_evening</td>
      <td>0.863568</td>
    </tr>
    <tr>
      <th>5</th>
      <td>before_number_lv1_hypos</td>
      <td>0.860458</td>
    </tr>
    <tr>
      <th>6</th>
      <td>duration</td>
      <td>0.858591</td>
    </tr>
    <tr>
      <th>7</th>
      <td>before_maximum_glucose</td>
      <td>0.870520</td>
    </tr>
    <tr>
      <th>8</th>
      <td>cpep</td>
      <td>0.881085</td>
    </tr>
    <tr>
      <th>9</th>
      <td>before_TIR_hyper_exercise</td>
      <td>0.886819</td>
    </tr>
    <tr>
      <th>10</th>
      <td>before_sd</td>
      <td>0.887635</td>
    </tr>
    <tr>
      <th>11</th>
      <td>before_TIR_lv1_hyper</td>
      <td>0.887749</td>
    </tr>
    <tr>
      <th>12</th>
      <td>before_TIR_lv1_hypo</td>
      <td>0.890173</td>
    </tr>
    <tr>
      <th>13</th>
      <td>before_number_hypos</td>
      <td>0.889604</td>
    </tr>
    <tr>
      <th>14</th>
      <td>month</td>
      <td>0.888638</td>
    </tr>
    <tr>
      <th>15</th>
      <td>form_of_exercise_aer</td>
      <td>0.889207</td>
    </tr>
    <tr>
      <th>16</th>
      <td>before_avg_length_of_hypo</td>
      <td>0.889432</td>
    </tr>
    <tr>
      <th>17</th>
      <td>form_of_exercise_mix</td>
      <td>0.888642</td>
    </tr>
    <tr>
      <th>18</th>
      <td>before_total_time_in_hypos</td>
      <td>0.888333</td>
    </tr>
    <tr>
      <th>19</th>
      <td>before_TIR_hypo_exercise</td>
      <td>0.888591</td>
    </tr>
    <tr>
      <th>20</th>
      <td>before_TIR_hyper</td>
      <td>0.888664</td>
    </tr>
    <tr>
      <th>21</th>
      <td>time_of_day_afternoon</td>
      <td>0.887230</td>
    </tr>
    <tr>
      <th>22</th>
      <td>before_number_hypos_below_5</td>
      <td>0.888709</td>
    </tr>
    <tr>
      <th>23</th>
      <td>before_percent_missing</td>
      <td>0.885995</td>
    </tr>
    <tr>
      <th>24</th>
      <td>sex_female</td>
      <td>0.886899</td>
    </tr>
    <tr>
      <th>25</th>
      <td>sex_male</td>
      <td>0.886899</td>
    </tr>
    <tr>
      <th>26</th>
      <td>before_TIR_normal_exercise</td>
      <td>0.886216</td>
    </tr>
    <tr>
      <th>27</th>
      <td>before_minimum_glucose</td>
      <td>0.886104</td>
    </tr>
    <tr>
      <th>28</th>
      <td>before_average_glucose</td>
      <td>0.886820</td>
    </tr>
    <tr>
      <th>29</th>
      <td>before_ea1c</td>
      <td>0.886820</td>
    </tr>
    <tr>
      <th>30</th>
      <td>before_TIR_hypo</td>
      <td>0.885585</td>
    </tr>
    <tr>
      <th>31</th>
      <td>time_of_day_morning</td>
      <td>0.883988</td>
    </tr>
    <tr>
      <th>32</th>
      <td>before_total_time_in_hypos_below_5</td>
      <td>0.882628</td>
    </tr>
    <tr>
      <th>33</th>
      <td>before_avg_length_hypo_below_5</td>
      <td>0.885816</td>
    </tr>
    <tr>
      <th>34</th>
      <td>before_TIR_lv2_hyper</td>
      <td>0.882466</td>
    </tr>
    <tr>
      <th>35</th>
      <td>before_cv</td>
      <td>0.881742</td>
    </tr>
    <tr>
      <th>36</th>
      <td>before_TIR_norm</td>
      <td>0.876998</td>
    </tr>
    <tr>
      <th>37</th>
      <td>day_of_week</td>
      <td>0.877537</td>
    </tr>
    <tr>
      <th>38</th>
      <td>age</td>
      <td>0.874563</td>
    </tr>
    <tr>
      <th>39</th>
      <td>before_mage_mean</td>
      <td>0.873648</td>
    </tr>
    <tr>
      <th>40</th>
      <td>day</td>
      <td>0.878171</td>
    </tr>
    <tr>
      <th>41</th>
      <td>years_since_diagnosis</td>
      <td>0.871509</td>
    </tr>
    <tr>
      <th>42</th>
      <td>intensity</td>
      <td>0.865920</td>
    </tr>
    <tr>
      <th>43</th>
      <td>hba1c</td>
      <td>0.856130</td>
    </tr>
    <tr>
      <th>44</th>
      <td>bmi</td>
      <td>0.853378</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../../Data/tidy_data/feature_selection_results.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">chart_x</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">number_of_features</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">chart_x</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;ROC AUC&#39;</span><span class="p">],</span>
        <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;ROC AUC&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of features&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy (ROC AUC)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a403e69d26c33cfa09a581703e7090f2aa3b56d448d5a8c172b21bf24a295b07.png" src="../../_images/a403e69d26c33cfa09a581703e7090f2aa3b56d448d5a8c172b21bf24a295b07.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">chart_x</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">number_of_features</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">chart_x</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;ROC AUC&#39;</span><span class="p">],</span>
        <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;ROC AUC&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of features&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy (ROC AUC)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c9050b2dc25ec052a127cfc46f20dc9c4feed6384abeb8d655428cf5903952c2.png" src="../../_images/c9050b2dc25ec052a127cfc46f20dc9c4feed6384abeb8d655428cf5903952c2.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ml_heatmap/2_feature_selection"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../1_preprocessing/4_ml_preparation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">4. Preparation for machine learning</p>
      </div>
    </a>
    <a class="right-next"
       href="../3_ml/0_landing.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Machine learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-feature-selection">Logistic regression feature selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xgb-with-all-the-features">XGB with all the features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xgb-with-a-few-features">XGB with a few features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypers">Hypers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">4.1.1. Logistic regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-shit">Other shit</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lr">LR</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Catherine Russon
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>